"""
Exploration Trajectory Summarizer

ä»æ¢ç´¢è½¨è¿¹ä¸­"æ€»ç»“"å‡ºæœ‰ä»·å€¼çš„ä»»åŠ¡å’ŒQAæ•°æ®
ä¸ç›´æ¥ç”Ÿæˆçš„åŒºåˆ«ï¼šè¿™é‡Œæ˜¯ä»å·²æœ‰æ¢ç´¢ä¸­"å‘ç°"å’Œ"æç‚¼"ä»»åŠ¡
"""

import openai
import json
import os
from typing import Optional, Dict, Any, List
from datetime import datetime

from models import Trajectory, SynthesizedTask, SynthesizedQA
from synthesis_config import SynthesisConfig


class ExplorationSummarizer:
    """
    æ¢ç´¢è½¨è¿¹æ€»ç»“å™¨
    
    åŠŸèƒ½ï¼š
    1. åˆ†ææ¢ç´¢è½¨è¿¹ï¼Œè¯†åˆ«æœ‰ä»·å€¼çš„æ“ä½œåºåˆ—
    2. ä»æ¢ç´¢ä¸­æç‚¼å‡ºå¯æ‰§è¡Œçš„ä»»åŠ¡
    3. ä¸ºä»»åŠ¡ç”Ÿæˆåˆé€‚çš„evaluator
    4. æˆ–è€…ç”Ÿæˆæ¨ç†å‹çš„QAå¯¹
    """
    
    def __init__(self, config: SynthesisConfig):
        """åˆå§‹åŒ–æ€»ç»“å™¨"""
        self.config = config
        
        self.client = openai.OpenAI(
            api_key=os.environ.get("OPENAI_API_KEY", ""),
            base_url=os.environ.get("OPENAI_API_URL", os.environ.get("OPENAI_API_BASE", ""))
        )
    
    def summarize_to_task(
        self, 
        trajectory: Trajectory,
        task_index: int = 0
    ) -> Optional[SynthesizedTask]:
        """
        ä»æ¢ç´¢è½¨è¿¹ä¸­æ€»ç»“å‡ºä»»åŠ¡
        
        å…³é”®ï¼šä¸æ˜¯åŸºäºè½¨è¿¹"ç”Ÿæˆ"ä»»åŠ¡ï¼Œè€Œæ˜¯"å‘ç°"è½¨è¿¹ä¸­éšå«çš„ä»»åŠ¡
        
        Args:
            trajectory: æ¢ç´¢è½¨è¿¹
            task_index: ä»»åŠ¡ç´¢å¼•
            
        Returns:
            æ€»ç»“å‡ºçš„ä»»åŠ¡
        """
        print(f"\nğŸ“ æ€»ç»“æ¢ç´¢è½¨è¿¹ä¸ºä»»åŠ¡ - Trajectory: {trajectory.trajectory_id}")
        
        # æ„å»ºè½¨è¿¹æè¿°
        traj_description = self._format_exploration_trajectory(trajectory)
        
        # ç”Ÿæˆæ€»ç»“prompt
        prompt = self._build_task_summarization_prompt(trajectory, traj_description)
        
        try:
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # ç”Ÿæˆä»»åŠ¡ID
            task_id = f"{trajectory.source_id}_task_{task_index}"
            
            # åˆ›å»ºä»»åŠ¡
            task = SynthesizedTask(
                id=task_id,
                question=result.get("task_instruction", ""),
                config=result.get("config", []),
                evaluator=result.get("evaluator", {}),
                trajectory_id=trajectory.trajectory_id,
                source_id=trajectory.source_id,
                answer=result.get("expected_score", 1.0),
                metadata={
                    "exploration_seed": trajectory.seed_data,
                    "trajectory_depth": trajectory.total_depth,
                    "num_actions": len(trajectory.nodes) - 1,
                    "synthesis_date": datetime.now().isoformat(),
                    "synthesis_type": "exploration_summary"
                }
            )
            
            print(f"  âœ“ æˆåŠŸæ€»ç»“å‡ºä»»åŠ¡")
            print(f"    Task ID: {task_id}")
            print(f"    ä»»åŠ¡æŒ‡ä»¤: {task.question[:100]}...")
            print(f"    Evaluatorç±»å‹: {task.evaluator.get('func', 'N/A')}")
            
            return task
            
        except Exception as e:
            print(f"  âœ— ä»»åŠ¡æ€»ç»“å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def summarize_to_qa(
        self,
        trajectory: Trajectory,
        qa_index: int = 0
    ) -> Optional[SynthesizedQA]:
        """
        ä»æ¢ç´¢è½¨è¿¹ä¸­æ€»ç»“å‡ºQAå¯¹
        
        å…³é”®ï¼šä»æ¢ç´¢ä¸­å‘ç°æœ‰ä»·å€¼çš„é—®é¢˜å’Œç­”æ¡ˆ
        
        Args:
            trajectory: æ¢ç´¢è½¨è¿¹
            qa_index: QAç´¢å¼•
            
        Returns:
            æ€»ç»“å‡ºçš„QAå¯¹
        """
        print(f"\nğŸ“ æ€»ç»“æ¢ç´¢è½¨è¿¹ä¸ºQA - Trajectory: {trajectory.trajectory_id}")
        
        # æ„å»ºè½¨è¿¹æè¿°
        traj_description = self._format_exploration_trajectory(trajectory)
        
        # ç”Ÿæˆæ€»ç»“prompt
        prompt = self._build_qa_summarization_prompt(trajectory, traj_description)
        
        try:
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # ç”ŸæˆQA ID
            qa_id = f"{trajectory.trajectory_id}_qa_{qa_index}"
            
            qa = SynthesizedQA(
                question=result.get("question", ""),
                answer=result.get("answer", ""),
                trajectory_id=trajectory.trajectory_id,
                source_id=trajectory.source_id,
                qa_id=qa_id,
                reasoning_steps=result.get("reasoning_steps", []),
                metadata={
                    "exploration_seed": trajectory.seed_data,
                    "trajectory_depth": trajectory.total_depth,
                    "synthesis_date": datetime.now().isoformat(),
                    "synthesis_type": "exploration_summary"
                }
            )
            
            print(f"  âœ“ æˆåŠŸæ€»ç»“å‡ºQAå¯¹")
            print(f"    QA ID: {qa_id}")
            print(f"    é—®é¢˜: {qa.question[:100]}...")
            print(f"    ç­”æ¡ˆ: {qa.answer[:100]}...")
            
            return qa
            
        except Exception as e:
            print(f"  âœ— QAæ€»ç»“å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def _build_task_summarization_prompt(
        self,
        trajectory: Trajectory,
        traj_description: str
    ) -> str:
        """æ„å»ºä»»åŠ¡æ€»ç»“prompt"""
        
        prompt = f"""You are a GUI task discovery expert. Analyze the following GUI exploration trajectory and DISCOVER a meaningful, executable task that was implicitly demonstrated.

ã€Exploration Directionã€‘
{trajectory.seed_data}

ã€Complete Exploration Trajectoryã€‘
{traj_description}

ã€Your Task: DISCOVER and SUMMARIZEã€‘
From this exploration trajectory, identify:
1. What meaningful task was accomplished or could be accomplished?
2. What would be a clear task instruction for this operation sequence?
3. How can we verify this task was completed correctly?

ã€Important Guidelinesã€‘
- The task should be DISCOVERED from the trajectory, not invented
- The task instruction should be natural and user-friendly
- The evaluator must be automatic and deterministic
- Focus on the VALUE and PURPOSE of the discovered operations

ã€Common Evaluator Typesã€‘

1. **Command Output Check** (for installations, file operations):
```json
{{
  "func": "check_include_exclude",
  "result": {{
    "type": "vm_command_line",
    "command": "which <program>" or "ls <path>"
  }},
  "expected": {{
    "type": "rule",
    "rules": {{
      "include": ["expected_text"],
      "exclude": ["error_text"]
    }}
  }}
}}
```

2. **File Content Check** (for file creation/editing):
```json
{{
  "func": "check_include_exclude",
  "result": {{
    "type": "vm_file_content",
    "path": "~/path/to/file"
  }},
  "expected": {{
    "type": "rule",
    "rules": {{
      "include": ["expected_content"],
      "exclude": []
    }}
  }}
}}
```

ã€Output Formatã€‘
Return JSON:
{{
  "task_instruction": "Clear, natural language instruction of what task to accomplish",
  "discovered_operations": ["list", "of", "key", "operations", "found"],
  "task_value": "Why this task is useful/meaningful",
  "config": [],
  "evaluator": {{
    "func": "evaluator_function",
    "result": {{...}},
    "expected": {{...}}
  }},
  "expected_score": 1.0
}}
"""
        
        # æ·»åŠ ç¤ºä¾‹
        if self.config.qa_examples:
            prompt += "\nã€Reference Examplesã€‘\n"
            for i, example in enumerate(self.config.qa_examples[:2], 1):
                prompt += f"\nExample {i}:\n"
                prompt += f"Task: {example.get('question', '')}\n"
                if 'evaluator' in example:
                    prompt += f"Evaluator: {json.dumps(example['evaluator'], indent=2)}\n"
        
        return prompt
    
    def _build_qa_summarization_prompt(
        self,
        trajectory: Trajectory,
        traj_description: str
    ) -> str:
        """æ„å»ºQAæ€»ç»“prompt"""
        
        prompt = f"""You are a GUI reasoning expert. Analyze the following GUI exploration trajectory and DISCOVER an interesting reasoning question that can be answered based on what was discovered.

ã€Exploration Directionã€‘
{trajectory.seed_data}

ã€Complete Exploration Trajectoryã€‘
{traj_description}

ã€Your Task: DISCOVER and CREATEã€‘
From this exploration trajectory, create:
1. An interesting question that requires understanding the discovered operations
2. A clear answer based on the trajectory
3. Reasoning steps showing how to derive the answer

ã€Question Types to Considerã€‘
- "What operations are needed to accomplish X?"
- "How many steps does it take to reach Y?"
- "What feature was discovered in Z location?"
- "What is the relationship between A and B operations?"

ã€Important Guidelinesã€‘
- Question should be GROUNDED in the actual exploration
- Answer must be DERIVABLE from the trajectory
- Reasoning should show the logical steps
- Focus on INSIGHTS discovered during exploration

ã€Output Formatã€‘
Return JSON:
{{
  "question": "Clear, interesting question",
  "answer": "Concise answer",
  "reasoning_steps": [
    {{
      "step": 1,
      "description": "step description",
      "observation": "what was observed",
      "conclusion": "what this tells us"
    }},
    ...
  ],
  "discovered_insights": ["key", "insights", "from", "exploration"]
}}
"""
        
        # æ·»åŠ QAç¤ºä¾‹
        if self.config.qa_examples:
            prompt += "\nã€Reference QA Examplesã€‘\n"
            for i, example in enumerate(self.config.qa_examples[:2], 1):
                if 'evaluator' not in example:  # åªå±•ç¤ºQAæ ¼å¼çš„ç¤ºä¾‹
                    prompt += f"\nExample {i}:\n"
                    prompt += f"Q: {example.get('question', '')}\n"
                    prompt += f"A: {example.get('answer', '')}\n"
        
        return prompt
    
    def _format_exploration_trajectory(self, trajectory: Trajectory) -> str:
        """æ ¼å¼åŒ–æ¢ç´¢è½¨è¿¹"""
        formatted = ""
        
        for i, node in enumerate(trajectory.nodes, 1):
            formatted += f"\n=== æ¢ç´¢æ­¥éª¤ {i} (æ·±åº¦: {node.depth}) ===\n"
            formatted += f"æ„å›¾: {node.intent}\n"
            
            if node.action:
                tool_name = node.action.get('tool_name', 'unknown')
                parameters = node.action.get('parameters', {})
                formatted += f"åŠ¨ä½œ: {tool_name}\n"
                formatted += f"å‚æ•°: {json.dumps(parameters, ensure_ascii=False)}\n"
            
            # ä¿ç•™observationçš„å…³é”®ä¿¡æ¯
            obs_lines = node.observation.split('\n')
            key_obs = '\n'.join(obs_lines[:20])  # å‰20è¡Œ
            if len(obs_lines) > 20:
                key_obs += f"\n... (çœç•¥{len(obs_lines)-20}è¡Œ)"
            
            formatted += f"è§‚å¯Ÿ:\n{key_obs}\n"
        
        return formatted

