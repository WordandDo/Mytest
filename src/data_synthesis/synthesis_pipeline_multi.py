"""
æ•°æ®åˆæˆä¸»Pipeline

æ•´åˆtrajectoryé‡‡æ ·ã€é€‰æ‹©å’ŒQAåˆæˆçš„å®Œæ•´æµç¨‹
"""

import json
import os
import bdb
import hashlib
from typing import List, Dict, Tuple, Set, Callable, Optional
from multiprocessing import Process, Manager, Queue, Lock

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from envs import (
    Environment,
    MathEnvironment,
    PythonEnvironment,
    RAGEnvironment,
    WebEnvironment,
    OSWorldEnvironment
)
from models import TrajectoryNode, Trajectory, SynthesizedQA, SynthesizedTask
from synthesis_config import SynthesisConfig
from trajectory_sampler import GenericTrajectorySampler
from trajectory_selector import GenericTrajectorySelector
from qa_synthesizer import GenericQASynthesizer
from task_synthesizer import OSWorldTaskSynthesizer


def run_synthesis_worker(
    worker_id: str,
    task_queue: Queue,
    config: SynthesisConfig,
    file_lock: Lock,
    qa_saver: Callable[[List[Dict]], None],
    traj_saver: Callable[[List[Dict]], None]
):
    """
    Worker function to process seeds in parallel using the Process/Queue model,
    implementing explicit resource allocation/release for heavy resources.
    """
    print(f"\n[Worker {worker_id}] Starting up...")

    # 1. åˆå§‹åŒ–ç¯å¢ƒå’Œç»„ä»¶ (Worker è¿›ç¨‹ç‹¬äº«)
    environment = _create_environment(config)
    
    sampler = GenericTrajectorySampler(
        environment=environment,
        config=config
    )
    
    selector = GenericTrajectorySelector(config=config)
    
    # ç®€åŒ–åˆæˆå™¨åˆå§‹åŒ–ï¼ˆåŸé€»è¾‘ï¼‰
    if config.output_format == "task":
        from task_synthesizer import OSWorldTaskSynthesizer
        synthesizer = OSWorldTaskSynthesizer(config=config)
    else:
        synthesizer = GenericQASynthesizer(config=config)

    # å°è¯•å¯åŠ¨ç¯å¢ƒè¿æ¥ (å»ºç«‹ MCP è¿æ¥ç­‰)
    if hasattr(environment, "env_start") and callable(environment.env_start):
        try:
            environment.env_start()
        except Exception as e:
            print(f"[Worker {worker_id}] env_start() failed: {e}")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¯ä»»åŠ¡èµ„æºåˆ†é… (Heavy Resource Check)
    env_has_heavy_resource = bool(getattr(environment, "has_heavy_resource", False) and callable(getattr(environment, "allocate_resource", None)))

    # 2. ä¸»ä»»åŠ¡å¾ªç¯ (Pull Model)
    while True:
        try:
            # å°è¯•ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼Œè®¾ç½®è¶…æ—¶ä»¥å…è®¸è¿›ç¨‹åœ¨ç©ºé—²æ—¶å…³é—­
            seed_task = task_queue.get(timeout=30) 
            
            if seed_task is None: # å“¨å…µå€¼
                print(f"[Worker {worker_id}] Received sentinel. Stopping loop.")
                break
        except Exception as e:
            # Queue is empty for a long time or other error
            print(f"[Worker {worker_id}] Error getting task: {e}")
            break

        seed_data = seed_task["seed_data"]
        seed_idx = seed_task["seed_idx"]
        source_id = _generate_source_id(seed_data, seed_idx)
        
        print(f"\n{'#'*80}")
        print(f"[Worker {worker_id}] START Seed {seed_idx}, Source ID: {source_id}")
        print(f"å†…å®¹: {seed_data[:100]}{'...' if len(seed_data) > 100 else ''}")
        print(f"{'#'*80}\n")
        
        resource_allocated = False
        
        try:
            # --- æ˜¾å¼èµ„æºåˆ†é… (Rollout æ¶æ„çš„æ ¸å¿ƒ) ---
            if env_has_heavy_resource:
                print(f"[Worker {worker_id}] ğŸ” Requesting heavy resource via MCP...")
                # ä½¿ç”¨ worker_id ä½œä¸º client ID
                if not environment.allocate_resource(worker_id):
                     raise RuntimeError("Failed to allocate resource via MCP")
                resource_allocated = True
                print(f"[Worker {worker_id}] âœ… Resource allocated.")
            
            # Step 1: Trajectory Sampling
            print(f"\nğŸ“Š æ­¥éª¤ 1/3: Trajectory Sampling")
            trajectory_tree = sampler.sample_trajectory_tree(seed_data)
            
            # Step 2: Trajectory Selection
            print(f"\nğŸ¯ æ­¥éª¤ 2/3: Trajectory Selection")
            selected_trajectories = selector.select_trajectories(
                nodes=trajectory_tree,
                root_id=sampler.root_id,
                seed_data=seed_data,
                source_id=source_id,
                max_selected_traj=config.max_selected_traj
            )
            
            # Step 3: æ•°æ®åˆæˆï¼ˆQAæˆ–Taskï¼‰
            outputs = []
            output_type = "QAå¯¹" if config.output_format != "task" else "ä»»åŠ¡"
            
            print(f"\nâœ¨ æ­¥éª¤ 3/3: {output_type} Synthesis")
            for qa_idx, trajectory in enumerate(selected_trajectories):
                try:
                    if config.output_format == "task":
                        synthesized_output = synthesizer.synthesize_task(trajectory, qa_idx)
                    else:
                        synthesized_output = synthesizer.synthesize_qa(trajectory, qa_idx)
                        
                    if synthesized_output:
                        outputs.append(synthesized_output.to_dict())
                except Exception as e:
                    print(f"[Worker {worker_id}] âŒ åˆæˆå¤±è´¥ (è½¨è¿¹ {qa_idx}): {str(e)}")
                    import traceback
                    traceback.print_exc()
            
            trajectories_data = [traj.to_dict() for traj in selected_trajectories]
            
            print(f"\nâœ… Seed {seed_idx} å®Œæˆ! ç”Ÿæˆäº† {len(outputs)} ä¸ª{output_type}")
            
            # --- å®æ—¶ä¿å­˜ç»“æœ (Worker è¿›ç¨‹è°ƒç”¨ä¸»è¿›ç¨‹ä¼ å…¥çš„ Saver) ---
            if outputs:
                qa_saver(outputs) 
            if trajectories_data:
                traj_saver(trajectories_data) 
                
        except Exception as e:
            error_msg = f"âŒ Seed {seed_idx} å¤±è´¥: {str(e)}"
            print(f"\n{error_msg}")
            import traceback
            traceback.print_exc()
            
        finally:
            # --- æ˜¾å¼èµ„æºé‡Šæ”¾ (Rollout æ¶æ„çš„æ ¸å¿ƒ) ---
            if env_has_heavy_resource and resource_allocated:
                print(f"[Worker {worker_id}] â™»ï¸ Releasing resource via MCP (reset=True)...")
                try:
                    environment.release_resource(worker_id, reset=True)
                except Exception as e:
                    print(f"[Worker {worker_id}] âš ï¸ Error releasing resource: {e}")
            
    # Worker é€€å‡ºæ—¶å…³é—­ç¯å¢ƒè¿æ¥
    if hasattr(environment, "env_close") and callable(environment.env_close):
        environment.env_close()
    print(f"[Worker {worker_id}] Stopped.")


# --- Keep the helper functions _generate_source_id and _create_environment here ---
# (As they were in the original file, just before the class definition)

def _generate_source_id(seed_data: str, seed_idx: int) -> str:
    """ç”Ÿæˆsourceçš„å”¯ä¸€æ ‡è¯†"""
    content_hash = hashlib.md5(seed_data.encode('utf-8')).hexdigest()[:8]
    return f"src_{seed_idx:04d}_{content_hash}"


def _create_environment(config: SynthesisConfig):
    """æ ¹æ®é…ç½®åˆ›å»ºç›¸åº”çš„ç¯å¢ƒ"""
    mode = config.environment_mode.lower()
    kwargs = config.environment_kwargs.copy()
    kwargs['model_name'] = config.model_name
    
    if mode == "web":
        from envs import WebEnvironment
        return WebEnvironment(**kwargs)
    elif mode == "math":
        from envs import MathEnvironment
        return MathEnvironment(**kwargs)
    elif mode == "python" or mode == "py":
        from envs import PythonEnvironment
        return PythonEnvironment(**kwargs)
    elif mode == "rag":
        if 'rag_index' not in kwargs:
            raise ValueError("RAGç¯å¢ƒéœ€è¦æä¾›rag_indexå‚æ•°")
        from envs import RAGEnvironment
        return RAGEnvironment(**kwargs)
    elif mode == "osworld" or mode == "gui":
        # OSWorld/GUIç¯å¢ƒéœ€è¦VMé…ç½®
        required_params = ['path_to_vm']
        missing = [p for p in required_params if p not in kwargs]
        if missing:
            raise ValueError(f"OSWorldç¯å¢ƒéœ€è¦æä¾›ä»¥ä¸‹å‚æ•°: {', '.join(missing)}")
        from envs import OSWorldEnvironment
        return OSWorldEnvironment(**kwargs)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç¯å¢ƒæ¨¡å¼: {mode}")


class GenericDataSynthesis:
    """
    é€šç”¨æ•°æ®åˆæˆä¸»ç±» - æ”¯æŒæ‰€æœ‰ç¯å¢ƒå’Œå·¥å…·
    """
    
    def __init__(self, config: SynthesisConfig, output_dir: str = "synthesis_results"):
        """
        åˆå§‹åŒ–é€šç”¨æ•°æ®åˆæˆç³»ç»Ÿ
        
        Args:
            config: åˆæˆé…ç½®
            output_dir: è¾“å‡ºç›®å½•
        """
        self.config = config
        self.output_dir = output_dir
        
        # éªŒè¯é…ç½®
        errors = config.validate()
        if errors:
            raise ValueError(f"é…ç½®é”™è¯¯: {', '.join(errors)}")
        
        # åˆ›å»ºç¯å¢ƒ
        print(f"åˆå§‹åŒ– {config.environment_mode.upper()} Environment...")
        self.environment = self._create_environment()
        
        # åˆ›å»ºä¸‰ä¸ªç»„ä»¶
        self.sampler = GenericTrajectorySampler(
            environment=self.environment,
            config=config
        )
        
        self.selector = GenericTrajectorySelector(config=config)
        
        # æ ¹æ®è¾“å‡ºæ ¼å¼é€‰æ‹©åˆæˆå™¨
        if config.output_format == "task":
            self.synthesizer = OSWorldTaskSynthesizer(config=config)
            print(f"ä½¿ç”¨OSWorldä»»åŠ¡åˆæˆå™¨ï¼ˆè¾“å‡ºæ ¼å¼ï¼štaskï¼‰")
        else:
            self.synthesizer = GenericQASynthesizer(config=config)
            print(f"ä½¿ç”¨QAåˆæˆå™¨ï¼ˆè¾“å‡ºæ ¼å¼ï¼šqaï¼‰")
        
        # å­˜å‚¨ç»“æœ (ç§»é™¤å†…å­˜åˆ—è¡¨ï¼Œä¾èµ–æ–‡ä»¶å®æ—¶å†™å…¥)
        # self.trajectory_tree: Dict[str, TrajectoryNode] = {}
        # self.selected_trajectories: List[Trajectory] = []
        # self.synthesized_qas: List[SynthesizedQA] = []  # QAæ ¼å¼
        # self.synthesized_tasks: List[SynthesizedTask] = []  # Taskæ ¼å¼
        
        # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆåœ¨runæ—¶åˆ›å»ºï¼‰
        self.qa_file_path = None
        self.traj_file_path = None
        
        # å·²å¤„ç†çš„source_idé›†åˆ
        self.processed_source_ids: Set[str] = set()
        
        # æ–‡ä»¶å†™å…¥é”ï¼ˆåœ¨ run() ä¸­ä½¿ç”¨ Manager.Lock() è¿›è¡Œåˆå§‹åŒ–ï¼‰
        self.file_lock: Optional[Lock] = None
    
    def _create_environment(self) -> Environment:
        """æ ¹æ®é…ç½®åˆ›å»ºç›¸åº”çš„ç¯å¢ƒ"""
        return _create_environment(self.config)
    
    def _initialize_output_files(self):
        """åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶è·¯å¾„å¹¶åˆ›å»ºè¾“å‡ºç›®å½•"""
        os.makedirs(self.output_dir, exist_ok=True)
        
        # æ ¹æ®è¾“å‡ºæ ¼å¼è®¾ç½®æ–‡ä»¶è·¯å¾„
        if self.config.output_format == "task":
            # OSWorldä»»åŠ¡æ ¼å¼
            self.qa_file_path = os.path.join(
                self.output_dir, 
                f"synthesized_tasks_{self.config.environment_mode}.jsonl"
            )
        else:
            # QAå¯¹æ ¼å¼
            self.qa_file_path = os.path.join(
                self.output_dir, 
                f"synthesized_qa_{self.config.environment_mode}.jsonl"
            )
        
        # è®¾ç½®trajectoriesè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå›ºå®šæ–‡ä»¶åï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        self.traj_file_path = os.path.join(
            self.output_dir, 
            f"trajectories_{self.config.environment_mode}.jsonl"
        )
        
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {self.qa_file_path}")
        print(f"ğŸ’¾ è¾“å‡ºæ ¼å¼: {self.config.output_format}")
        
        # åŠ è½½å·²å¤„ç†çš„source_id
        self._load_processed_source_ids()
    
    def _load_processed_source_ids(self):
        """ä»å·²æœ‰çš„è¾“å‡ºæ–‡ä»¶ä¸­åŠ è½½å·²å¤„ç†çš„source_id"""
        self.processed_source_ids.clear()
        
        # ä»QAæ–‡ä»¶ä¸­è¯»å–å·²å¤„ç†çš„source_id
        if os.path.exists(self.qa_file_path):
            try:
                with open(self.qa_file_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            qa_dict = json.loads(line)
                            if "source_id" in qa_dict:
                                self.processed_source_ids.add(qa_dict["source_id"])
                
                if self.processed_source_ids:
                    print(f"ğŸ”„ å‘ç° {len(self.processed_source_ids)} ä¸ªå·²å¤„ç†çš„sourceï¼Œå°†è·³è¿‡è¿™äº›seed")
            except Exception as e:
                print(f"âš ï¸  è¯»å–å·²å¤„ç†è®°å½•æ—¶å‡ºé”™: {e}")
                self.processed_source_ids.clear()
    
    def _save_qa_immediately(self, qas_dicts: List[Dict]):
        """ç«‹å³å°†QAå¯¹è¿½åŠ ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆè¿›ç¨‹å®‰å…¨ï¼‰"""
        with self.file_lock:
            with open(self.qa_file_path, "a", encoding="utf-8") as f:
                for qa_dict in qas_dicts:
                    f.write(json.dumps(qa_dict, ensure_ascii=False) + "\n")
    
    def _save_trajectories_immediately(self, trajectories_data: List[Dict]):
        """ç«‹å³å°†trajectoriesè¿½åŠ ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆè¿›ç¨‹å®‰å…¨ï¼‰"""
        with self.file_lock:
            with open(self.traj_file_path, "a", encoding="utf-8") as f:
                for traj in trajectories_data:
                    f.write(json.dumps(traj, ensure_ascii=False) + "\n")
    
    def run(self, seeds: List[str]) -> List[Dict]:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®åˆæˆpipelineï¼ˆä½¿ç”¨ Process/Queue æ¶æ„ï¼‰
        
        Args:
            seeds: Seedæ•°æ®åˆ—è¡¨
            
        Returns:
            åˆæˆçš„QAå¯¹å­—å…¸åˆ—è¡¨ (ä¸ºäº†å…¼å®¹æ€§è¿”å›ç©ºåˆ—è¡¨ï¼Œå®é™…ç»“æœå·²å†™å…¥æ–‡ä»¶)
        """
        # æ ¹æ®é…ç½®é™åˆ¶å¤„ç†çš„seedæ•°é‡
        if self.config.number_of_seed is not None:
            seeds = seeds[:self.config.number_of_seed]
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ é€šç”¨Agentæ•°æ®åˆæˆ Pipeline å¯åŠ¨")
        print(f"{'='*80}")
        print(f"ç¯å¢ƒæ¨¡å¼: {self.config.environment_mode}")
        print(f"Seedè¯´æ˜: {self.config.seed_description or '(æœªæŒ‡å®š)'}")
        print(f"å¯ç”¨å·¥å…·: {[t['name'] for t in self.sampler.available_tools]}")
        print(f"æ€»Seedæ•°é‡: {len(seeds)}")
        print(f"å¹¶è¡Œåº¦: {self.config.max_workers} workers")
        print(f"æ¨¡å‹: {self.config.model_name}")
        print(f"{'='*80}\n")
        
        # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶
        self._initialize_output_files()
        
        skipped_count = 0
        
        # --- æ›¿æ¢ ProcessPoolExecutor æ¶æ„ ---
        with Manager() as manager:
            task_queue = manager.Queue()
            
            # è¿›ç¨‹å®‰å…¨é” (ç”¨äºæ–‡ä»¶ I/O)
            self.file_lock = manager.Lock() 

            # 1. å¡«å……ä»»åŠ¡é˜Ÿåˆ—ï¼Œå¹¶å¤„ç†æ–­ç‚¹ç»­ä¼ 
            seeds_to_process = []
            for seed_idx, seed_data in enumerate(seeds, 1):
                source_id = _generate_source_id(seed_data, seed_idx)
                
                if source_id in self.processed_source_ids:
                    skipped_count += 1
                    # print(f"\nâ­ï¸  è·³è¿‡ Seed {seed_idx}/{len(seeds)} (å·²å¤„ç†: {source_id})")
                else:
                    seeds_to_process.append({
                        "seed_idx": seed_idx,
                        "seed_data": seed_data,
                        "source_id": source_id,
                    })

            if not seeds_to_process:
                print("\næ‰€æœ‰seedéƒ½å·²å¤„ç†ï¼Œæ— éœ€ç»§ç»­")
            
            total_tasks = len(seeds_to_process)
            
            # å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
            for task in seeds_to_process:
                task_queue.put(task)

            # 2. æ·»åŠ å“¨å…µå€¼ (Poison Pill)
            for _ in range(self.config.max_workers):
                task_queue.put(None)

            # 3. å¯åŠ¨ Worker è¿›ç¨‹
            processes = []
            for i in range(self.config.max_workers):
                worker_id = f"worker-{i+1}"
                
                # å¯åŠ¨ Worker è¿›ç¨‹å¹¶ä¼ å…¥å…±äº«èµ„æºå’Œæ–¹æ³•
                proc = Process(
                    target=run_synthesis_worker,
                    args=(
                        worker_id,
                        task_queue,
                        self.config,
                        self.file_lock,
                        self._save_qa_immediately, 
                        self._save_trajectories_immediately, 
                    )
                )
                proc.start()
                processes.append(proc)
                print(f"Started worker process: {worker_id}")
            
            # 4. ç­‰å¾… Worker è¿›ç¨‹å®Œæˆ (Join)
            try:
                for proc in processes:
                    proc.join()
            except KeyboardInterrupt:
                print("Main process interrupted. Terminating workers...")
                for proc in processes:
                    if proc.is_alive():
                        proc.terminate()
            
        # 5. æ¸…ç†ï¼ˆåŸä»£ç ä¸­ cleanup æ”¾åœ¨ finally å—ä¸­ï¼Œè¿™é‡Œä¿æŒä¸å˜ï¼‰
        # self.cleanup() # This is the final step outside the Manager block

        # Final statistics based on total tasks processed (approximation)
        newly_processed_count = total_tasks
        
        print(f"\n\n{'='*80}")
        print(f"ğŸ‰ æ•°æ®åˆæˆå®Œæˆ!")
        print(f"{'='*80}")
        print(f"æ€»Seedæ•°é‡: {len(seeds)} ä¸ª")
        print(f"å·²è·³è¿‡: {skipped_count} ä¸ª")
        print(f"æ–°å¤„ç†: {newly_processed_count} ä¸ª")
        print(f"{'='*80}\n")
        
        # è¿”å›ç©ºåˆ—è¡¨ï¼Œå…¼å®¹è°ƒç”¨è€…
        return []
    
    def save_results(self):
        """æ˜¾ç¤ºç»“æœä¿å­˜ä½ç½®ï¼ˆQAå¯¹å’Œtrajectorieså·²å®æ—¶ä¿å­˜ï¼‰"""
        if not self.qa_file_path:
            print("âš ï¸  è­¦å‘Š: æ²¡æœ‰è¿è¡Œè¿‡pipelineï¼Œæ— æ³•ä¿å­˜ç»“æœ")
            return
        
        print(f"ğŸ’¾ QAå¯¹å·²ä¿å­˜åˆ°: {self.qa_file_path}")
        print(f"ğŸ’¾ Trajectorieså·²ä¿å­˜åˆ°: {self.traj_file_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="é€šç”¨Agentæ•°æ®åˆæˆç³»ç»Ÿ")
    
    parser.add_argument("--config", type=str, required=True,
                       help="é…ç½®æ–‡ä»¶è·¯å¾„ (.json æˆ– .yaml)")
    parser.add_argument("--seeds", type=str, required=True,
                       help="Seedæ•°æ®JSONæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒä»»æ„ç±»å‹çš„seedï¼šentity/problem/text/urlç­‰ï¼‰")
    parser.add_argument("--output-dir", type=str, default="synthesis_results",
                       help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    if args.config.endswith('.json'):
        config = SynthesisConfig.from_json(args.config)
    elif args.config.endswith('.yaml') or args.config.endswith('.yml'):
        config = SynthesisConfig.from_yaml(args.config)
    else:
        raise ValueError("é…ç½®æ–‡ä»¶å¿…é¡»æ˜¯ .json æˆ– .yaml æ ¼å¼")
    
    # è¯»å–seedæ•°æ®ï¼ˆç®€å•å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
    print(f"è¯»å– seed æ•°æ®æ–‡ä»¶: {args.seeds}")
    with open(args.seeds, "r", encoding="utf-8") as f:
        seeds = json.load(f)
        if not isinstance(seeds, list):
            raise ValueError("Seedæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šå¿…é¡»æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¾‹å¦‚: [\"seed1\", \"seed2\", \"seed3\"]")
        if not all(isinstance(s, str) for s in seeds):
            raise ValueError("Seedæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šæ‰€æœ‰seedå¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    print(f"åŠ è½½äº† {len(seeds)} ä¸ª seed æ•°æ®")
    
    # åˆ›å»ºæ•°æ®åˆæˆç³»ç»Ÿ
    synthesizer = GenericDataSynthesis(config=config, output_dir=args.output_dir)
    
    # è¿è¡Œåˆæˆpipeline
    qas = synthesizer.run(seeds)
    
    # ä¿å­˜ç»“æœï¼ˆtrajectorieså’Œç»Ÿè®¡ä¿¡æ¯ï¼ŒQAå¯¹å·²å®æ—¶ä¿å­˜ï¼‰
    synthesizer.save_results()
    
    print(f"\nâœ… å…¨éƒ¨å®Œæˆ! å…±ç”Ÿæˆ {len(qas)} ä¸ªQAå¯¹")


if __name__ == "__main__":
    main()

