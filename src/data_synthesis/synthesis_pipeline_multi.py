"""
æ•°æ®åˆæˆä¸»Pipeline (Multi-Process / MCP Compatible)

æ•´åˆtrajectoryé‡‡æ ·ã€é€‰æ‹©å’ŒQAåˆæˆçš„å®Œæ•´æµç¨‹
å·²é€‚é… HttpMCPEnv, HttpMCPRagEnv, HttpMCPSearchEnvï¼Œå¹¶ä¿®å¤äº†ç¼ºå¤±æ¨¡å—çš„å¯¼å…¥é—®é¢˜ã€‚
"""

import json
import os
import hashlib
import sys
import time
from typing import List, Dict, Callable, Optional, Set, Any, Union
from multiprocessing import Process, Manager

# ================= ğŸ”§ æ–°å¢ä»£ç å¼€å§‹ =================
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶åˆ°ç¯å¢ƒå˜é‡
# verbose=True ä¼šåœ¨æ‰¾ä¸åˆ°æ–‡ä»¶æ—¶æ‰“å°è­¦å‘Š
# override=True ç¡®ä¿ .env ä¸­çš„å€¼è¦†ç›–ç³»ç»Ÿé»˜è®¤å€¼ï¼ˆå¯é€‰ï¼‰
load_dotenv(verbose=True, override=True)
# ================= ğŸ”§ æ–°å¢ä»£ç ç»“æŸ =================

# æ·»åŠ æºç è·¯å¾„åˆ° sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¼•å…¥åŸºç¡€æ¨¡å‹å’Œé…ç½®
from models import TrajectoryNode, Trajectory, SynthesizedQA, SynthesizedTask
from synthesis_config import SynthesisConfig
from trajectory_sampler import GenericTrajectorySampler
from trajectory_selector import GenericTrajectorySelector
from qa_synthesizer import GenericQASynthesizer
from task_synthesizer import OSWorldTaskSynthesizer

# å¼•å…¥ MCP ç¯å¢ƒç±» (ç›´æ¥ä»æ–‡ä»¶å¯¼å…¥ï¼Œé¿å¼€ envs/__init__.py ä¸­å¯èƒ½å­˜åœ¨çš„é”™è¯¯å¼•ç”¨)
try:
    from envs.http_mcp_env import HttpMCPEnv
    from envs.http_mcp_rag_env import HttpMCPRagEnv
    from envs.http_mcp_search_env import HttpMCPSearchEnv
except ImportError as e:
    print(f"âŒ Critical Error: MCP Environment files missing: {e}")
    sys.exit(1)


def _generate_source_id(seed_data: str, seed_idx: int) -> str:
    """ç”Ÿæˆsourceçš„å”¯ä¸€æ ‡è¯†"""
    content_hash = hashlib.md5(seed_data.encode('utf-8')).hexdigest()[:8]
    return f"src_{seed_idx:04d}_{content_hash}"


def _create_environment(config: SynthesisConfig, worker_id: Optional[str] = None) -> Union[HttpMCPEnv, HttpMCPRagEnv, HttpMCPSearchEnv]:
    """
    æ ¹æ®é…ç½®åˆ›å»ºç›¸åº”çš„ç¯å¢ƒã€‚
    
    ç­–ç•¥ï¼š
    ç”±äºæœ¬åœ°ç¼ºå¤± Math/Python/Web ç­‰åŸç”Ÿç¯å¢ƒä»£ç ï¼Œæˆ‘ä»¬å°†è¿™äº›æ¨¡å¼
    ç»Ÿä¸€æ˜ å°„åˆ°é€šç”¨çš„ HttpMCPEnvï¼Œä¾é  MCP Server ç«¯åŠ è½½å¯¹åº”å·¥å…·æ¥æä¾›èƒ½åŠ›ã€‚
    
    Args:
        config: åˆæˆé…ç½®
        worker_id: è¿›ç¨‹å”¯ä¸€ID (ç”¨äº MCP èµ„æºåˆ†é…å’Œæ—¥å¿—)
    """
    mode = config.environment_mode.lower()
    kwargs = config.environment_kwargs.copy()
    kwargs['model_name'] = config.model_name
    
    # å°† worker_id æ³¨å…¥åˆ° kwargs ä¸­ï¼Œä¾› MCP ç¯å¢ƒä½¿ç”¨
    if worker_id:
        kwargs['worker_id'] = worker_id
    
    # 1. RAG ä¸“ç”¨ç¯å¢ƒ
    if mode == "rag":
        return HttpMCPRagEnv(**kwargs)
        
    # 2. Search ä¸“ç”¨ç¯å¢ƒ
    elif mode == "search":
        return HttpMCPSearchEnv(**kwargs)
        
    # 3. é€šç”¨ MCP ç¯å¢ƒ (å¤„ç† Math, Python, Web, OSWorld ç­‰)
    elif mode in ["mcp", "http_mcp", "math", "python", "py", "web", "osworld", "gui"]:
        return HttpMCPEnv(**kwargs)
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç¯å¢ƒæ¨¡å¼: {mode} (ä¸”æœªæ‰¾åˆ°å¯¹åº”çš„ MCP æ˜ å°„)")


def run_synthesis_worker(
    worker_id: str,
    task_queue: Any,  # Manager.Queue() proxy object
    config: SynthesisConfig,
    file_lock: Any,  # Manager.Lock() proxy object
    qa_saver: Callable[[List[Dict]], None],
    traj_saver: Callable[[List[Dict]], None]
):
    """
    Worker è¿›ç¨‹å‡½æ•°ï¼šå¹¶è¡Œå¤„ç† Seedsï¼ŒåŒ…å« MCP èµ„æºç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚
    """
    print(f"\n[Worker {worker_id}] Starting up...")

    # 1. åˆå§‹åŒ–ç¯å¢ƒ
    try:
        environment = _create_environment(config, worker_id=worker_id)
    except Exception as e:
        print(f"[Worker {worker_id}] âŒ Failed to create environment: {e}")
        return

    # 2. ã€å…³é”®ä¿®æ”¹ã€‘å…ˆå¯åŠ¨ç¯å¢ƒè¿æ¥ï¼Œç¡®ä¿èƒ½è·å–å·¥å…·åˆ—è¡¨
    if hasattr(environment, "env_start") and callable(environment.env_start):
        try:
            environment.env_start()
            # print(f"[Worker {worker_id}] Connected to Gateway") # å¯é€‰æ—¥å¿—
        except Exception as e:
            print(f"[Worker {worker_id}] env_start() failed: {e}")

    # 3. ã€å…³é”®ä¿®æ”¹ã€‘ç¯å¢ƒè¿æ¥åå†åˆå§‹åŒ– Sampler
    sampler = GenericTrajectorySampler(
        environment=environment,
        config=config
    )
    
    selector = GenericTrajectorySelector(config=config)
    
    # åˆå§‹åŒ–åˆæˆå™¨
    if config.output_format == "task":
        synthesizer = OSWorldTaskSynthesizer(config=config)
    else:
        synthesizer = GenericQASynthesizer(config=config)

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¯ä»»åŠ¡èµ„æºåˆ†é… (Heavy Resource Check)
    # HttpMCPEnv é»˜è®¤ä¸º True, HttpMCPSearchEnv active_resources ä¸ºç©ºï¼Œallocate ä¼šå¿«é€Ÿè¿”å› True
    env_has_heavy_resource = bool(getattr(environment, "has_heavy_resource", False) and callable(getattr(environment, "allocate_resource", None)))

    # 2. ä¸»ä»»åŠ¡å¾ªç¯ (Pull Model)
    while True:
        try:
            # å°è¯•ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
            seed_task = task_queue.get(timeout=30) 
            
            if seed_task is None: # å“¨å…µå€¼
                print(f"[Worker {worker_id}] Received sentinel. Stopping loop.")
                break
        except Exception as e:
            # é˜Ÿåˆ—è¶…æ—¶æˆ–ç©º
            break

        seed_data = seed_task["seed_data"]
        seed_idx = seed_task["seed_idx"]
        source_id = _generate_source_id(seed_data, seed_idx)
        
        print(f"\n{'#'*60}")
        print(f"[Worker {worker_id}] START Seed {seed_idx}, Source ID: {source_id}")
        print(f"{'#'*60}\n")
        
        resource_allocated = False
        
        try:
            # --- æ˜¾å¼èµ„æºåˆ†é… (MCP Resource Lifecycle) ---
            if env_has_heavy_resource:
                # print(f"[Worker {worker_id}] ğŸ” Requesting resource via MCP...")
                if not environment.allocate_resource(worker_id):
                     raise RuntimeError(f"Failed to allocate resource via MCP for {worker_id}")
                resource_allocated = True
                # print(f"[Worker {worker_id}] âœ… Resource ready.")
            
            # Step 1: Trajectory Sampling
            # print(f"\nğŸ“Š æ­¥éª¤ 1/3: Trajectory Sampling")
            trajectory_tree = sampler.sample_trajectory_tree(seed_data)
            
            # Step 2: Trajectory Selection
            # print(f"\nğŸ¯ æ­¥éª¤ 2/3: Trajectory Selection")
            if not sampler.root_id:
                raise RuntimeError(f"[Worker {worker_id}] Sampler root_id is None after sampling")

            selected_trajectories = selector.select_trajectories(
                nodes=trajectory_tree,
                root_id=sampler.root_id,
                seed_data=seed_data,
                source_id=source_id,
                max_selected_traj=config.max_selected_traj
            )
            
            # Step 3: æ•°æ®åˆæˆï¼ˆQAæˆ–Taskï¼‰
            outputs = []
            output_type = "QAå¯¹" if config.output_format != "task" else "ä»»åŠ¡"
            
            # print(f"\nâœ¨ æ­¥éª¤ 3/3: {output_type} Synthesis")
            for qa_idx, trajectory in enumerate(selected_trajectories):
                try:
                    if config.output_format == "task":
                        # OSWorldTaskSynthesizer has synthesize_task method
                        if hasattr(synthesizer, 'synthesize_task'):
                            synthesized_output = synthesizer.synthesize_task(trajectory, qa_idx)  # type: ignore
                        else:
                            raise AttributeError(f"Synthesizer does not have 'synthesize_task' method")
                    else:
                        # GenericQASynthesizer has synthesize_qa method
                        if hasattr(synthesizer, 'synthesize_qa'):
                            synthesized_output = synthesizer.synthesize_qa(trajectory, qa_idx)  # type: ignore
                        else:
                            raise AttributeError(f"Synthesizer does not have 'synthesize_qa' method")

                    if synthesized_output:
                        outputs.append(synthesized_output.to_dict())
                except Exception as e:
                    print(f"[Worker {worker_id}] âŒ åˆæˆå¤±è´¥ (è½¨è¿¹ {qa_idx}): {str(e)}")
            
            trajectories_data = [traj.to_dict() for traj in selected_trajectories]
            
            print(f"[Worker {worker_id}] âœ… Seed {seed_idx} å®Œæˆ! ç”Ÿæˆ {len(outputs)} {output_type}")
            
            # --- å®æ—¶ä¿å­˜ç»“æœ (ä½¿ç”¨é”) ---
            if outputs:
                qa_saver(outputs) 
            if trajectories_data:
                traj_saver(trajectories_data) 
                
        except Exception as e:
            error_msg = f"[Worker {worker_id}] âŒ Seed {seed_idx} å¤±è´¥: {str(e)}"
            print(f"\n{error_msg}")
            # import traceback
            # traceback.print_exc()
            
        finally:
            # --- æ˜¾å¼èµ„æºé‡Šæ”¾ (MCP Resource Lifecycle) ---
            if env_has_heavy_resource and resource_allocated:
                # print(f"[Worker {worker_id}] â™»ï¸ Releasing resource...")
                try:
                    environment.release_resource(worker_id, reset=True)
                except Exception as e:
                    print(f"[Worker {worker_id}] âš ï¸ Error releasing resource: {e}")
            
    # Worker é€€å‡ºæ—¶å…³é—­ç¯å¢ƒè¿æ¥
    if hasattr(environment, "env_close") and callable(environment.env_close):
        environment.env_close()
    print(f"[Worker {worker_id}] Stopped.")


class GenericDataSynthesis:
    """
    é€šç”¨æ•°æ®åˆæˆä¸»ç±» - æ”¯æŒæ‰€æœ‰ç¯å¢ƒå’Œå·¥å…·
    """
    
    def __init__(self, config: SynthesisConfig, output_dir: str = "synthesis_results"):
        """
        åˆå§‹åŒ–é€šç”¨æ•°æ®åˆæˆç³»ç»Ÿ
        """
        self.config = config
        self.output_dir = output_dir
        
        # éªŒè¯é…ç½®
        errors = config.validate()
        if errors:
            raise ValueError(f"é…ç½®é”™è¯¯: {', '.join(errors)}")
        
        # åˆ›å»ºä¸»ç¯å¢ƒ (ç”¨äº Main Process è·å–å…ƒæ•°æ®/å·¥å…·åˆ—è¡¨)
        print(f"åˆå§‹åŒ– {config.environment_mode.upper()} Environment (Main Process)...")
        # ä¸»è¿›ç¨‹ä½¿ç”¨ "main" ä½œä¸º ID
        self.environment = _create_environment(config, worker_id="main")

        # å…ˆè¿æ¥ç¯å¢ƒï¼Œç¡®ä¿å·¥å…·åˆ—è¡¨å¯ç”¨
        if hasattr(self.environment, "env_start"):
            try:
                self.environment.env_start()
                print(f"âœ… Main Process å·²è¿æ¥åˆ° Gateway")
            except Exception as e:
                print(f"âš ï¸ Main Process è¿æ¥å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

        # åˆ›å»ºç»„ä»¶ (æ³¨æ„ï¼šSampler éœ€è¦ç¯å¢ƒå·²è¿æ¥æ‰èƒ½è·å–å·¥å…·åˆ—è¡¨)
        self.sampler = GenericTrajectorySampler(
            environment=self.environment,
            config=config
        )
        
        self.selector = GenericTrajectorySelector(config=config)
        
        if config.output_format == "task":
            self.synthesizer = OSWorldTaskSynthesizer(config=config)
            print(f"ä½¿ç”¨OSWorldä»»åŠ¡åˆæˆå™¨ï¼ˆè¾“å‡ºæ ¼å¼ï¼štaskï¼‰")
        else:
            self.synthesizer = GenericQASynthesizer(config=config)
            print(f"ä½¿ç”¨QAåˆæˆå™¨ï¼ˆè¾“å‡ºæ ¼å¼ï¼šqaï¼‰")
        
        self.qa_file_path = None
        self.traj_file_path = None
        self.processed_source_ids: Set[str] = set()
        self.file_lock: Optional[Any] = None  # Manager.Lock() proxy object
    
    def _initialize_output_files(self):
        """åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶è·¯å¾„å¹¶åˆ›å»ºè¾“å‡ºç›®å½•"""
        os.makedirs(self.output_dir, exist_ok=True)
        
        if self.config.output_format == "task":
            self.qa_file_path = os.path.join(
                self.output_dir, 
                f"synthesized_tasks_{self.config.environment_mode}.jsonl"
            )
        else:
            self.qa_file_path = os.path.join(
                self.output_dir, 
                f"synthesized_qa_{self.config.environment_mode}.jsonl"
            )
        
        self.traj_file_path = os.path.join(
            self.output_dir, 
            f"trajectories_{self.config.environment_mode}.jsonl"
        )
        
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {self.qa_file_path}")
        self._load_processed_source_ids()
    
    def _load_processed_source_ids(self):
        """ä»å·²æœ‰çš„è¾“å‡ºæ–‡ä»¶ä¸­åŠ è½½å·²å¤„ç†çš„source_id"""
        self.processed_source_ids.clear()

        if self.qa_file_path and os.path.exists(self.qa_file_path):
            try:
                with open(self.qa_file_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            qa_dict = json.loads(line)
                            if "source_id" in qa_dict:
                                self.processed_source_ids.add(qa_dict["source_id"])
                
                if self.processed_source_ids:
                    print(f"ğŸ”„ å‘ç° {len(self.processed_source_ids)} ä¸ªå·²å¤„ç†çš„sourceï¼Œå°†è·³è¿‡è¿™äº›seed")
            except Exception as e:
                print(f"âš ï¸  è¯»å–å·²å¤„ç†è®°å½•æ—¶å‡ºé”™: {e}")
                self.processed_source_ids.clear()
    
    def _save_qa_immediately(self, qas_dicts: List[Dict]):
        """ç«‹å³å°†QAå¯¹è¿½åŠ ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆè¿›ç¨‹å®‰å…¨ï¼‰"""
        if not self.file_lock or not self.qa_file_path:
            return
        with self.file_lock:
            with open(self.qa_file_path, "a", encoding="utf-8") as f:
                for qa_dict in qas_dicts:
                    f.write(json.dumps(qa_dict, ensure_ascii=False) + "\n")
    
    def _save_trajectories_immediately(self, trajectories_data: List[Dict]):
        """ç«‹å³å°†trajectoriesè¿½åŠ ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆè¿›ç¨‹å®‰å…¨ï¼‰"""
        if not self.file_lock or not self.traj_file_path:
            return
        with self.file_lock:
            with open(self.traj_file_path, "a", encoding="utf-8") as f:
                for traj in trajectories_data:
                    f.write(json.dumps(traj, ensure_ascii=False) + "\n")
    
    def run(self, seeds: List[str]) -> List[Dict]:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®åˆæˆpipelineï¼ˆä½¿ç”¨ Process/Queue æ¶æ„ï¼‰
        """
        if self.config.number_of_seed is not None:
            seeds = seeds[:self.config.number_of_seed]
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ é€šç”¨Agentæ•°æ®åˆæˆ Pipeline å¯åŠ¨")
        print(f"{'='*80}")
        print(f"ç¯å¢ƒæ¨¡å¼: {self.config.environment_mode}")
        print(f"æ€»Seedæ•°é‡: {len(seeds)}")
        print(f"å¹¶è¡Œåº¦: {self.config.max_workers} workers")
        
        # æ˜¾ç¤ºå¯ç”¨å·¥å…·åˆ—è¡¨
        try:
            tool_names = [t['name'] for t in self.sampler.available_tools]
            print(f"å¯ç”¨å·¥å…· ({len(tool_names)}): {tool_names[:5] if len(tool_names) > 5 else tool_names}...")
        except Exception as e:
            print(f"Warning: Failed to list tools (Non-fatal): {e}")

        # å…³é—­ä¸»è¿›ç¨‹çš„è¿æ¥ï¼ˆWorker ä¼šå»ºç«‹è‡ªå·±çš„è¿æ¥ï¼‰
        try:
            if hasattr(self.environment, "env_close"):
                self.environment.env_close()
                print(f"âœ… Main Process å·²æ–­å¼€è¿æ¥ï¼ˆWorker å°†å»ºç«‹ç‹¬ç«‹è¿æ¥ï¼‰")
        except Exception as e:
            print(f"âš ï¸ Main Process æ–­å¼€è¿æ¥å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

        print(f"{'='*80}\n")
        
        self._initialize_output_files()
        
        skipped_count = 0
        
        with Manager() as manager:
            task_queue = manager.Queue()
            self.file_lock = manager.Lock() 

            # 1. å¡«å……ä»»åŠ¡é˜Ÿåˆ—
            seeds_to_process = []
            for seed_idx, seed_data in enumerate(seeds, 1):
                source_id = _generate_source_id(seed_data, seed_idx)
                
                if source_id in self.processed_source_ids:
                    skipped_count += 1
                else:
                    seeds_to_process.append({
                        "seed_idx": seed_idx,
                        "seed_data": seed_data,
                        "source_id": source_id,
                    })

            if not seeds_to_process:
                print("\næ‰€æœ‰seedéƒ½å·²å¤„ç†ï¼Œæ— éœ€ç»§ç»­")
                return []
            
            total_tasks = len(seeds_to_process)
            
            for task in seeds_to_process:
                task_queue.put(task)

            # 2. æ·»åŠ å“¨å…µå€¼ (Poison Pills)
            for _ in range(self.config.max_workers):
                task_queue.put(None)

            # 3. å¯åŠ¨ Worker è¿›ç¨‹
            processes = []
            for i in range(self.config.max_workers):
                worker_id = f"worker-{i+1}"
                
                proc = Process(
                    target=run_synthesis_worker,
                    args=(
                        worker_id,
                        task_queue,
                        self.config,
                        self.file_lock,
                        self._save_qa_immediately, 
                        self._save_trajectories_immediately, 
                    )
                )
                proc.start()
                processes.append(proc)
                print(f"Started worker process: {worker_id}")
            
            # 4. ç­‰å¾… Worker è¿›ç¨‹å®Œæˆ
            try:
                for proc in processes:
                    proc.join()
            except KeyboardInterrupt:
                print("Main process interrupted. Terminating workers...")
                for proc in processes:
                    if proc.is_alive():
                        proc.terminate()
        
        print(f"\n\n{'='*80}")
        print(f"ğŸ‰ æ•°æ®åˆæˆå®Œæˆ!")
        print(f"{'='*80}")
        print(f"æ€»Seedæ•°é‡: {len(seeds)} ä¸ª")
        print(f"å·²è·³è¿‡: {skipped_count} ä¸ª")
        print(f"æ–°å¤„ç†: {total_tasks} ä¸ª")
        print(f"{'='*80}\n")
        
        return []
    
    def save_results(self):
        """æ˜¾ç¤ºç»“æœä¿å­˜ä½ç½®"""
        if not self.qa_file_path:
            return
        
        print(f"ğŸ’¾ QAå¯¹å·²ä¿å­˜åˆ°: {self.qa_file_path}")
        print(f"ğŸ’¾ Trajectorieså·²ä¿å­˜åˆ°: {self.traj_file_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="é€šç”¨Agentæ•°æ®åˆæˆç³»ç»Ÿ (å¹¶è¡Œç‰ˆ)")
    
    parser.add_argument("--config", type=str, required=True,
                       help="é…ç½®æ–‡ä»¶è·¯å¾„ (.json æˆ– .yaml)")
    parser.add_argument("--seeds", type=str, required=True,
                       help="Seedæ•°æ®JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output-dir", type=str, default="synthesis_results",
                       help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    print(f"åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    if args.config.endswith('.json'):
        config = SynthesisConfig.from_json(args.config)
    elif args.config.endswith('.yaml') or args.config.endswith('.yml'):
        config = SynthesisConfig.from_yaml(args.config)
    else:
        raise ValueError("é…ç½®æ–‡ä»¶å¿…é¡»æ˜¯ .json æˆ– .yaml æ ¼å¼")
    
    print(f"è¯»å– seed æ•°æ®æ–‡ä»¶: {args.seeds}")
    with open(args.seeds, "r", encoding="utf-8") as f:
        seeds = json.load(f)
    
    # å…¼å®¹å•ä¸ªå­—ç¬¦ä¸²è¾“å…¥
    if isinstance(seeds, str):
        seeds = [seeds]
    
    # ç¡®ä¿æ˜¯åˆ—è¡¨
    if not isinstance(seeds, list):
        raise ValueError("Seedæ–‡ä»¶æ ¼å¼é”™è¯¯")

    print(f"åŠ è½½äº† {len(seeds)} ä¸ª seed æ•°æ®")
    
    synthesizer = GenericDataSynthesis(config=config, output_dir=args.output_dir)
    synthesizer.run(seeds)
    synthesizer.save_results()
    
    print(f"\nâœ… å…¨éƒ¨å®Œæˆ!")


if __name__ == "__main__":
    main()
