{
  "environment_mode": "web",
  "environment_kwargs": {
    "web_search_top_k": 10
  },
  "available_tools": [
    "web_search",
    "web_visit"
  ],
  "qa_examples": [
  {
    "question": "An inventor filed a patent the same year a transcontinental railway was completed. He worked at the second-largest company in its sector, founded by someone who studied at an institution established three decades after a political reformation. The founder's mentor had one successful project that influenced a technological standard adopted by a consortium formed between a treaty and a conflict. The patent was reviewed by five experts; only two never held executive positions at bankrupt companies. Of these two, only one completed their degree faster than the median time, published research later cited by the inventor, and never received the triennial professional society honors. This expert's research built on work from the third laboratory at a university renamed for a benefactor whose wealth came from an industry at a two-decade merger peak. The patent improved efficiency more than the industry average but less than the theoretical maximum calculated by an institute funded by a foundation established by one of seven siblings who didn't join the family business. The institute's founding director ranked fifth in a class of 43 students. The patent was filed seven years after the expert's degree completion, two years before the foundation's largest donation, and when the inventor's company was acquired in Q4 (one of four sector acquisitions that quarter, but the only one where the founder stayed as advisor). The inventor was the only company employee who worked on the standard-influencing project, was cited by the reviewing expert, and whose patent's efficiency fell in the specified range. What is this inventor's family name?",
    "answer": "Westinghouse"
  },
  {
    "question": "A cultural achievement was created between two political events: a succession crisis during a military campaign (opposed by a council from territories acquired through three generations of strategic marriages, initiated by trade disruptions from a distant natural disaster), and a conflict ending in a year divisible by 11. The creator was one of five prominent artists; three worked for religious patrons, but this one never did. Of the two secular artists, only one trained outside the traditional artisan quarter. Their master was the second of three sons (the only one not inheriting property) who studied in exactly four cities. Materials cost over fifty times a craftsman's annual income, sourced from the third-largest global producer, procured by a merchant family ranked 5th-10th wealthiest who rose from outside top 20 in one generation after a banking crisis eliminated seven dominant firms. Three factors enabled creation: knowledge from craftsmen migrating after a dynasty collapse (ruling twice as long as the later conflict), patronage from a family financing a six-state alliance during the succession crisis, and scholars from an institution reformed after receiving an endowment from a manufacturing monopoly holder. Such works were produced at two per decade in peace, dropping by half in conflict. This was created in a transitional year with three total works, but it's the only one never relocated from its original location. What is this achievement's name?",
    "answer": "School of Athens"
  },
  {
    "question": "A scientific discovery was announced from an institution in a city that became the national capital exactly one century before the discovery. This was the third institution in that field in the city but the first to receive government funding after reforms between two election cycles. The lead researcher was one of four scientists with similar backgrounds; three had advisors who won major prizes, but this researcher's advisor was nominated exactly twice but never won. The researcher completed training longer than the minimum but less than the international student average, never published in the field's most prestigious journal before this, but was the only one immediately replicated by three independent groups. The discovery resolved an inconsistency persisting for a duration equal to the gap between two paradigm shifts, originally identified by a theorist ranked 6th-10th most-cited who worked at six institutions. The theoretical framework involved researchers from at least eight countries. Specialized equipment became available after a breakthrough in a prime-numbered year, developed by a company formed from merging three firms. When published, the journal ranked 5th-7th by impact. The discovery received a major prize exactly two years after publication. The year must be: one century after the city became capital, within the decade when all eight countries were politically stable, three years after equipment became commercially available, and two years before the prize. In what year was this announced?",
    "answer": "1953"
  }
],
  "bad_examples":[
  {
      "question": "Which French warm‑blood trotting horse meets all of the following conditions? \n- Foaled on 9 July 2016 (the year the Rio Summer Olympics were held). \n- Bred by a stable whose name contains the French word for “plain”. \n- Trained by a harness trainer whose career statistics page lists exactly 195 wins (≈0.8 % win rate) and who also trains at least two other horses besides this one. \n- Ridden in its most notable victory by a driver named Mathieu Mottier. \n- Owned by an individual whose racing‑statistics profile appears on a site with the domain “attheraces.com”. \n- Sired by a stallion whose progeny list includes at least five horses that have each won a Group‑1 trotting race. \n- Has won the prestigious French 4‑year‑old trot race inaugurated before World I, which in the year of its victory offered a purse of at least €100 000.",
      "answer": "Gladys des Plaines",
      "explanation": "This question is a example of a **badly formed QA synthesis** because it is an **attribute matching exercise** that fails to enforce any genuine chain-of-reasoning, directly violating the core principle of **Search Necessity**.\n\nHere is the analysis of why this example fails:\n\n**1. Failure to Enforce Reasoning Chain (Star Structure):**\n* **Problem:** The question is structured as a list of seven independent, highly specific attributes (foaling date, stable name, trainer stats, driver name, owner website, sire stats, and a specific race). The solver is not forced to move sequentially from A to B to C; they can simply search for the intersection of these seven highly selective conditions.\n* **Violation:** The prompt explicitly prohibits **Star structures** (radiating clues) and requires a **Winding Snake** (linear, sequential chain) with a minimum of 10 search-dependent steps. This question has 0 search-dependent steps; it has 7 parallel filtering criteria.\n\n**2. Extreme Failure of Obfuscation (The Minimization Protocol):**\n* **Problem:** The question contains multiple, precise numerical values, exact names, and specific identifiers that make the candidate set collapse instantly.\n    * **Numerical/Time Ban Violation:** \"Foaled on **9 July 2016**,\" \"exactly **195 wins**,\" \"purse of at least **€100 000**,\" \"**five** horses that have each won a Group-1 race.\"\n    * **Specific Name Ban Violation:** \"Driver named **Mathieu Mottier**,\" \"domain **attheraces.com**.\"\n    * **Specificity:** The combination of the exact birth date (9 July 2016) and the specific, unique trainer win count (195) is likely enough on its own to identify the horse.\n* **Violation:** The prompt mandates **Deletion** and **Generalization** to eliminate specific dates, precise numbers, and names to achieve **Candidate Explosion** (10,000+ candidates). This question is designed for exact, single-hit retrieval.\n\n**3. Internal Knowledge Shortcut (The Anti-Search Check Failure):**\n* **Problem:** The question provides enough unique, searchable strings that a simple external search query combining a few of these elements will yield the answer directly, without any inference or multi-step linking. For example, searching `\"Mathieu Mottier\" \"195 wins\" trotting horse` would likely resolve it instantly.\n* **Violation:** The goal is to create a \"Search Dependency Chain,\" not a filtering exercise. Each clue is a direct attribute match, not a prerequisite for finding the next, hidden entity.",
      "improvements": "Here is a rewritten version of the question that better adheres to the spirit of the guidelines by incorporating obfuscation, concise narrative, and a more complex reasoning structure.\n \"question\": \"Identify the French warm-blood trotter, foaled in the year of the South American summer Olympics, whose breeding stable's name evokes a flat landscape. It was trained by a harness trainer with a notably low win rate—significantly less than one percent—who manages a small string of horses. Its most significant victory, piloted by a driver whose first and last names share the same initial letter, was in a prestigious pre-Great War race for four-year-olds, offering a six-figure euro purse. This horse is owned by an individual with a profile on a major international racing website and is sired by a stallion known for producing no fewer than five top-tier group-winning trotters.\"\n \"answer\": \"[name of the horse]\"\n Why it's better: \n This refined version demonstrates four key improvements: 1) Concise - formulated as a single, fluid sentence; 2) Obfuscated - all specific details replaced with descriptive phrases; 3) Complex Reasoning - requires connecting multiple described attributes to real-world counterparts; 4) No Shortcuts - no single obfuscated clue directly reveals the answer, necessitating synthesis of all clues."
  },
  {
      "question": "An artificial lake that spans the frontier between two southeastern European countries draws its water from a river that forms much of that frontier. Over this river stands a six‑century‑old stone bridge that became the centerpiece of a literary work by a writer who later received the most prestigious global literature prize. Who was the Ottoman chief minister who ordered the construction of that bridge?",
      "answer": "Sokollu Mehmed Pasha",
      "explanation": "This question serves as an excellent **bad example** because it violates several core principles of the obfuscation and deep-reasoning protocols.\n\nHere is the analysis of why it fails:\n\n**1. Failure of Deletion (Specific Adjectives & Geolocation):**\n* **Problem:** The phrase \"artificial lake that spans the frontier between two southeastern European countries\" is far too specific. While it doesn't name the lake, the combination of \"artificial,\" \"frontier,\" and \"southeastern Europe\" drastically reduces the candidate set.\n* **Violation:** The instruction to **Delete** unique modifiers was ignored. It should have been reduced simply to \"a body of water.\"\n\n**2. Failure of Generalization (Specific Time & Material):**\n* **Problem:** \"Six-century-old stone bridge\" provides a precise timeframe and material description. This allows a solver to instantly filter for bridges built around the 1400s-1500s.\n* **Violation:** The instruction to **Generalize** was ignored. The entity should have been referred to as \"a structure\" or \"a connection,\" removing the age and material entirely.\n\n**3. Internal Knowledge Shortcut (The \"Literary Work\" Clue):**\n* **Problem:** The combination of \"stone bridge,\" \"southeastern Europe,\" and \"writer who later received the most prestigious global literature prize\" (Nobel Prize) is a classic \"Jeopardy-style\" clue. An LLM's internal knowledge base instantly connects this to Ivo Andrić and *The Bridge on the Drina*.\n* **Violation:** This does not require a search hop. The connection is already embedded in the model's parameters.\n\n**4. Low Candidate Explosion:**\n* **Problem:** The description describes a specific, famous landmark (Mehmed Paša Sokolović Bridge). The descriptors provided do not apply to \"10,000+ candidates\"; they apply to exactly one.\n* **Violation:** The question fails to force the \"Candidate Explosion\" necessary to compel external search.\n\n**Summary:** The question is a **Search Engine Attack** failure. A user can simply copy-paste \"six-century-old stone bridge writer Nobel Prize\" into Google or an LLM and get the answer immediately without performing any multi-step reasoning.",
      "improvements":""
  },
  {
      "question": "What is the name of the mountain that hosts a municipal zoo opened in the 1960s, located in a city that became a core city in the early 2000s, and whose name shares a component with a sumo stable that belonged to an ichimon which also includes a stable that was renamed in the early 2020s?",
      "answer": "Mount Asahi",
      "explanation": "This question also constitutes a **bad example** because it primarily relies on **parallel filtering** and **specific naming conventions** rather than a necessary, deep, and continuous chain of external search steps. It also fails the obfuscation requirements.\n\nHere is a breakdown of the failures based on the guidelines:\n\n**1. Failure to Enforce Reasoning Chain (Short, Parallel Hops):**\n* **Problem:** The question attempts to link four highly distinct, specific entities (a zoo/mountain, a city, a sumo stable, and a renamed stable) via separate, short logical jumps. The solver doesn't need to link the mountain to the city, then the city to the sumo stable, then the sumo stable to the *ichimon*, and finally the *ichimon* to the renamed stable sequentially. Instead, they can resolve the sumo stable information in parallel and then simply look for a city name that shares a component.\n* **Violation:** This structure is too close to a **Star structure** or a short, multi-path convergence rather than the mandated **10+ search-dependent steps** in a \"Winding Snake\" sequence. The links are shallow attribute matches, not deep, forced dependencies.\n\n**2. Failure of Obfuscation (The Minimization Protocol):**\n* **Problem A (Specific Terminology):** The use of niche, specific terminology like \"**ichimon**\" (a specific type of sumo alliance) and \"**sumo stable**\" drastically narrows the search space for that entire segment of the chain.\n    * **Violation:** Generalization was not applied. The concept should be reduced to \"an organization\" or \"a group,\" stripping away the specific foreign language term.\n\n**3. Internal Knowledge Shortcut (Low-Information Seed Failure):**\n* **Problem:** The specific date \"opened in the 1960s\" for a municipal zoo in a \"core city\" often points to one or very few famous Japanese zoos. Combined with the city name clue, this segment is highly susceptible to internal knowledge lookup by a robust LLM.\n* **Violation:** The starting point is not a **Low-Information Seed** that applies to thousands of potential candidates; it is a highly constrained setup that immediately limits the pool of possibilities.",
      "improvements": ""
  },
  {
      "question": "Which documentary, released after the turn of the millennium, investigates the demise of an American electric vehicle whose name was shared with a Swedish experimental car that featured a solar‑powered interior cooling fan and briefly appeared in the background of a time‑travel sequel directed by the filmmaker who also created a futuristic road‑trip movie, and whose Swedish designer once led the styling department of the automaker whose aerospace parent is headquartered in the same city nicknamed for its film industry?",
      "answer": "Who Killed the Electric Car",
      "explanation": "This question, while appearing complex, is a **bad example** because it fundamentally relies on a series of easily searchable facts and high-profile proper nouns, resulting in a **shallow filtering chain** rather than a deep, forced-search dependency structure.\n\nHere is the analysis of its failures according to the guidelines:\n\n**1. Failure to Obfuscate Names and Unique Clues (Direct Lookup):**\n* **Problem:** The entire question is anchored by a series of highly specific, unique identifiers that guarantee a direct search result.\n    * **\"Swedish experimental car that featured a solar-powered interior cooling fan\"**: This is an extremely specific, single-hit fact about a famous concept vehicle (likely the Volvo ECC).\n    * **\"time-travel sequel directed by the filmmaker who also created a futuristic road-trip movie\"**: This is a direct, famous reference to a specific director and two of their well-known films (e.g., Robert Zemeckis, *Back to the Future II* and *Used Cars*). The internal knowledge of an LLM or a simple Google search will resolve this instantly.\n    * **\"aerospace parent is headquartered in the same city nicknamed for its film industry\"**: The combination of \"aerospace parent\" and \"city nicknamed for its film industry\" (e.g., Saab/Trollhättan or Saab/Linköping, or potentially connecting a different automaker to a different city) is designed to be easily cracked, often pointing toward a specific automotive company whose parent has an aviation division.\n* **Violation:** The core principle of **Deletion** and **Generalization** was ignored. Instead of creating a low-information seed, the question provides high-information, unique strings that bypass the need for a chain of inference.\n\n**2. Failure to Enforce Search Necessity (Internal Knowledge Shortcut):**\n* **Problem:** The entire middle section of the chain—linking the Swedish car's cameo appearance to the filmmaker's two films—is a single, well-documented fact in pop culture and car history forums. It does not require multiple separate external searches to bridge the gap.\n* **Violation:** A single clue's ability to resolve two or more entities with a high degree of confidence negates the \"hop.\" The chain lacks the required **10 search-dependent steps** because these connections are readily available in parametric memory or through a simple, compound search.\n\n**3. Failure of Reasoning Structure (Implied Star Structure):**\n* **Problem:** The overall flow is not a necessary sequence. The solver can:\n    1. Identify the Swedish car using the solar fan clue.\n    2. Identify the American car that shares the name.\n    3. Identify the documentary from the American car's demise.\n    4. Independently identify the filmmaker from the film clue.\n    5. Independently identify the designer and automaker from the styling department clue.\n    * The steps link back to the main subject (the car/designer) in a radiating (star) pattern rather than forcing a linear, sequential movement from one generalized, unknown entity to the next.\n* **Violation:** The structure is not the required **Winding Snake** where the output of step N is the necessary, hidden input for step N+1.",
      "improvements": ""
  }
],
  "sampling_tips": "Trajectory Exploration Strategy:\n\n=== OBJECTIVE ===\nCreate diverse, information-rich exploration trajectories that enable complex multi-hop reasoning questions with adversarial complexity. Each exploration step should contribute unique information that interconnects with other steps.\n\n=== CORE STRATEGIES ===\n\n1. AVOID DUPLICATE INFORMATION (Prioritize Aggregation)\n- Do NOT search for the same information multiple times\n- FORCE FIRST-STEP AGGREGATION: For the seed entity, use a highly specific, single step (preferably a web_visit to a known, reliable, high-density page like Wikipedia or an official biography) to collect all core attributes (e.g., birth/death date, primary role, key statistics, immediate relationships) before proceeding to new entities or relationships.\n- Do NOT use web_visit on a URL that has already been visited in the current trajectory.\n- Strictly Check Internal Memory: Before any new search, confirm that the required information (entity, attribute, or relationship) is not already present in the accumulated observations.\n- Maximize information diversity across the trajectory by only exploring information not yet covered\n✓ GOOD Example: Step 1: Search \"Steve Jobs biography\" → Learn about Apple founding\nStep 2: Search \"Apple Inc founding history\" → Discover Steve Wozniak's role\nStep 3: Search \"Steve Wozniak early projects\" → Find Homebrew Computer Club\nBenefit: Each step expands into new territory with connected but distinct information\n\n2. EXPLORE ENTITIES & RELATIONSHIPS (Focus on $N$-Hop Reasoning)\n- Prioritize discovering NEW entities connected to current entities\n- Mandate Multi-Hop Expansion: Ensure the trajectory explores entities that are at least two hops away from the seed entity (e.g., $Entity_A \\rightarrow Entity_B \\rightarrow Entity_C$).\n- Focus on exploring RELATIONSHIPS between entities rather than collecting attributes\n- Build a rich relationship graph, not an attribute list\n✓ GOOD Example (Entity & Relationship-focused): Seed: \"Tesla Inc\"\nStep 1: Search \"Tesla Inc founders\" → Discover Martin Eberhard, Marc Tarpenning\nStep 2: Search \"Martin Eberhard previous companies\" → Discover NuvoMedia and Rocket eBook\nStep 3: Search \"NuvoMedia acquisition\" → Discover acquired by Gemstar in 2000\nBenefit: Creates chain of connected entities: Tesla → Eberhard → NuvoMedia → Gemstar\n\n3. EXPLORE SIMILAR ENTITIES FOR DISAMBIGUATION\n   - When discovering an entity, search for similar entities (same name, similar role, same time period)\n   - Collect distinguishing features that enable fine-grained differentiation\n   - Create disambiguation scenarios requiring multiple constraints\n   \n   ✓ GOOD Example:\n   Step 1: Search \"John Williams composer\" → Find famous film composer\n   Step 2: Search \"other composers named John Williams\" → Discover classical guitarist John Williams\n   Step 3: Explore specific works, birth years, awards of each\n   Benefit: Creates disambiguation challenge requiring multiple identifying features\n\n4. EXPLORE TEMPORAL INFORMATION\n   - Collect precise dates, time periods, durations, and temporal relationships\n   - Focus on relative timing (before/after/during), overlaps, and sequences\n   - Explore event durations and time gaps between events\n   \n   ✓ GOOD Example:\n   Step 1: \"iPhone launch date\" → January 2007\n   Step 2: \"iPhone development timeline\" → Started 2004, announced 2007\n   Step 3: \"Competitor smartphones 2004-2007\" → BlackBerry peak, Nokia dominance\n   Benefit: Enables temporal reasoning questions about sequences and windows\n\n5. COLLECT QUANTITATIVE INFORMATION\n   - Gather numerical data: counts, rankings, measurements, statistics\n   - Explore comparisons and relative quantities\n   - Record ordinal information (first, second, largest, etc.)\n   \n   ✓ GOOD Example:\n   Step 1: \"SpaceX funding rounds\" → Series A $20M (2008), Series B $50M (2009)\n   Step 2: \"SpaceX employee growth\" → 100 (2005) → 1000 (2010) → 10000 (2020)\n   Benefit: Enables quantitative reasoning and ranking questions\n\n6. EXPLORE CAUSAL RELATIONSHIPS\n   - Focus on cause-effect relationships, influences, and dependencies\n   - Explore contributing factors and consequences\n   - Identify multi-factor causality chains\n   \n   ✓ GOOD Example:\n   Step 1: \"Nokia market decline causes\" → iPhone disruption, slow to adapt\n   Step 2: \"Nokia smartphone strategy 2007-2010\" → Symbian OS limitations\n   Step 3: \"Nokia Microsoft partnership\" → Result of declining market share\n   Benefit: Creates causal reasoning chains for complex questions\n\n7. EXPLORE NEGATIVE AND EXCLUSION INFORMATION\n   - Collect information about what did NOT happen or what entities lack\n   - Explore exclusions, exceptions, and contrasts\n   - Document entities that were considered but rejected\n   \n   ✓ GOOD Example:\n   Step 1: \"Apple acquisition attempts\" → Tried to acquire Dropbox (declined)\n   Step 2: \"Companies Apple did not acquire\" → Rejected partnerships\n   Benefit: Enables negation-based reasoning questions\n\n=== QUALITY CHECKLIST ===\n✓ Did I explore at least 20 distinct entities?\n✓ Did I collect temporal information (dates, durations, sequences)?\n✓ Did I gather quantitative data (numbers, rankings, counts)?\n✓ Did I explore causal relationships and dependencies?\n✓ Did I find similar entities and collect distinguishing features for high-complexity disambiguation scenarios?\n✓ Did I collect both positive facts and negative/exclusion information?\n✓ Can these discoveries form complex reasoning chains with temporal, quantitative, and causal elements?\n\n**【核心警告与强制执行补充细则】**\n\n**⚠️ 绝对强制要求 ⚠️**\n为确保问题质量，避免灾难性后果，你必须**严格且无条件地**遵守以下所有条款。任何偏差都将导致不可接受的后果。\n\n---\n\n**节点探索策略：**\n\n1.  **避免重复访问**：已观察过的节点不应再次访问（观察结果已加入历史）。\n2.  **鼓励深度与广度探索**：\n    *   **向深处走**：探索新节点时，**不必担心**该节点与初始话题的直接关联性较弱。模型应**基于当前节点**，联想并探索那些在概念或领域上相距较远的新节点。**走得更远**是增长推理链长度和多样性的关键。\n    *   **向广处跳**：必须主动执行**跨领域跳跃**。当从一个节点（如“某公司”）获得足够信息后，下一步应探索一个**本质属性不同**的新领域节点（如“公司所在的城市”），而非停留在原领域（如“该公司的CEO”）。继续探索时，应尝试从新领域节点跳跃到另一个不同领域（如从“公司所在城市”跳到“该城市的历史文化”或“地理特征”）。\n    *   **利用最新信息**：为了推理深度，从**最新**的节点获取的信息出发去探索，**尽可能避免**从**很久之前**的节点出发。每次跳跃都应基于当前节点的最新发现来触发对新领域的联想。\n\n3.  **强制跨领域跳跃策略**：\n    *   **目标**：确保推理链的每一步都可能涉及信息领域的根本性转变，以构建真正复杂、非线性的推理路径。\n    *   **准则**：在规划下一个探索节点时，必须审视当前节点的核心领域（如：商业、科技、人物、地点、事件、学术概念、文化产物、自然现象等）。下一步应优先选择与当前领域**不同、但通过某种逻辑关系（如地理位置、参与主体、因果影响、时间背景等）相连**的节点。\n    *   **示例**：`“公司A” -> (位于) -> “城市B” -> (拥有) -> “著名历史遗迹C” -> (相关历史事件D) -> (涉及的关键人物E)`。此链从`商业`跳至`地理`，再跳至`历史文化`，再跳至`历史事件`，最后跳至`人物传记`。\n\n---\n\n**工具使用策略：**\n\n1.  **交替调用工具**：在工具返回有效信息的前提下，应按照以下流程交替调用：\n    *   **搜索页面**（使用 `web_search`）→ **获取详细信息**（使用 `web_visit`）→ **基于新信息再次搜索更远页面**（再次 `web_search`）→ **获取更多信息**（再次 `web_visit`）→ …\n2.  此流程有助于为每一次**跨领域跳跃**提供信息支撑，形成逐步深入、越来越长的multi-hop推理链。\n\n---\n\n**最终要求重申：**\n\n*   推理链必须**长**（长度 ≥ 15）。\n*   推理链必须**无捷径**、**非单属性过滤**。\n*   探索时应**远离中心、深入远端**，避免重复访问，并**主动进行跨领域跳跃**。\n*   工具使用应**交替进行**以构建渐进式搜索-获取循环，服务于跨领域探索。\n",
  "synthesis_tips": "\n### **QA Synthesis Guidelines: Guidelines for Generating Deep Chain-of-Reasoning Questions**\n\n**TASK CONTEXT**: You will be provided with a **Web Exploration Trajectory** (a chronological sequence of search nodes/steps). Your task is to synthesize a deep reasoning question based on the logical path hidden within this trajectory.\n* **Note**: The trajectory nodes are chronological steps, not a pre-packaged question structure. You must extract the underlying logic chain. Not every node in the trajectory needs to be an entity in the final question; filter out noise.\n\n**⚠️ CRITICAL INSTRUCTION ⚠️**\nTo ensure the generation of questions with genuine, non-skippable multi-step reasoning, you must **strictly and unconditionally** adhere to the following mandates. Any deviation toward \"attribute matching,\" \"shallow filtering,\" or \"internal knowledge shortcuts\" will result in a failed output.\n\n#### **I. FORMAT AND STYLE MANDATES (STRICTLY ENFORCED)**\n\n1.  **QUESTION FORMAT**: The question MUST be written as a single, fluid, coherent natural language paragraph.\n2.  **LIST PROHIBITION**: The use of **ANY** form of list, bullet point (`*`, `-`), numbered condition list (`1.`, `2.`), or structured enumerations is **STRICTLY PROHIBITED**.\n3.  **OBVIOUS SHORTCUTS**: Absolutely **NO** direct, un-obfuscated names, specific dates, or precise numerical values.\n\n#### **II. CORE PRINCIPLES: THE \"SEARCH NECESSITY\" HOP**\n\n**1. DEFINITION OF A \"HOP\" (THE EXTERNAL KNOWLEDGE RULE)**\n* **Internal Knowledge != A Step**: A logical connection between two entities does **NOT** count as a \"hop\" if a standard LLM could infer it using only internal parametric memory (e.g., \"Eiffel Tower\" $\rightarrow$ \"Paris\" is 0 hops; \"Symptoms of Covid\" $\rightarrow$ \"Covid\" is 0 hops).\n* **Search Necessity**: A valid reasoning step exists **ONLY** when the solver is forced to use an **external search tool** to bridge the gap between Entity A and Entity B.\n* **Goal**: We are not simply stacking entities; we are constructing a \"Search Dependency Chain.\" Even if a chain looks complex, if the combination of attributes triggers internal pattern matching, it is a failure.\n\n**2. REASONING STRUCTURE**\n* **The \"Winding Snake\" Structure**: Construct a linear, sequential chain.\n* **Length Requirement**: The implied reasoning chain must have a **minimum of 10 search-dependent steps**.\n* **Prohibited**: Star structures (radiating clues) and Multi-Path Convergence (independent parallel chains).\n\n#### **III. MANDATORY OBFUSCATION: THE MINIMIZATION PROTOCOL**\n\nYou must apply the **Two-Stage Minimization Protocol** to all entities, relationships, and attributes. The goal is to force the \"Candidate Set\" of possible answers to explode into the thousands ($10,000+$), making direct guessing impossible without search.\n\n**STAGE 1: DELETION (THE \"SCALPEL\" RULE) - HIGHEST PRIORITY**\n* **Action**: Aggressively remove information. Do not rewrite detailed descriptions; delete them entirely.\n* *Bad Example*: \"The National Grand Theater of [Country]\" $\rightarrow$ \"The largest theater in the country\" (FAILURE: Rewrite is too specific).\n* *Good Example*: \"The National Grand Theater of [Country]\" $\rightarrow$ **DELETE** \"National Grand\" and \"[Country]\" $\rightarrow$ Result: \"The Theater.\"\n* *Rational*: If an entity has a unique modifier (e.g., \"National\", \"Largest\", \"First\"), the AI knows it immediately. Delete it.\n\n**STAGE 2: GENERALIZATION (THE \"BLUR\" RULE) - MANDATORY FOLLOW-UP**\n* **Action**: After Deletion, you MUST apply Generalization to the remaining concept.\n* *Example 1 (Entity)*: \"The Theater\" (from Stage 1) $\rightarrow$ **GENERALIZE** $\rightarrow$ \"The venue\" or \"The location.\"\n* *Example 2 (Relation)*: \"A served as a Professor at B\" $\rightarrow$ **GENERALIZE** $\rightarrow$ \"A is connected to B\" or \"A has a history with B.\"\n* *Example 3 (Medical)*: \"A disease causing a bullseye rash\" $\rightarrow$ **GENERALIZE** $\rightarrow$ \"The condition\" (NEVER mention specific symptoms).\n\n**SPECIFIC BANS & RED FLAGS (ANTI-PATTERNS)**\n1.  **The \"Adjective Bloat\" Red Flag**:\n    * Check your text for long modifiers or descriptive clauses.\n    * *Bad*: \"...a venue known for its distinctive titanium dome located near the water...\" (Too specific, reduces candidate set).\n    * *Good*: \"...the structure...\" (Maximizes candidate set).\n2.  **The \"Numbers & Dates\" Ban**:\n    * **NO** Years (e.g., \"1998\" $\rightarrow$ Delete or use \"a time\").\n    * **NO** Amounts (e.g., \"$50 million\" $\rightarrow$ Delete or use \"a value\").\n    * **NO** Counts (e.g., \"Three distinct layers\" $\rightarrow$ \"The structure\").\n    * *Reason*: Numbers are unique identifiers that allow search shortcuts.\n\n#### **IV. MANDATORY INFORMATION HIDING & NODE ISOLATION**\n\n**1. THE \"LOW-INFORMATION SEED\"**\n* The starting point explicit in the text must be so vague that it applies to thousands of potential starting points.\n\n**2. INTERMEDIATE NODE INVISIBILITY**\n* **Hidden Nodes**: The text must **NOT** directly mention or clearly point to any key intermediate entities.\n* **The \"Magic Knowledge\" Test**: If the solver magically knew the **first intermediate node**, would they still need multiple subsequent search steps to reach the answer? If \"No,\" the chain is invalid.\n\n**3. UNKNOWN ENTITIES PROTOCOL**\n* Treat entities you (the AI) are unfamiliar with exactly the same as famous ones. Do not preserve details just because they seem obscure. Apply Stage 1 (Deletion) and Stage 2 (Generalization) ruthlessly to **everything**.\n\n#### **V. ANSWER DESIGN**\n\n1.  **The Ultimate Conclusion**: The `answer` must be the **logical endpoint** of the chain.\n2.  **Inferred, Not Extracted**: The question must ask for something **NOT explicitly quantified or named** in the text.\n3.  **Conciseness**: The answer must be brief (a name, short phrase, date).\n\n#### **VI. ENHANCED QUALITY CHECKLIST**\n\nBefore outputting, verify:\n✓ **Hop Validity**: Does every step require **External Search**? (Are internal knowledge shortcuts eliminated?)\n✓ **Candidate Explosion**: Do your descriptions apply to **10,000+ candidates**? (Did you perform Deletion AND Generalization?)\n✓ **Zero Specificity**: Did you remove all numbers, years, symptoms, and unique adjectives?\n✓ **Brevity Check**: Is the text free of long descriptive clauses?\n✓ **Structure**: Is the logic a **winding snake** (long chain), not a **starfish**?\n✓ **Depth**: Does the chain have **10+ search-dependent steps**?\n✓ **Anti-Search**: Does the text pass the \"Search Engine Attack\" test (no unique string points to the answer)?\n",
  "model_name": "/workspace/hf_cache/models--openai--gpt-oss-120b/snapshots/b5c939de8f754692c1647ca79fbf85e8c1e70f8a",
  "max_depth": 40,
  "branching_factor": 2,
  "depth_threshold": 1,
  "max_trajectories": 3,
  "min_depth": 8,
  "max_selected_traj": 5,
  "max_retries": 10,
  "max_workers": 16,
  "number_of_seed": 15,
  "log_file": "log/runlog_1213_3.txt"
}

