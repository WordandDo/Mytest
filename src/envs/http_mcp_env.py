# src/envs/http_mcp_env.py
import sys
import os
import json
import logging
import asyncio
import time
from typing import Dict, Any, Union, Optional, List, Tuple
from datetime import datetime

# ä¿æŒåŸæœ‰å¼•ç”¨
from tools.tool import Tool
from envs.data_models import Observation, TrajectoryStep, TaskTrajectory
from prompts.system_prompts import get_system_prompt as load_system_prompt

# å¼•å…¥ MCP SDK
from mcp.types import CallToolResult
# å¼•å…¥æ–°çš„ MCP SSE å®¢æˆ·ç«¯
from utils.mcp_sse_client import MCPSSEClient

# [æ–°å¢] å¼•å…¥ä»»åŠ¡è¶…æ—¶ç›‘æ§å·¥å…·
from utils.task_timeout import TaskTimeoutMonitor, TaskTimeoutError, check_execution_timeout

import openai
from openai.types.chat import ChatCompletionMessageParam, ChatCompletionToolParam

logger = logging.getLogger(__name__)


class HttpMCPEnv:
    """
    é…ç½®é©±åŠ¨çš„ MCP ç¯å¢ƒé€‚é…å™¨ (ç‹¬ç«‹è§£è€¦ç‰ˆ)
    
    ä¸å†ç»§æ‰¿ Environment åŸºç±»ï¼Œé›†æˆäº†æ‰€æœ‰å¿…è¦çš„ Agent æ‰§è¡Œä¸èµ„æºç®¡ç†é€»è¾‘ã€‚
    """
    
    # å¼€å¯é‡å‹èµ„æºæ¨¡å¼ï¼Œé€šçŸ¥æ¡†æ¶åœ¨ run_task å‰åè°ƒç”¨ allocate/release
    has_heavy_resource = True 

    def __init__(self,
                 model_name: str = "gpt-4.1-2025-04-14",
                 parallel_degree=1,
                 **kwargs):
        
        # --- åŸ Environment.__init__ çš„é€»è¾‘ ---
        self.model_name = model_name
        self.config = kwargs
        
        # å·¥å…·ç®¡ç†
        self.tools: Dict[str, Tool] = {}
        self.tool_schemas: List[Dict[str, Any]] = []
        self.tool_descriptions: str = ""

        # --- åŸ HttpMCPEnv.__init__ çš„é€»è¾‘ ---
        
        # 1. åŸºç¡€é…ç½®
        self.server_url = kwargs.get("mcp_server_url", "http://localhost:8000")
        self.config_name = "default"
        
        # 2. è·å– worker_id
        if "worker_id" in kwargs:
            self.worker_id = kwargs["worker_id"]
        else:
            import multiprocessing
            self.worker_id = multiprocessing.current_process().name

        # 3. å®ä¾‹åŒ– MCP å®¢æˆ·ç«¯
        self.mcp_client = MCPSSEClient(f"{self.server_url}/sse")

        # 4. åŠ è½½ Gateway é…ç½®
        config_path = kwargs.get("gateway_config_path", "gateway_config.json")
        self.modules_config = self._load_gateway_config(config_path)

        # è§£æå‡ºéœ€è¦ç®¡ç†çš„èµ„æºç±»å‹åˆ—è¡¨
        # ç›´æ¥ä»é…ç½®ä¸­è¯»å–å¯ç”¨çš„èµ„æºç±»å‹ï¼Œæ— éœ€æ˜ å°„è¡¨
        self.active_resources = [
            m.get("resource_type")
            for m in self.modules_config.get("modules", [])
            if m.get("resource_type")  # åªè¦æœ‰å®šä¹‰èµ„æºç±»å‹å³å¯
        ]
        
        # åˆå§‹åŒ–çŠ¶æ€å˜é‡
        self.initial_observation = None
        self.allocated_resources = {}
        self._tools_initialized = False

        # åˆå§‹åŒ–æŒä¹…äº‹ä»¶å¾ªç¯
        self._loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self._loop)

        logger.info(f"HttpMCPEnv initialized for {self.worker_id} -> {self.server_url}")
        
        # è§¦å‘å·¥å…·åˆå§‹åŒ– (æ•´åˆäº†åŸåŸºç±»çš„è°ƒç”¨)
        self._initialize_tools()

    @property
    def mode(self) -> str:
        return "http_mcp"

    # =========================================================================
    # æ ¸å¿ƒ Agent æ‰§è¡Œé€»è¾‘ (ä» Environment è¿ç§»)
    # =========================================================================

    def run_task(self, task: Dict[str, Any], agent_config: Dict[str, Any], logger: logging.Logger) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„ Agent ä»»åŠ¡å¾ªç¯
        [ç¬¬2å±‚è¶…æ—¶] åœ¨ä»»åŠ¡çº§åˆ«ç›‘æ§æ‰§è¡Œæ—¶é—´ï¼Œè¶…æ—¶è‡ªåŠ¨æŠ›å‡ºå¼‚å¸¸
        """
        task_id = task.get("id", "unknown")
        question = task.get("question", "")

        # è·å– Agent é…ç½®å‚æ•°
        model_name = agent_config.get("model_name", self.model_name)
        max_turns = agent_config.get("max_turns", 3)
        max_retries = agent_config.get("max_retries", 3)

        # [ç¬¬2å±‚è¶…æ—¶] è·å–ä»»åŠ¡è¶…æ—¶é…ç½®ï¼Œé»˜è®¤600ç§’
        task_timeout = float(
            agent_config.get("task_timeout",
            os.environ.get("TASK_EXECUTION_TIMEOUT", "600"))
        )

        # è·å–ä»»åŠ¡è¾“å‡ºç›®å½•ï¼ˆå¦‚æœå®ç°äº†è¯¥æ–¹æ³•ï¼‰
        task_output_dir = None
        if hasattr(self, "get_task_output_dir") and callable(self.get_task_output_dir):
            task_output_dir = self.get_task_output_dir(
                agent_config.get("output_dir", "results"),
                task_id,
                model_name
            )

        # [ç¬¬2å±‚è¶…æ—¶] ä½¿ç”¨è¶…æ—¶ç›‘æ§å™¨
        monitor = TaskTimeoutMonitor(task_timeout, task_id, self.worker_id)

        try:
            # å¯åŠ¨è¶…æ—¶ç›‘æ§
            monitor.start()

            # æ‰§è¡Œå¯¹è¯ï¼ˆå°†start_timeå’Œtimeoutä¼ é€’ç»™å¯¹è¯å‡½æ•°ç”¨äºå®šæœŸæ£€æŸ¥ï¼‰
            messages = self._run_conversation(
                question, model_name, max_turns, max_retries, logger,
                task_timeout=task_timeout,
                task_start_time=time.time()
            )

            # æå–ç­”æ¡ˆ
            final_answer = self._extract_final_answer(messages)

            # æ„å»ºç»“æœ
            result = {
                "task_id": task_id,
                "question": question,
                "answer": final_answer,
                "messages": messages,
                "success": True,
                "error": None,
            }

            # ä¿å­˜æ—¥å¿— (å¦‚æœæ”¯æŒ)
            if task_output_dir:
                self._save_conversation_log(
                    task_output_dir,
                    task_id,
                    question,
                    model_name,
                    messages,
                    result
                )

            return result

        except TaskTimeoutError as e:
            # ä»»åŠ¡è¶…æ—¶å¼‚å¸¸
            logger.error(f"âŒ [TaskTimeout] Task {task_id} timeout: {e}")
            return {
                "task_id": task_id,
                "question": question,
                "answer": "",
                "messages": [],
                "success": False,
                "error": f"Task execution timeout: {e}",
            }

        except Exception as e:
            # å…¶ä»–å¼‚å¸¸
            logger.error(f"âŒ [TaskError] Task {task_id} failed: {e}")
            raise

        finally:
            # å–æ¶ˆè¶…æ—¶ç›‘æ§
            monitor.cancel()

    def _run_conversation(self,
                         question: str,
                         model_name: str,
                         max_turns: int,
                         max_retries: int,
                         logger: logging.Logger,
                         task_timeout: Optional[float] = None,
                         task_start_time: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œ Agent å¯¹è¯å¾ªç¯
        [ç¬¬2å±‚è¶…æ—¶] åœ¨æ¯æ¬¡å¾ªç¯å‰æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¶…æ—¶
        """
        system_prompt = self.get_system_prompt(question)
        messages: List[ChatCompletionMessageParam] = [
            {"role": "system", "content": system_prompt},
        ]

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_content: List[Dict[str, Any]] = [{"type": "text", "text": f"Question: {question}\n"}]

        # æ³¨å…¥åˆå§‹è§‚å¯Ÿ
        initial_obs = getattr(self, "initial_observation", None)
        if initial_obs and isinstance(initial_obs, dict):
            # æ·»åŠ æˆªå›¾
            screenshot_b64 = initial_obs.get("screenshot")
            if screenshot_b64:
                user_content.append({
                    "type": "text",
                    "text": "Here is the initial screen state of the computer:"
                })
                user_content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{screenshot_b64}",
                        "detail": "high"
                    }
                })

            # æ·»åŠ  Accessibility Tree
            a11y_tree = initial_obs.get("accessibility_tree")
            if a11y_tree:
                user_content.append({
                    "type": "text",
                    "text": f"Accessibility Tree:\n{a11y_tree}"
                })

        messages.append({"role": "user", "content": user_content})

        client = self._get_openai_client()
        turn_count = 0

        while turn_count < max_turns:
            # [ç¬¬2å±‚è¶…æ—¶] åœ¨æ¯è½®å¼€å§‹å‰æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if task_timeout and task_start_time:
                if check_execution_timeout(task_start_time, task_timeout, "current_task", self.worker_id):
                    raise TaskTimeoutError(
                        f"Task timeout after {time.time() - task_start_time:.1f}s "
                        f"(limit: {task_timeout}s) at turn {turn_count}"
                    )

            retry = 0
            while retry < max_retries:
                try:
                    # è°ƒç”¨ LLM
                    print(f"Messages length: {len(messages)}")
                    response = client.chat.completions.create(
                        model=model_name,
                        messages=messages,
                        tools=self.get_tool_schemas(),
                    )

                    if not hasattr(response, "choices") or not response.choices:
                        raise ValueError("OpenAI API returned empty response")

                    assistant_message = response.choices[0].message
                    # print(f"Assistant message: {assistant_message}")
                    messages.append(assistant_message.model_dump())

                    # å¤„ç†å·¥å…·è°ƒç”¨
                    if assistant_message.tool_calls:
                        # ä¿®å¤ content ä¸º None çš„æƒ…å†µ
                        if messages[-1]['content'] is None:
                             messages[-1]['content'] = ""

                        for tool_call in assistant_message.tool_calls: # å¤„ç†æ‰€æœ‰ call è€Œä¸ä»…ä»…æ˜¯ [:1]
                            tool_name = tool_call.function.name
                            tool_args = json.loads(tool_call.function.arguments)

                            print(f"Round {turn_count}: ğŸ”§ Using tool: {tool_name}")
                            print(f"Round {turn_count}:    Arguments: {tool_args}")

                            # ã€å…³é”®ä¿®æ”¹ã€‘ç›´æ¥è°ƒç”¨ execute_toolï¼Œå®ƒç°åœ¨é€‚é…äº† MCP
                            tool_output = self.execute_tool(tool_name, tool_args)

                            # è§£ææ ‡å‡†åŒ–è¾“å‡º
                            if isinstance(tool_output, dict) and "images" in tool_output:
                                content_str = tool_output.get("text", "")
                                image_list = tool_output.get("images", [])
                            else:
                                content_str = str(tool_output)
                                image_list = []

                            print(f"Round {turn_count}:    Result: {content_str[:100]}... (Images: {len(image_list)})")

                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "name": tool_name,
                                "content": content_str
                            })

                            # æ³¨å…¥ Vision è§‚å¯Ÿ
                            if image_list:
                                user_content_blocks = []
                                user_content_blocks.append({
                                    "type": "text", 
                                    "text": f"Observation from tool '{tool_name}' (Screenshots):"
                                })
                                for img_b64 in image_list:
                                    user_content_blocks.append({
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{img_b64}",
                                            "detail": "high"
                                        }
                                    })
                                messages.append({
                                    "role": "user",
                                    "content": user_content_blocks
                                })
                        
                    else:
                        logger.info(f"Turn {turn_count}: final answer produced")
                        return messages 
                    
                    # æˆåŠŸæ‰§è¡Œå®Œä¸€è½®ï¼Œè·³å‡ºé‡è¯•
                    break 

                except Exception as exc:
                    retry += 1
                    logger.warning(f"Retry {retry}/{max_retries} due to error: {exc}")
                    if retry >= max_retries:
                        raise
            turn_count += 1
            
        logger.warning("Max turns reached without final answer")
        return messages

    def _extract_final_answer(self, messages: List[Dict[str, Any]]) -> Optional[str]:
        """ä»å¯¹è¯å†å²ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
        if not messages:
            return None
        for msg in reversed(messages):
            if msg.get("role") == "assistant":
                content = msg.get("content")
                if isinstance(content, str) and content.strip():
                    return content
                if content is not None:
                    return str(content)
        return None
    
    def _save_conversation_log(self, 
                             output_dir: str, 
                             task_id: str, 
                             question: str, 
                             model: str, 
                             messages: List[Dict[str, Any]], 
                             result: Dict[str, Any]):
        """
        ä¿å­˜è¯¦ç»†çš„å¯¹è¯æ—¥å¿—åˆ° JSON æ–‡ä»¶ã€‚
        
        ç»“æ„åŒ…å«ï¼š
        - åŸºç¡€å…ƒæ•°æ® (task_id, model, timestamp)
        - ç»Ÿè®¡ä¿¡æ¯ (steps, status)
        - å®Œæ•´ç»“æœ (result å¯¹è±¡)
        - æ‰å¹³åŒ–çš„å¯¹è¯å†å² (ä¾¿äºäººç±»é˜…è¯»)
        """
        import os
        import json
        
        try:
            # 1. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # 2. æ–‡ä»¶åå®‰å…¨å¤„ç† (é˜²æ­¢ task_id åŒ…å«éæ³•å­—ç¬¦)
            safe_task_id = "".join([c if c.isalnum() or c in "-_." else "_" for c in str(task_id)])
            file_path = os.path.join(output_dir, f"{safe_task_id}.json")
            
            # 3. è®¡ç®—ä¸€äº›åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            assistant_turns = sum(1 for m in messages if m.get("role") == "assistant")
            
            # 4. æ„é€ æœ€ç»ˆçš„æ—¥å¿—å¯¹è±¡
            # æˆ‘ä»¬å°† result åŒ…è£…åœ¨ä¸€ä¸ªæ›´æœ‰æ¡ç†çš„ç»“æ„ä¸­
            log_content = {
                "meta": {
                    "task_id": task_id,
                    "model_name": model,
                    "timestamp": datetime.now().isoformat(),
                    "output_file": file_path
                },
                "task": {
                    "question": question,
                    "status": "success" if result.get("success") else "failed",
                    "final_answer": result.get("answer"),
                    "total_turns": assistant_turns
                },
                # ä¿å­˜åŸå§‹çš„å®Œæ•´ç»“æœå­—å…¸ï¼ˆåŒ…å« messagesï¼‰
                "raw_result": result,
            }
            
            # 5. å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                # default=str ç”¨äºå¤„ç†æŸäº›å¯èƒ½æ— æ³•åºåˆ—åŒ–çš„å¯¹è±¡ï¼ˆå¦‚è‡ªå®šä¹‰ç±»å®ä¾‹ï¼‰
                json.dump(log_content, f, ensure_ascii=False, indent=2, default=str)
                
            logger.info(f"[{self.worker_id}] âœ… Conversation log saved to: {file_path}")
            
        except Exception as e:
            logger.error(f"[{self.worker_id}] âŒ Failed to save conversation log: {e}")

    # =========================================================================
    # OpenAI Client ç®¡ç† (ä» Environment è¿ç§»)
    # =========================================================================

    def _get_openai_client(self) -> openai.OpenAI:
        if not hasattr(self, '_openai_client') or self._openai_client is None:
            api_key = self.config.get("openai_api_key") or os.environ.get("OPENAI_API_KEY", "")
            base_url = self.config.get("openai_api_url") or os.environ.get("OPENAI_API_URL") or os.environ.get("OPENAI_API_BASE")

            # [ç¬¬1å±‚è¶…æ—¶] APIè°ƒç”¨è¶…æ—¶é…ç½®
            # å•æ¬¡APIè¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤30ç§’
            timeout = float(self.config.get("openai_timeout", os.environ.get("OPENAI_TIMEOUT", "30")))
            # APIè¯·æ±‚å¤±è´¥åçš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤2æ¬¡
            max_retries = int(self.config.get("openai_max_retries", os.environ.get("OPENAI_MAX_RETRIES", "2")))

            logger.info(f"[{self.worker_id}] Initializing OpenAI client with timeout={timeout}s, max_retries={max_retries}")

            openai.api_key = api_key
            if base_url:
                openai.base_url = base_url
                self._openai_client = openai.OpenAI(
                    api_key=api_key,
                    base_url=base_url,
                    timeout=timeout,
                    max_retries=max_retries
                )
            else:
                self._openai_client = openai.OpenAI(
                    api_key=api_key,
                    timeout=timeout,
                    max_retries=max_retries
                )
        return self._openai_client

    # =========================================================================
    # å·¥å…·ç®¡ç†ä¸æ‰§è¡Œ (é€‚é… MCP)
    # =========================================================================

    def execute_tool(self, tool_name: str, params: Union[str, dict], **kwargs) -> Union[str, Dict[str, Any]]:
        """
        [æ ¸å¿ƒé‡æ„] æ‰§è¡Œå·¥å…·
        åŸ Environment å°è¯•æœ¬åœ°è°ƒç”¨ self.tools[name].call()ã€‚
        å¯¹äº HttpMCPEnvï¼Œæ‰€æœ‰å·¥å…·éƒ½æ˜¯è¿œç¨‹çš„ï¼Œå› æ­¤ç›´æ¥ä»£ç†åˆ° _call_tool_syncã€‚
        """
        # å…¼å®¹å‚æ•°æ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µ (æœ‰æ—¶å€™ LLM ä¼šè¿”å› JSON string)
        if isinstance(params, str):
            try:
                params = json.loads(params)
            except:
                pass
        
        # è°ƒç”¨ MCP åŒæ­¥æ¥å£
        return self._call_tool_sync(tool_name, params)

    def get_tool_schemas(self) -> List[ChatCompletionToolParam]:
        """è·å–ç”¨äº LLM API çš„å·¥å…·å®šä¹‰"""
        return self.tool_schemas  # type: ignore

    def get_tool_descriptions(self) -> str:
        """è·å–ç”¨äº Prompt çš„å·¥å…·æè¿°æ–‡æœ¬"""
        return self.tool_descriptions

    def register_tool(self, tool: Tool):
        """ä¸ºäº†ä¿æŒæ¥å£å…¼å®¹æ€§ä¿ç•™ï¼Œä½† MCP æ¨¡å¼ä¸‹é€šå¸¸ä¸ç”¨æœ¬åœ°æ³¨å†Œ"""
        self.tools[tool.name] = tool
        # æ›´æ–° metadata
        pass

    # =========================================================================
    # Prompt å·¥ç¨‹ (ä» Environment è¿ç§»)
    # =========================================================================

    def get_action_space(self) -> Optional[str]:
        mode_config = self.config.get(self.mode)
        if isinstance(mode_config, dict) and "action_space" in mode_config:
            return mode_config.get("action_space")
        return self.config.get("action_space")

    def get_system_prompt(
        self,
        task_question: Optional[str] = None,
        extra_context: Optional[str] = None,
        action_space: Optional[str] = None,
    ) -> str:
        resolved_action_space = action_space or self.get_action_space()
        if resolved_action_space is None:
            prompt_template = load_system_prompt(environment_mode=self.mode)
        else:
            prompt_template = load_system_prompt(
                environment_mode=self.mode,
                action_space=resolved_action_space
            )

        prompt_with_tools = prompt_template.replace(
            "{tool_descriptions}",
            self.get_tool_descriptions()
        )

        prompt_with_placeholders = self._replace_prompt_placeholders(prompt_with_tools)

        suffix_parts: List[str] = []
        if task_question:
            suffix_parts.append(f"You are asked to complete the following task: {task_question}")
        if extra_context:
            suffix_parts.append(extra_context)

        if suffix_parts:
            prompt_with_placeholders = "\n".join([prompt_with_placeholders, *suffix_parts])

        return prompt_with_placeholders

    def _replace_prompt_placeholders(self, prompt: str) -> str:
        return prompt

    # =========================================================================
    # MCP ä¸“ç”¨é€»è¾‘ (ä¿æŒä¸å˜)
    # =========================================================================

    def _load_gateway_config(self, config_path: str) -> Dict[str, Any]:
        if not os.path.exists(config_path):
            logger.warning(f"Gateway config not found at {config_path}, using default VM-only config.")
            return {"modules": [{"resource_type": "vm"}]}
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load gateway config: {e}")
            return {"modules": [{"resource_type": "vm"}]}

    def _initialize_tools(self):
        """
        æ ¹æ®é…ç½®åŠ¨æ€ç”Ÿæˆå·¥å…· Schema å’Œæè¿°ã€‚
        """
        if not self._tools_initialized:
            logger.info(f"[{self.worker_id}] Skipping tool initialization before connection is established")
            return

        try:
            logger.info(f"[{self.worker_id}] Fetching tools from MCP Server...")
            mcp_tools = self._list_tools_sync()

            # é»‘åå•ï¼šAgent ä¸åº”ç›´æ¥è°ƒç”¨çš„ç³»ç»Ÿå·¥å…·
            blacklist = {
                # æ—§çš„è¯„ä¼°å’Œè§‚å¯Ÿå·¥å…·ï¼ˆå·²åºŸå¼ƒæˆ–ç”±ç³»ç»Ÿè‡ªåŠ¨è°ƒç”¨ï¼‰
                "get_observation",
                "evaluate_task",
                # èµ„æºç”Ÿå‘½å‘¨æœŸç®¡ç†å·¥å…·ï¼ˆç”±ç³»ç»Ÿç»Ÿä¸€è°ƒç”¨ï¼ŒAgent ä¸åº”ç›´æ¥ä½¿ç”¨ï¼‰
                "allocate_batch_resources",
                "setup_batch_resources",
                "get_batch_initial_observations",
                "setup_vm_session",
                "setup_rag_session",
                "teardown_environment",
                "release_rag_session",
            }

            valid_tools = [t for t in mcp_tools if t.name not in blacklist]

            self.tool_schemas = [self._convert_mcp_tool_to_openai(t) for t in valid_tools]

            descriptions = []
            for t in valid_tools:
                desc = t.description if t.description else "No description provided."
                descriptions.append(f"- {t.name}: {desc}")

            self.tool_descriptions = "\n".join(descriptions)

            logger.info(f"[{self.worker_id}] Initialized {len(valid_tools)} tools (Metadata only). Blacklisted: {len(blacklist)}")

        except Exception as e:
            logger.error(f"Failed to initialize tools: {e}")
            self.tool_schemas = []
            self.tool_descriptions = "Error loading tools."

    def _convert_mcp_tool_to_openai(self, mcp_tool) -> ChatCompletionToolParam:
        parameters = mcp_tool.inputSchema.copy() if hasattr(mcp_tool, "inputSchema") else {}
        if "properties" in parameters and "worker_id" in parameters["properties"]:
            del parameters["properties"]["worker_id"]
        if "required" in parameters and "worker_id" in parameters["required"]:
            parameters["required"] = [p for p in parameters["required"] if p != "worker_id"]

        return {
            "type": "function",
            "function": {
                "name": mcp_tool.name,
                "description": mcp_tool.description,
                "parameters": parameters 
            }
        }

    def env_start(self):
        logger.info(f"Worker [{self.worker_id}] started (Config-Driven Mode)")
        self._run_sync(self.mcp_client.connect())
        self._tools_initialized = True
        self._initialize_tools()

    def env_close(self):
        if hasattr(self, '_loop') and not self._loop.is_closed():
            self._loop.close()

    def _run_sync(self, awaitable):
        return self._loop.run_until_complete(awaitable)

    def _list_tools_sync(self):
        return self._run_sync(self.mcp_client.list_tools())

    def _call_tool_sync(self, name, arguments) -> Union[Dict[str, Any], Any]:
        """åŒæ­¥è°ƒç”¨ MCP å·¥å…·"""
        if isinstance(arguments, dict) and "worker_id" not in arguments:
            arguments["worker_id"] = self.worker_id
            
        logger.info(f"[{self.worker_id}] â³ Sync Calling: {name}...")
        start_time = time.time()

        res = self._run_sync(self.mcp_client.call_tool(name, arguments))

        duration = time.time() - start_time
        logger.info(f"[{self.worker_id}] âœ… Sync Call Done: {name} (Took {duration:.2f}s)")

        # ç”Ÿå‘½å‘¨æœŸå·¥å…·å¤„ç†
        lifecycle_tools = {
            "allocate_batch_resources", "setup_batch_resources", 
            "get_batch_initial_observations", "setup_vm_session", 
            "setup_rag_session", "teardown_environment", "release_rag_session"
        }
        
        if name in lifecycle_tools:
            return res 

        # é€šç”¨è¾“å‡ºæ ‡å‡†åŒ–
        standardized_output = {
            "text": "",
            "images": []
        }
        text_parts = []
        
        if hasattr(res, 'content') and res.content:
            for item in res.content:
                if item.type == 'text':
                    text_parts.append(item.text)
                elif item.type == 'image':
                    standardized_output["images"].append(item.data)
                elif item.type == 'resource':
                    # æ­£ç¡®è®¿é—® EmbeddedResource çš„ uri å±æ€§
                    text_parts.append(f"[Resource: {item.resource.uri}]")
        else:
            text_parts.append(str(res) if res else "Success (No content)")

        standardized_output["text"] = "\n".join(text_parts)
        return standardized_output

    def _parse_mcp_response(self, response: CallToolResult) -> Dict[str, Any]:
        try:
            if response.content and len(response.content) > 0:
                content_item = response.content[0]
                # æ­£ç¡®è®¿é—® TextContent çš„ text å±æ€§
                if hasattr(content_item, 'text'):
                    text_content = content_item.text
                elif hasattr(content_item, 'resource') and hasattr(content_item.resource, 'text'):
                    text_content = content_item.resource.text
                elif hasattr(content_item, '__dict__') and 'text' in content_item.__dict__:
                    text_content = content_item.__dict__['text']
                else:
                    text_content = str(content_item)
                if text_content:
                    return json.loads(text_content)
            return {"status": "unknown"}
        except Exception as e:
            logger.error(f"Failed to parse MCP response: {e}")
            return {"status": "error", "message": str(e)}


    def get_inital_obs(self) -> Dict[str, Any]:
        logger.info(f"[{self.worker_id}] Fetching batch initial observations from MCP...")
        combined_obs = {
            "vm": None,
            "rag": None,
            "raw_response": {}
        }
        try:
            res = self._call_tool_sync("get_batch_initial_observations", {"worker_id": self.worker_id})
            data = self._parse_mcp_response(res)
            combined_obs["raw_response"] = data

            if isinstance(data, dict) and "error" not in data:
                if "vm" in data and data["vm"]:
                    combined_obs["vm"] = data["vm"]
                    self.initial_observation = data["vm"]
                if "rag" in data:
                    combined_obs["rag"] = data["rag"]
            else:
                logger.warning(f"[{self.worker_id}] Failed to fetch obs: {data.get('error')}")
            return combined_obs
        except Exception as e:
            logger.error(f"[{self.worker_id}] Exception in get_inital_obs: {e}")
            return combined_obs

    def allocate_resource(self, worker_id: str, resource_init_data: Optional[Dict[str, Any]] = None) -> bool:
        """
        ç»Ÿä¸€çš„èµ„æºåˆ†é…å…¥å£å‡½æ•°

        Args:
            worker_id: Worker ID
            resource_init_data: èµ„æºåˆå§‹åŒ–é…ç½®æ•°æ®

        Returns:
            åˆ†é…æ˜¯å¦æˆåŠŸ
        """
        resource_init_data = resource_init_data or {}
        logger.info(f"Worker [{worker_id}] requesting resources: {self.active_resources}...")

        self.initial_observation = None

        # ç»Ÿä¸€ä½¿ç”¨åŸå­åŒ–æ‰¹é‡åˆ†é…
        return self._allocate_resources_atomically(resource_init_data)

    def _allocate_resources_atomically(self, resource_init_data: Dict[str, Any]) -> bool:
        """
        [ä¿®å¤ç‰ˆ] åŸå­åŒ–èµ„æºç”³è¯· + è‡ªåŠ¨åˆå§‹åŒ–
        """
        try:
            # 1. ç”³è¯·èµ„æº (Allocate)
            args = {
                "resource_types": self.active_resources,
                "timeout": 600
            }
            logger.info(f"Worker [{self.worker_id}] calling MCP tool 'allocate_batch_resources' via Gateway...")
            res = self._call_tool_sync("allocate_batch_resources", args)
            data = self._parse_mcp_response(res)

            if isinstance(data, dict) and data.get("status") == "error":
                 logger.error(f"Atomic alloc tool failed: {data.get('message')}")
                 return False

            # 2. ä¿å­˜èµ„æºä¿¡æ¯
            self.allocated_resources = data

            # 3. æ‰§è¡Œèµ„æºåˆå§‹åŒ– (Setup)
            if resource_init_data:
                logger.info(f"[{self.worker_id}] Setting up resources...")
                setup_res = self._call_tool_sync("setup_batch_resources", {
                    "resource_init_configs": resource_init_data,
                    "allocated_resources": data
                })
                setup_result = self._parse_mcp_response(setup_res)

                if setup_result.get("status") not in ["success", "partial_error"]:
                    logger.error(f"[{self.worker_id}] Resource setup failed: {setup_result}")
                    self.release_resource(self.worker_id)
                    return False

            # 4. è·å–åˆå§‹è§‚å¯Ÿ
            self.get_inital_obs()
            return True

        except Exception as e:
            logger.error(f"Failed to allocate resources atomically via MCP: {e}")
            return False

    def release_resource(self, worker_id: str, reset: bool = True) -> None:
        """
        ç»Ÿä¸€é‡Šæ”¾æ‰€æœ‰å·²åˆ†é…çš„èµ„æº

        é€šè¿‡ Resource API ç»Ÿä¸€é‡Šæ”¾ï¼ŒMCP Server ç«¯ä¼šè‡ªåŠ¨è°ƒç”¨ç›¸åº”çš„ teardown/release å·¥å…·

        Args:
            worker_id: Worker ID
            reset: æ˜¯å¦é‡ç½®èµ„æºçŠ¶æ€ï¼ˆä¿ç•™å‚æ•°ä»¥å…¼å®¹æ—§æ¥å£ï¼‰
        """
        logger.info(f"Worker [{worker_id}] releasing all resources...")

        # éå†å·²åˆ†é…çš„èµ„æºå¹¶é€ä¸€é‡Šæ”¾
        for res_type, res_data in list(self.allocated_resources.items()):
            resource_id = res_data.get("id")
            if resource_id:
                try:
                    # ç›´æ¥è°ƒç”¨ Resource API é‡Šæ”¾ï¼Œé€šè¿‡ MCP åŒæ­¥è°ƒç”¨
                    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä½¿ç”¨åŒæ­¥çš„æ–¹å¼è°ƒç”¨
                    import httpx
                    with httpx.Client() as client:
                        response = client.post(
                            f"{os.environ.get('RESOURCE_API_URL', 'http://localhost:8000')}/release",
                            json={"resource_id": resource_id, "worker_id": worker_id},
                            timeout=10.0
                        )
                        logger.info(f"Released {res_type} (resource_id={resource_id}): {response.status_code}")
                except Exception as e:
                    logger.warning(f"Failed to release {res_type} (resource_id={resource_id}): {e}")

        # æ¸…ç©ºå·²åˆ†é…èµ„æºè®°å½•
        self.allocated_resources.clear()
        logger.info(f"Worker [{worker_id}] resource cleanup completed")

    def get_allocated_resource_id(self) -> str:
        return self.worker_id