"""
AgentFlow - Main execution script using Environment and Benchmark modules.

This script provides a unified interface for running agents on different benchmarks
using the Environment and Benchmark classes.
"""

from pyexpat import model
import openai
import json
import os
import argparse
import concurrent.futures
import base64 # osworld
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Import our custom modules
from envs import (
    # Data models
    Observation,
    TrajectoryStep,
    TaskTrajectory,
    # Environments
    MathEnvironment,
    PythonEnvironment,
    RAGEnvironment,
    WebEnvironment,
    Environment
)
from envs.osworld_environment import OSWorldEnvironment
from benchmark import Benchmark, create_benchmark


# Configuration
@dataclass
class AgentConfig:
    """Configuration for agent execution."""
    model_name: str = "gpt-4.1-2025-04-14"
    max_turns: int = 100
    max_retries: int = 3
    max_workers: int = 1
    save_results: bool = True
    evaluate_results: bool = True
    evaluation_metric: str = "exact_match"


# ============================================================================
# System Prompts Management
# ============================================================================
# All system prompts have been moved to prompts/system_prompts.py
# They are now imported via: from prompts import get_system_prompt
#
# This allows for:
# 1. Clean separation of prompt definitions from execution logic
# 2. Easy extension to support new environments and action spaces
# 3. Centralized prompt management and version control
#
# Available prompts:
# - default: Generic prompt for all non-specialized environments
# - osworld_computer_13: OSWorld with structured action tools
# - osworld_pyautogui: OSWorld with Python script execution
# ============================================================================


class AgentRunner:
    """
    Main agent runner that coordinates Environment and Benchmark.
    
    This class handles:
    - Loading benchmarks
    - Setting up environments
    - Running agents on benchmark tasks
    - Evaluating results
    - Saving outputs
    """
    
    def __init__(self, config: AgentConfig):
        """Initialize the agent runner."""
        self.config = config
        self.environment: Optional[Environment] = None
        self.benchmark: Optional[Benchmark] = None
        self.results: List[Dict[str, Any]] = []
        self.output_file: Optional[str] = None

        # ‚≠ê Êñ∞Â¢ûÔºöËΩ®ËøπÂ≠òÂÇ®
        self.trajectories: List[TaskTrajectory] = []
        self.current_trajectory: Optional[TaskTrajectory] = None

        # Validate OpenAI configuration
        self._validate_openai_config()
    
    def _validate_openai_config(self):
        """Validate OpenAI API configuration."""
        openai.api_key = os.environ.get("OPENAI_API_KEY", "")
        openai.base_url = os.environ.get("OPENAI_API_URL", os.environ.get("OPENAI_API_BASE", ""))
        
        if not openai.api_key:
            print("Warning: OPENAI_API_KEY is not set. Some features may not work properly.")
        if not openai.base_url:
            print("Warning: OPENAI_API_URL or OPENAI_API_BASE is not set. Some features may not work properly.")
    
    def setup_environment(self, mode: str, **kwargs) -> Environment:
        """
        Setup environment based on mode.
        
        Args:
            mode: Environment mode ("math", "py", "rag", "web")
            **kwargs: Additional configuration for the environment
            
        Returns:
            Configured environment
        """
        print(f"Setting up {mode} environment...")
        
        if mode == "math":
            self.environment = MathEnvironment(**kwargs)
        elif mode == "py":
            self.environment = PythonEnvironment(**kwargs)
        elif mode == "rag":
            self.environment = RAGEnvironment(**kwargs)
        elif mode == "web":
            self.environment = WebEnvironment(**kwargs)
        elif mode == "osworld":
            self.environment = OSWorldEnvironment(**kwargs)
        else:
            raise ValueError(f"Unknown mode: {mode}")
        
        print(f"Environment setup complete. Available tools: {self.environment.list_tools()}")
        return self.environment
    
    def load_benchmark(self, data_path: str, name: Optional[str] = None, **kwargs) -> Benchmark:
        """
        Load benchmark from data file.
        
        Args:
            data_path: Path to benchmark data
            name: Name of the benchmark
            **kwargs: Additional configuration (filtered for benchmark)
            
        Returns:
            Loaded benchmark
        """
        print(f"Loading benchmark from {data_path}...")
        
        # Filter kwargs to only include benchmark-relevant parameters
        benchmark_kwargs = {k: v for k, v in kwargs.items() 
                           if k in ['description']}
        
        self.benchmark = create_benchmark(
            data_path=data_path,
            name=name or f"Benchmark_{os.path.basename(data_path)}",
            **benchmark_kwargs
        )
        
        print(f"Benchmark loaded: {len(self.benchmark.items)} items")
        return self.benchmark
    
    def run_single_task(self, task: Dict[str, Any], output_dir: str = "results") -> Dict[str, Any]:
        """
        Run agent on a single task.

        This method is now environment-agnostic. All environment-specific logic
        has been moved to the Environment class methods.

        Args:
            task: Task dictionary with 'id', 'question', and optionally 'metadata'
            output_dir: Base output directory for results

        Returns:
            Result dictionary
        """
        if not self.environment:
            raise ValueError("Environment not set up")

        task_id = task["id"]
        question = task["question"]

        print(f"\n{'='*60}")
        print(f"Processing Task {task_id}")
        print(f"Question: {question}")
        print(f"{'='*60}")

        # ‚≠ê ÂàõÂª∫Êñ∞ÁöÑ‰ªªÂä°ËΩ®Ëøπ
        self.current_trajectory = TaskTrajectory(
            task_id=task_id,
            question=question,
            metadata={"model": self.config.model_name}
        )

        # Get task output directory from environment (None if not needed)
        task_output_dir = self.environment.get_task_output_dir(output_dir, task_id, self.config.model_name)

        if task_output_dir:
            print(f"   Task output directory: {task_output_dir}")

        # Initialize environment for this task and get initial observation
        initial_obs = self.environment.env_task_init(task)

        try:
            # Run multi-turn conversation with initial observation
            messages = self._run_conversation(question, task_id, initial_obs=initial_obs)

            # Extract final answer
            final_answer = self._extract_final_answer(messages)

            # ‚≠ê ÂÆåÊàêËΩ®Ëøπ
            self.current_trajectory.final_answer = final_answer
            self.current_trajectory.success = True
            self.current_trajectory.end_time = datetime.now().isoformat()
            self.current_trajectory.total_steps = len(self.current_trajectory.steps)

            result = {
                "task_id": task_id,
                "question": question,
                "answer": final_answer,
                "messages": messages,
                "success": True,
                "error": None
            }

            print(f"‚úì Task {task_id} completed successfully")
            if final_answer != "":
                print(f"Answer: {final_answer[:100]}...")
            else:
                print("No answer found")

            # Save conversation and trajectory (if task_output_dir exists)
            if task_output_dir:
                self._save_conversation_and_trajectory(
                    task_id=task_id,
                    question=question,
                    messages=messages,
                    result=result,
                    output_dir=task_output_dir
                )

        except Exception as e:
            print(f"‚úó Task {task_id} failed: {str(e)}")

            # ‚≠ê Ê†áËÆ∞ËΩ®ËøπÂ§±Ë¥•
            self.current_trajectory.success = False
            self.current_trajectory.end_time = datetime.now().isoformat()
            self.current_trajectory.metadata["error"] = str(e)

            result = {
                "task_id": task_id,
                "question": question,
                "answer": "",
                "messages": [],
                "success": False,
                "error": str(e)
            }

        finally:
            # ‚≠ê ‰øùÂ≠òËΩ®ËøπÂà∞ÂàóË°®
            self.trajectories.append(self.current_trajectory)

            # Finalize task: end recording, save trajectory, and evaluate
            # This is handled by env_task_end() which does everything in one call
            try:
                self.environment.env_task_end(task_id, task_output_dir)
            except Exception as e:
                print(f"‚ö†Ô∏è  Warning: Failed to finalize task: {e}")

            # ‚≠ê Ê∏ÖÁ©∫ÂΩìÂâçËΩ®Ëøπ
            self.current_trajectory = None

        return result
    
    def _run_conversation(
        self,
        question: str,
        task_id: str,
        initial_obs: Optional[Dict[str, Any]] = None,
        trajectory_dir: Optional[str] = None,
        collect_trajectory: bool = False
    ):
        """
        Run multi-turn conversation with the agent.

        Args:
            question: User question
            task_id: Task identifier
            initial_obs: Initial observation from env_task_init (None if not applicable)
                         Format: {'text': str, 'image': str}
            trajectory_dir: Optional directory to save trajectory (screenshots and accessibility trees)
            collect_trajectory: Whether to collect trajectory data (for OSWorld)

        Returns:
            If collect_trajectory is False: List of messages from the conversation
            If collect_trajectory is True: Tuple of (messages, trajectory_data)
        """
        from datetime import datetime

        # Initialize trajectory data collection
        trajectory_data = None
        if collect_trajectory:
            trajectory_data = {
                "task_id": task_id,
                "task_question": question,
                "model": self.config.model_name,
                "start_time": datetime.now().isoformat(),
                "steps": []
            }

        # ========================================================================
        # Step 1: Get system prompt from environment
        # ========================================================================
        # The environment handles:
        # - Selecting appropriate prompt template based on mode and action_space
        # - Replacing placeholders (tool_descriptions, CLIENT_PASSWORD, etc.)
        # - Adding task question
        print(f"üìã Getting system prompt from environment (mode='{self.environment.mode}')...")
        system_prompt = self.environment.get_system_prompt(question)
        print(f"System prompt preview: {system_prompt[:500]}...")

        # Initialize messages with system prompt
        messages = [
            {"role": "system", "content": system_prompt},
        ]

        # ========================================================================
        # Step 2: Use initial observation from env_task_init (if provided)
        # ========================================================================
        # The initial_obs comes from env_task_init() in run_single_task
        # Format: {'text': str, 'image': str} or None

        # Initialize step counter for trajectory saving
        step_idx = 0

        # Helper function to save trajectory step (for OSWorld)
        def save_trajectory_step(obs_content_parts: List[Dict[str, Any]], step_number: int, trajectory_dir: str):
            """Save screenshot and accessibility tree for a step from content parts."""
            if not trajectory_dir or not obs_content_parts:
                return

            import base64
            from datetime import datetime

            os.makedirs(trajectory_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # Extract data from content parts
            for part in obs_content_parts:
                part_type = part.get("type")

                # Save screenshot from image_url part
                if part_type == "image_url":
                    try:
                        url = part.get("image_url", {}).get("url", "")
                        if url.startswith("data:image/png;base64,"):
                            base64_data = url.replace("data:image/png;base64,", "")
                            screenshot_bytes = base64.b64decode(base64_data)

                            screenshot_filename = f"step_{step_number}_{timestamp}.png"
                            screenshot_path = os.path.join(trajectory_dir, screenshot_filename)
                            with open(screenshot_path, 'wb') as f:
                                f.write(screenshot_bytes)
                            print(f"   Saved screenshot: {screenshot_filename}")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Failed to save screenshot: {e}")

                # Save accessibility tree from text part
                elif part_type == "text":
                    text_content = part.get("text", "")
                    if "Accessibility tree" in text_content:
                        try:
                            a11y_filename = f"step_{step_number}_{timestamp}_accessibility_tree.txt"
                            a11y_path = os.path.join(trajectory_dir, a11y_filename)
                            with open(a11y_path, 'w', encoding='utf-8') as f:
                                f.write(text_content)
                            print(f"   Saved a11y tree: {a11y_filename}")
                        except Exception as e:
                            print(f"   ‚ö†Ô∏è  Failed to save a11y tree: {e}")

        # Build initial user message content
        user_content = []
        user_content.append({
            "type": "text",
            "text": f"Question: {question}\n"
        })

        # Add initial observation if available (from env_task_init)
        if initial_obs is not None:
            # Convert observation to message content parts
            # initial_obs format: {'text': str, 'image': str}
            obs_content_parts = []

            # Add introduction text
            obs_content_parts.append({
                "type": "text",
                "text": "The following are the computer's initial observations.\n"
            })

            # Add accessibility tree (text)
            if initial_obs.get('text'):
                obs_content_parts.append({
                    "type": "text",
                    "text": f"\nAccessibility tree as below:\n{initial_obs['text']}\n"
                })

            # Add screenshot (image)
            if initial_obs.get('image'):
                obs_content_parts.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{initial_obs['image']}",
                        "detail": "high"
                    }
                })

            # Note: Trajectory saving is now handled by env_task_init/env_task_end
            # in OSWorldEnvironment, so we don't need to save it here

            # Add observation content parts to user message
            user_content.extend(obs_content_parts)
            print(f"   Initial observation added to conversation")

        # Add user message
        messages.append({
            "role": "user",
            "content": user_content
        })
        
        
        # Initialize OpenAI client
        client = openai.OpenAI(
            api_key=openai.api_key,
            base_url=openai.base_url
        )
        
        turn_count = 0
        
        while turn_count < self.config.max_turns:
            retry = 0
            
            while retry < self.config.max_retries:
                try:
                    # Get response from OpenAI
                    response = client.chat.completions.create(
                        model=self.config.model_name,
                        messages=messages,
                        tools=self.environment.get_tool_schemas(),
                    )

                    # üêõ DEBUG: Print complete response for debugging
                    print(f"üîç DEBUG - API Response Type: {type(response)}")
                    print(f"üîç DEBUG - API Response Content: {response}")
                    if hasattr(response, '__dict__'):
                        print(f"üîç DEBUG - Response Attributes: {response.__dict__}")

                    # Validate response type
                    if isinstance(response, str):
                        raise ValueError(f"API returned string instead of ChatCompletion object: {response[:200]}")
                    if not hasattr(response, 'choices'):
                        raise ValueError(f"API returned invalid response type: {type(response)}, content: {str(response)[:200]}")
                    if not response.choices:
                        raise ValueError("API returned empty choices list")

                    assistant_message = response.choices[0].message
                    # Convert to dict format for consistency
                    messages.append(assistant_message.model_dump())

                    # ‚≠ê ÊèêÂèñ thoughtÔºàLLM ÁöÑÊÄùËÄÉËøáÁ®ãÔºâ
                    thought = assistant_message.content if assistant_message.content else ""

                    if assistant_message.tool_calls:
                        # Execute tool calls
                        if messages[-1]["content"] == "":
                            # FIX: Âéü‰ª£Á†ÅÊúâËØ≠Ê≥ïÈîôËØØ - ‰∏§‰∏™Ë°®ËææÂºèËøûÂú®‰∏ÄËµ∑Ê≤°ÊúâÊìç‰ΩúÁ¨¶
                            # ÂéüÂõ†Ôºötc =messages[-1].tool_calls[0].model_dump()['function']messages[-1]
                            #      ËøôË°å‰ª£Á†ÅÁúãËµ∑Êù•ÊòØÂ§çÂà∂Á≤òË¥¥ÈîôËØØÔºåÊúâ‰∏§‰∏™ÈóÆÈ¢òÔºö
                            #      1. messages[-1] Â∑≤ÁªèÊòØdictÊ†ºÂºèÔºàÈÄöËøámodel_dump()ËΩ¨Êç¢ÔºâÔºåÂ∫îËØ•Áî®["tool_calls"]ËÆøÈóÆ
                            #      2. Êú´Â∞æÁöÑ messages[-1] Â∫îËØ•ÊòØÊÉ≥ËµãÂÄºÁªôcontentÔºå‰ΩÜÂÜôÈîô‰∫Ü‰ΩçÁΩÆ
                            # ‰øÆÂ§çÔºöÊ≠£Á°ÆËÆøÈóÆtool_callsÂπ∂ËÆæÁΩÆcontent
                            tc = messages[-1]["tool_calls"][0]["function"]
                            messages[-1]["content"] = f'Calling tools: {tc}'
                        for tool_call in assistant_message.tool_calls[:1]:
                            tool_name = tool_call.function.name
                            tool_args = json.loads(tool_call.function.arguments)

                            print(f"Round {turn_count}: üîß Using tool: {tool_name}")
                            print(f"Round {turn_count}:    Arguments: {tool_args}")

                            # Execute tool
                            tool_result_str = self.environment.execute_tool(
                                tool_name,
                                tool_args
                            )

                            print(f"Round {turn_count}:    Result: {tool_result_str[:100]}...")

                            # Parse tool result (JSON string from ToolResponse)
                            try:
                                tool_result = json.loads(tool_result_str)
                            except json.JSONDecodeError:
                                # Fallback for non-JSON responses
                                tool_result = {
                                    'status': 'unknown',
                                    'response': tool_result_str,
                                    'observation': {}
                                }

                            # ‚≠ê ÂàõÂª∫ TrajectoryStep
                            trajectory_step = TrajectoryStep(
                                step_id=step_idx + 1,
                                action=tool_name,
                                action_input=tool_args,
                                thought=thought,
                                result=tool_result.get('response', ''),
                                status=tool_result.get('status', 'unknown')
                            )

                            # Build tool response content
                            content_parts = []

                            # 1. Add execution result (text)
                            base_result = {
                                "status": tool_result.get('status', 'unknown'),
                                "response": tool_result.get('response', ''),
                            }
                            content_parts.append({
                                "type": "text",
                                "text": f"Execution Result:\n{json.dumps(base_result, indent=2)}"
                            })

                            # 2. Add observation if tool returns one (e.g., OSWorld tools)
                            # Tools can include observation data (screenshots, state info) in their results
                            # observationÊ†ºÂºè: {'text': str, 'image': str (base64)}
                            observation_dict = tool_result.get('observation', {})
                            if observation_dict:
                                # Increment step counter for this action
                                step_idx += 1

                                # ‚≠ê ËΩ¨Êç¢‰∏∫ObservationÂØπË±°Âπ∂Ê∑ªÂä†Âà∞TrajectoryStep
                                # ËøôÈÅµÂæ™Áªü‰∏ÄÁöÑObservationÊé•Âè£Ôºàenvs/enviroment.pyÂÆö‰πâÔºâ
                                observation_objects = []

                                # ÊñáÊú¨ËßÇÂØüÔºàÂèØËÆøÈóÆÊÄßÊ†ëÔºâ
                                if observation_dict.get('text'):
                                    text_obs = Observation(
                                        type="text",
                                        content=observation_dict['text'],
                                        metadata={"source": "accessibility_tree"}
                                    )
                                    observation_objects.append(text_obs)
                                    trajectory_step.observations.append(text_obs)

                                # ÂõæÂÉèËßÇÂØüÔºàÊà™ÂõæÔºâ
                                if observation_dict.get('image'):
                                    image_obs = Observation(
                                        type="image",
                                        content=observation_dict['image'],  # base64 Â≠óÁ¨¶‰∏≤
                                        metadata={"format": "png", "encoding": "base64"}
                                    )
                                    observation_objects.append(image_obs)
                                    trajectory_step.observations.append(image_obs)

                                # ‚≠ê Ê∑ªÂä†Ê≠•È™§Âà∞ÂΩìÂâçËΩ®Ëøπ
                                if self.current_trajectory is not None:
                                    self.current_trajectory.steps.append(trajectory_step)

                                # Add observation to environment's trajectory storage
                                # (OSWorldEnvironment will store it for later saving)
                                if hasattr(self.environment, 'add_step_to_trajectory'):
                                    self.environment.add_step_to_trajectory(observation_dict, step_idx)

                                # ‚≠ê Format observation for LLM message (OpenAI format)
                                # ‰ªéObservationÂØπË±°ËΩ¨Êç¢‰∏∫OpenAIÊ∂àÊÅØÊ†ºÂºè
                                obs_content_parts = []

                                for obs in observation_objects:
                                    if obs.type == "text":
                                        obs_content_parts.append({
                                            "type": "text",
                                            "text": f"\n--- Current Page State ---\n{obs.content}"
                                        })
                                    elif obs.type == "image":
                                        obs_content_parts.append({
                                            "type": "image_url",
                                            "image_url": {
                                                "url": f"data:image/png;base64,{obs.content}",
                                                "detail": "high"
                                            }
                                        })

                                # Add formatted observation to content
                                content_parts.extend(obs_content_parts)

                            # Format content based on whether we have multimodal data
                            if len(content_parts) == 1 and content_parts[0]["type"] == "text":
                                # Pure text response - use string format for better compatibility
                                final_content = content_parts[0]["text"]
                            else:
                                # Multimodal response (text + image) - use list format
                                final_content = content_parts

                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "name": tool_name,
                                "content": final_content
                            })
                        
                        # Continue conversation after tool use
                        break
                    
                    else:
                        # No tool calls, conversation complete
                        print(f"üí¨ Final answer at turn {turn_count}")
                        return messages
                
                except Exception as e:
                    print(f"‚ö†Ô∏è  Retry {retry + 1}/{self.config.max_retries}: {str(e)}")
                    retry += 1
                    if retry >= self.config.max_retries:
                        raise e
            
            turn_count += 1
        
        print(f"‚ö†Ô∏è  Max turns ({self.config.max_turns}) reached")
        return messages
    
    def _extract_final_answer(self, messages: List[Dict[str, Any]]) -> str:
        """
        Extract final answer from conversation messages.
        
        Args:
            messages: List of conversation messages
            
        Returns:
            Final answer string
        """
        # Find the last assistant message without tool calls
        for message in reversed(messages):
            # Handle both dict and object types
            if hasattr(message, 'role'):
                # OpenAI message object
                if (message.role == "assistant" and 
                    not hasattr(message, 'tool_calls') or not message.tool_calls):
                    return getattr(message, 'content', "")
            else:
                # Dict format
                if (message.get("role") == "assistant" and 
                    not message.get("tool_calls")):
                    return message.get("content", "")
        
        return "No final answer found"

    # ‰øùÂ≠ò
    def _save_conversation_and_trajectory(
        self,
        task_id: str,
        question: str,
        messages: List[Dict[str, Any]],
        result: Dict[str, Any],
        output_dir: str
    ):
        """
        Save conversation and trajectory data for OSWorld tasks.

        Creates three files:
        1. trajectory.json - Simplified action trace for analysis
        2. conversation.json - Complete LLM interaction history
        3. trajectory.txt - Human-readable summary

        Args:
            task_id: Task identifier
            question: Task instruction
            messages: List of conversation messages
            result: Task result dictionary with evaluation score
            output_dir: Directory to save files
        """
        import datetime

        # Extract metadata
        start_time = datetime.datetime.now().isoformat()
        success = result.get("success", False)
        evaluation_score = result.get("evaluation_score", 0.0)

        # Parse messages to extract action steps
        steps = []
        step_num = 0

        for i, msg in enumerate(messages):
            # Handle both dict and object types
            if hasattr(msg, 'role'):
                role = msg.role
                content = getattr(msg, 'content', '')
                tool_calls = getattr(msg, 'tool_calls', None)
            else:
                role = msg.get('role', '')
                content = msg.get('content', '')
                tool_calls = msg.get('tool_calls', None)

            # Track assistant tool calls (actions)
            if role == "assistant" and tool_calls:
                for tool_call in tool_calls:
                    step_num += 1

                    # Extract tool call info
                    if hasattr(tool_call, 'function'):
                        tool_name = tool_call.function.name
                        tool_args_str = tool_call.function.arguments
                    else:
                        tool_name = tool_call.get('function', {}).get('name', '')
                        tool_args_str = tool_call.get('function', {}).get('arguments', '{}')

                    # Parse arguments
                    try:
                        tool_args = json.loads(tool_args_str) if isinstance(tool_args_str, str) else tool_args_str
                    except:
                        tool_args = {}

                    # Find corresponding tool result in next messages
                    tool_result = None
                    tool_status = "unknown"
                    observation_files = []

                    for j in range(i + 1, len(messages)):
                        next_msg = messages[j]
                        if hasattr(next_msg, 'role'):
                            next_role = next_msg.role
                            next_content = getattr(next_msg, 'content', '')
                            next_tool_call_id = getattr(next_msg, 'tool_call_id', None)
                        else:
                            next_role = next_msg.get('role', '')
                            next_content = next_msg.get('content', '')
                            next_tool_call_id = next_msg.get('tool_call_id', None)

                        if next_role == "tool":
                            # Parse tool response
                            try:
                                tool_response_data = json.loads(next_content) if isinstance(next_content, str) else next_content
                                tool_result = tool_response_data.get('response', next_content)
                                tool_status = tool_response_data.get('status', 'unknown')

                                # Check for observation files
                                observation = tool_response_data.get('observation', {})
                                if observation.get('screenshot_file'):
                                    observation_files.append(observation['screenshot_file'])
                                if observation.get('a11y_tree_file'):
                                    observation_files.append(observation['a11y_tree_file'])
                            except:
                                tool_result = next_content
                            break

                    # Add step to trajectory
                    step_data = {
                        "step": step_num,
                        "action": tool_name,
                        "parameters": tool_args,
                        "result": tool_result,
                        "status": tool_status,
                        "observation_files": observation_files
                    }
                    steps.append(step_data)

        # 1. Save trajectory.json (simplified action trace)
        trajectory_data = {
            "task_id": task_id,
            "task_question": question,
            "model": self.config.model_name,
            "total_steps": len(steps),
            "success": success,
            "evaluation_score": evaluation_score,
            "timestamp": start_time,
            "steps": steps
        }

        trajectory_file = os.path.join(output_dir, "trajectory.json")
        with open(trajectory_file, "w", encoding="utf-8") as f:
            json.dump(trajectory_data, f, indent=2, ensure_ascii=False)

        # 2. Save conversation.json (complete LLM history)
        # Convert messages to serializable format
        serializable_messages = []
        for msg in messages:
            if hasattr(msg, 'model_dump'):
                # OpenAI message object with model_dump method
                serializable_messages.append(msg.model_dump())
            elif hasattr(msg, '__dict__'):
                # Convert object to dict
                msg_dict = {}
                for key in ['role', 'content', 'tool_calls', 'tool_call_id', 'name']:
                    if hasattr(msg, key):
                        value = getattr(msg, key)
                        # Handle tool_calls specially
                        if key == 'tool_calls' and value:
                            msg_dict[key] = [
                                {
                                    'id': getattr(tc, 'id', ''),
                                    'type': getattr(tc, 'type', 'function'),
                                    'function': {
                                        'name': tc.function.name if hasattr(tc, 'function') else '',
                                        'arguments': tc.function.arguments if hasattr(tc, 'function') else ''
                                    }
                                }
                                for tc in value
                            ]
                        else:
                            msg_dict[key] = value
                serializable_messages.append(msg_dict)
            else:
                # Already a dict
                serializable_messages.append(msg)

        conversation_data = {
            "task_id": task_id,
            "task_question": question,
            "model": self.config.model_name,
            "timestamp": start_time,
            "messages": serializable_messages
        }

        conversation_file = os.path.join(output_dir, "conversation.json")
        with open(conversation_file, "w", encoding="utf-8") as f:
            json.dump(conversation_data, f, indent=2, ensure_ascii=False)

        # 3. Save trajectory.txt (human-readable summary)
        trajectory_txt = []
        trajectory_txt.append(f"Task ID: {task_id}")
        trajectory_txt.append(f"Question: {question}")
        trajectory_txt.append(f"Model: {self.config.model_name}")
        trajectory_txt.append(f"Timestamp: {start_time}")
        trajectory_txt.append(f"Success: {success}")
        trajectory_txt.append(f"Evaluation Score: {evaluation_score:.2f}")
        trajectory_txt.append(f"Total Steps: {len(steps)}")
        trajectory_txt.append("")
        trajectory_txt.append("=" * 80)
        trajectory_txt.append("TRAJECTORY")
        trajectory_txt.append("=" * 80)
        trajectory_txt.append("")

        for step in steps:
            trajectory_txt.append(f"Step {step['step']}: {step['action']}")
            trajectory_txt.append(f"  Parameters: {json.dumps(step['parameters'], ensure_ascii=False)}")
            trajectory_txt.append(f"  Status: {step['status']}")
            if step['result']:
                result_str = str(step['result'])
                if len(result_str) > 200:
                    result_str = result_str[:200] + "..."
                trajectory_txt.append(f"  Result: {result_str}")
            if step['observation_files']:
                trajectory_txt.append(f"  Observations: {', '.join(step['observation_files'])}")
            trajectory_txt.append("")

        trajectory_txt_file = os.path.join(output_dir, "trajectory.txt")
        with open(trajectory_txt_file, "w", encoding="utf-8") as f:
            f.write("\n".join(trajectory_txt))

        print(f"   Trajectory saved:")
        print(f"     - {trajectory_file}")
        print(f"     - {conversation_file}")
        print(f"     - {trajectory_txt_file}")

    def run_benchmark(self, parallel: bool = False, output_dir: str = "results") -> List[Dict[str, Any]]:
        """
        Run agent on all benchmark tasks.

        Args:
            parallel: Whether to run tasks in parallel
            output_dir: Output directory for results

        Returns:
            List of results
        """
        if not self.benchmark:
            raise ValueError("Benchmark not loaded")

        print(f"\nüöÄ Starting benchmark execution...")
        print(f"   Tasks: {len(self.benchmark.items)}")
        print(f"   Parallel: {parallel}")
        print(f"   Max workers: {self.config.max_workers}")

        # Start environment (initialize resources for all tasks)
        self.environment.env_start()

        try:
            # Prepare tasks - always include full metadata
            # Environment decides which fields it needs
            tasks = [
                {
                    "id": item.id,
                    "question": item.question,
                    "answer": getattr(item, 'answer', None),
                    "metadata": getattr(item, 'metadata', {})
                }
                for item in self.benchmark.items
            ]

            if parallel and len(tasks) > 1:
                # Run in parallel
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                    futures = [
                        executor.submit(self.run_single_task, task, output_dir)
                        for task in tasks
                    ]

                    self.results = []
                    for future in concurrent.futures.as_completed(futures):
                        result = future.result()
                        self.results.append(result)
                        # Write result immediately after completion
                        if self.config.save_results:
                            self._write_single_result(result, output_dir=output_dir)
            else:
                # Run sequentially
                self.results = []
                for task in tasks:
                    result = self.run_single_task(task, output_dir)
                    self.results.append(result)
                    # Write result immediately after completion
                    if self.config.save_results:
                        self._write_single_result(result, output_dir=output_dir)
            print(f"\n‚úÖ Benchmark execution completed!")
            print(f"   Successful: {sum(1 for r in self.results if r['success'])}")
            print(f"   Failed: {sum(1 for r in self.results if not r['success'])}")

            return self.results

        except KeyboardInterrupt:
            print(f"\n‚ö†Ô∏è  Benchmark execution interrupted by user (Ctrl+C)")
            raise

        except Exception as e:
            print(f"\n‚ùå Benchmark execution failed with exception: {e}")
            raise

        finally:
            # Close environment (cleanup resources used across all tasks)
            try:
                print(f"\nüßπ Closing environment...")
                self.environment.env_close()
                print(f"   Environment closed successfully")
            except Exception as cleanup_error:
                print(f"‚ö†Ô∏è  Warning: Failed to close environment: {cleanup_error}")
    
    def evaluate_results(self) -> Dict[str, Any]:
        """
        Evaluate results against benchmark ground truth.
        
        Returns:
            Evaluation summary
        """
        if not self.benchmark or not self.results:
            raise ValueError("No benchmark or results to evaluate")
        
        print(f"\nüìä Evaluating results...")
        
        # Prepare predictions
        predictions = {}
        for result in self.results:
            if result["success"]:
                predictions[result["task_id"]] = result["answer"]
        
        # Run evaluation
        evaluation_results = self.benchmark.evaluate(
            predictions, 
            metric=self.config.evaluation_metric
        )
        
        # Get summary
        summary = self.benchmark.get_summary()
        
        print(f"üìà Evaluation Summary:")
        print(f"   Metric: {summary.get('metric', 'unknown')}")
        print(f"   Average Score: {summary.get('average_score', 0.0):.3f}")
        print(f"   Perfect Matches: {summary.get('perfect_matches', 0)}")
        print(f"   Total Items: {summary.get('total_items', 0)}")
        
        return summary
    
    def _write_single_result(self, result: Dict[str, Any], output_dir: str = "results"):
        """
        Write a single result to file immediately.
        
        Args:
            result: Single result dictionary
            output_dir: Output directory
        """
        if self.output_file is None:
            # Create output directory
            os.makedirs(output_dir, exist_ok=True)
            
            # Generate output filename
            benchmark_name = self.benchmark.name if self.benchmark else "unknown"
            self.output_file = os.path.join(output_dir, f"result_{benchmark_name}.jsonl")
        
        # Append result to file
        with open(self.output_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(result, ensure_ascii=False) + "\n")
    
    def save_results(self, output_dir: str = "results") -> str:
        """
        Save results to files.
        
        Args:
            output_dir: Output directory
            
        Returns:
            Path to saved results
        """
        if not self.results:
            print("No results to save")
            return ""
        
        # Get benchmark name
        benchmark_name = self.benchmark.name if self.benchmark else "unknown"
        
        # Output file should already be created during run_benchmark
        if not self.output_file:
            # Create output directory
            os.makedirs(output_dir, exist_ok=True)
            
            # Generate output filename
            self.output_file = os.path.join(output_dir, f"result_{benchmark_name}.jsonl")
        
        print(f"üíæ Results saved to: {self.output_file}")
        
        # Save evaluation results if available
        if self.config.evaluate_results and self.benchmark.evaluation_results:
            eval_file = os.path.join(output_dir, f"evaluation_{benchmark_name}.json")
            self.benchmark.save_results(eval_file)
            print(f"üìä Evaluation results saved to: {eval_file}")
        
        return self.output_file
    
    def run(self, mode: str, data_path: str, **kwargs) -> Dict[str, Any]:
        """
        Complete run pipeline.
        
        Args:
            mode: Environment mode
            data_path: Path to benchmark data
            **kwargs: Additional configuration
            
        Returns:
            Run summary
        """
        print(f"üéØ Starting AgentFlow run...")
        print(f"   Mode: {mode}")
        print(f"   Data: {data_path}")
        
        # Setup environment
        self.setup_environment(mode, **kwargs)
        
        # Load benchmark
        self.load_benchmark(data_path, **kwargs)
        
        # Run benchmark
        output_dir = kwargs.get('output_dir', 'results')
        self.run_benchmark(parallel=kwargs.get('parallel', False), output_dir=output_dir)
        
        # Evaluate results
        if self.config.evaluate_results:
            evaluation_summary = self.evaluate_results()
        else:
            evaluation_summary = None
        
        # Save results
        if self.config.save_results:
            output_file = self.save_results(output_dir=output_dir)
        else:
            output_file = ""
        
        # Return summary
        summary = {
            "mode": mode,
            "data_path": data_path,
            "total_tasks": len(self.results),
            "successful_tasks": sum(1 for r in self.results if r["success"]),
            "failed_tasks": sum(1 for r in self.results if not r["success"]),
            "evaluation": evaluation_summary,
            "output_file": output_file
        }
        
        print(f"\nüéâ Run completed successfully!")
        print(f"   Summary: {summary}")
        
        return summary


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="AgentFlow - Agent execution with Environment and Benchmark")
    
    # Required arguments
    parser.add_argument("--mode", type=str, choices=["math", "py", "rag", "web", "osworld"],
                       required=True, help="Environment mode")
    parser.add_argument("--data", type=str, required=True,
                       help="Path to benchmark data file")
    
    # Optional arguments
    parser.add_argument("--model", type=str, default="gpt-4.1-2025-04-14",
                       help="OpenAI model name")
    parser.add_argument("--max-turns", type=int, default=100,
                       help="Maximum conversation turns")
    parser.add_argument("--max-retries", type=int, default=3,
                       help="Maximum retries per turn")
    parser.add_argument("--max-workers", type=int, default=10,
                       help="Maximum parallel workers")
    parser.add_argument("--output-dir", type=str, default="results",
                       help="Output directory for results")
    parser.add_argument("--no-eval", action="store_true",
                       help="Skip evaluation")
    parser.add_argument("--no-save", action="store_true",
                       help="Skip saving results")
    parser.add_argument("--parallel", action="store_true",
                       help="Run tasks in parallel")
    parser.add_argument("--metric", type=str, default="exact_match",
                       choices=["exact_match", "f1_score", "similarity", "contains_answer", "numeric_match", "llm_judgement"],
                       help="Evaluation metric")
    
    # Environment-specific arguments
    parser.add_argument("--web-search-top-k", type=int, default=5,
                       help="Web search top-k results")
    parser.add_argument("--web-search-type", type=str, default="search",
                       choices=["search", "news", "images"],
                       help="Web search type")
    parser.add_argument("--kb-path", type=str,
                       help="Knowledge base path for RAG mode")
    parser.add_argument("--emb-model", type=str, default="text-embedding-3-small",
                       help="Embedding model used for RAG mode")
    parser.add_argument("--emb-batchsize", type=int, default=512,
                       help="Batchsize of embedding for RAG mode")
    parser.add_argument("--index-path", type=str, default='',
                       help="Index path for RAG mode")
    parser.add_argument("--use-faiss", action="store_true",
                       help="Whether to use Faiss for RAG mode")
    parser.add_argument("--load-index", action="store_true",
                       help="Whether to load exsiting index for RAG mode")

    # OSWorld-specific arguments
    parser.add_argument("--path-to-vm", type=str,
                       help="Path to VM image for OSWorld mode (required for osworld mode)")
    parser.add_argument("--provider-name", type=str, default="vmware",
                       help="VM provider for OSWorld mode")
    parser.add_argument("--action-space", type=str, default="computer_13",
                       help="Action space for OSWorld mode")
    parser.add_argument("--observation-type", type=str, default="screenshot_a11y_tree",
                       help="Observation type for OSWorld mode")
    parser.add_argument("--screen-width", type=int, default=1920,
                       help="Screen width for OSWorld mode")
    parser.add_argument("--screen-height", type=int, default=1080,
                       help="Screen height for OSWorld mode")
    parser.add_argument("--headless", action="store_true",
                       help="Run OSWorld in headless mode")
    parser.add_argument("--snapshot-name", type=str,
                       help="VM snapshot name for OSWorld mode")
    parser.add_argument("--client-password", type=str, default="password",
                       help="VM client password for OSWorld mode")
    parser.add_argument("--sleep-after-execution", type=float, default=2.0,
                       help="Sleep time after each action in seconds for OSWorld mode")

    args = parser.parse_args()

    # Validate OSWorld required parameters
    if args.mode == "osworld" and not args.path_to_vm:
        parser.error("--path-to-vm is required when using --mode osworld")

    # Create configuration
    config = AgentConfig(
        model_name=args.model,
        max_turns=args.max_turns,
        max_retries=args.max_retries,
        max_workers=args.max_workers,
        save_results=not args.no_save,
        evaluate_results=not args.no_eval,
        evaluation_metric=args.metric
    )

    # Prepare environment-specific arguments
    env_kwargs = {}
    if args.mode == "web":
        env_kwargs.update({
            "web_search_top_k": args.web_search_top_k,
            "web_search_type": args.web_search_type
        })
    elif args.mode == "rag" and args.kb_path:
        from tools.rag_tools import get_rag_index_class
        use_faiss_flag = args.use_faiss
        IndexClass = get_rag_index_class(use_faiss=use_faiss_flag)
        rag_index = None
        client = openai.OpenAI(
            api_key=openai.api_key,
            base_url=openai.base_url 
        )
        if args.load_index:
            try:
                print(f"Attempting to load RAG index from {args.index_path}")
                rag_index = IndexClass.load_index(args.index_path, client)
            except FileNotFoundError:
                print(f"No existing index found at {args.index_path}. Will build a new one.")
        
        if rag_index is None:
            if not args.kb_path:
                raise ValueError("--kb-path is required to build a new RAG index.")
            
            print(f"Building new RAG index from {args.kb_path}...")
            rag_index = IndexClass(client, model=args.emb_model)
            rag_index.build_index(file_path=args.kb_path, batch_size=args.emb_batchsize)
            
            if args.index_path:
                rag_index.save_index(args.index_path)

        env_kwargs["rag_index"] = rag_index    # will pass it to RAGEnvironment

    elif args.mode == "osworld":
        env_kwargs.update({
            "path_to_vm": args.path_to_vm,
            "provider_name": args.provider_name,
            "action_space": args.action_space,
            "observation_type": args.observation_type,
            "screen_width": args.screen_width,
            "screen_height": args.screen_height,
            "headless": args.headless,
            "client_password": args.client_password,
            "sleep_after_execution": args.sleep_after_execution,
        })
        if args.snapshot_name:
            env_kwargs["snapshot_name"] = args.snapshot_name

    # Create and run agent
    runner = AgentRunner(config)
    
    try:
        summary = runner.run(
            mode=args.mode,
            data_path=args.data,
            parallel=args.parallel,
            output_dir=args.output_dir,
            **env_kwargs
        )
        
        print(f"\nüèÅ Final Summary:")
        for key, value in summary.items():
            print(f"   {key}: {value}")
            
    except Exception as e:
        print(f"‚ùå Run failed: {str(e)}")
        raise


if __name__ == "__main__":
    main()