import os
import json
import math
import numpy as np
import struct
import csv
import torch
from typing import Dict, List, Any, Optional, Sequence
from tqdm import trange, tqdm
import sys
import gc
import pickle
from multiprocessing import Pool, cpu_count
import time
from abc import ABC, abstractmethod

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pdb

# Check if Faiss is available
_FAISS_AVAILABLE = True
try:
    import faiss
except ImportError:
    _FAISS_AVAILABLE = False

# Check if sentence_transformers is available
_SENTENCE_TRANSFORMERS_AVAILABLE = True
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    _SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ sentence_transformers æœªå®‰è£…,æœ¬åœ°embeddingåŠŸèƒ½ä¸å¯ç”¨")

try:
    from transformers import AutoTokenizer, AutoConfig, BertModel  # type: ignore
except ImportError:
    AutoTokenizer = None  # type: ignore
    AutoConfig = None  # type: ignore
    BertModel = None  # type: ignore

_GAINRAG_TRANSFORMERS_AVAILABLE = all(
    dependency is not None for dependency in (AutoTokenizer, AutoConfig, BertModel)
)


# ============================================================================
# Faiss è¾…åŠ©å‡½æ•°
# ============================================================================

def _suggest_ivf_params(num_vectors: int,
                        max_nlist: int = 4096,
                        max_nprobe: int = 16) -> tuple[int, int]:
    """
    æ ¹æ®å‘é‡æ•°é‡ç»™å‡ºåˆé€‚çš„IVFå‚æ•°

    Returns:
        (nlist, nprobe)
    """
    if num_vectors <= 0:
        return 1, 1
    nlist = max(1, min(max_nlist, int(np.sqrt(num_vectors))))
    nprobe = max(1, min(max_nprobe, nlist))
    return nlist, nprobe


# ============================================================================
# å¤šè¿›ç¨‹è¾…åŠ©å‡½æ•°ï¼ˆå¿…é¡»åœ¨æ¨¡å—çº§åˆ«å®šä¹‰æ‰èƒ½è¢«pickleï¼‰
# ============================================================================

def _encode_chunk_worker(args):
    """
    å¤šè¿›ç¨‹workerå‡½æ•°ï¼šç¼–ç ä¸€æ‰¹æ–‡æœ¬
    å¿…é¡»åœ¨æ¨¡å—çº§åˆ«å®šä¹‰ä»¥æ”¯æŒpickleåºåˆ—åŒ–
    
    Args:
        args: (chunk_texts, model_name, max_seq_length, batch_size_inner, worker_id)
    
    Returns:
        ç¼–ç åçš„å‘é‡æ•°ç»„
    """
    chunk_texts, model_name, max_seq_length, batch_size_inner, worker_id = args
    
    import os
    pid = os.getpid()
    
    # åœ¨workerè¿›ç¨‹ä¸­åŠ è½½æ¨¡å‹
    from sentence_transformers import SentenceTransformer
    import numpy as np
    import time
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # åŠ è½½æ¨¡å‹ï¼ˆè¿™æ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†ï¼‰
    print(f"  Worker {worker_id} (PID {pid}): å¼€å§‹åŠ è½½æ¨¡å‹...")
    model = SentenceTransformer(model_name, device='cpu')
    model.max_seq_length = max_seq_length
    load_time = time.time() - start_time
    print(f"  Worker {worker_id} (PID {pid}): æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {load_time:.1f}ç§’")
    
    # å¤„ç†æ–‡æœ¬
    print(f"  Worker {worker_id} (PID {pid}): å¼€å§‹å¤„ç† {len(chunk_texts):,} æ¡æ–‡æœ¬...")
    chunk_embeddings = []
    process_start = time.time()
    
    for i in range(0, len(chunk_texts), batch_size_inner):
        batch = chunk_texts[i:i+batch_size_inner]
        batch_emb = model.encode(
            batch,
            convert_to_numpy=True,
            show_progress_bar=False,
            normalize_embeddings=False
        )
        chunk_embeddings.append(batch_emb)
    
    process_time = time.time() - process_start
    total_time = time.time() - start_time
    print(f"  Worker {worker_id} (PID {pid}): å¤„ç†å®Œæˆï¼Œå¤„ç†è€—æ—¶ {process_time:.1f}ç§’ï¼Œæ€»è€—æ—¶ {total_time:.1f}ç§’")
    
    return np.vstack(chunk_embeddings).astype(np.float32)


class DiskBasedChunks:
    """
    Lazy loading list-like object for large JSONL files.
    Uses an offset file to jump to specific lines without reading the whole file.
    """
    def __init__(self, jsonl_path: str, offset_path: Optional[str] = None):
        self.jsonl_path = jsonl_path
        if offset_path is None:
            # If path ends with .jsonl, replace it with .offsets, otherwise append
            if jsonl_path.endswith('.jsonl'):
                offset_path = jsonl_path[:-6] + ".offsets"
            else:
                offset_path = jsonl_path + ".offsets"
        self.offset_path = offset_path
        
        if not os.path.exists(self.jsonl_path):
             raise FileNotFoundError(f"JSONL file not found: {self.jsonl_path}")
        
        if not os.path.exists(self.offset_path):
             raise FileNotFoundError(f"Offset file not found: {self.offset_path}. Please run convert_index.py first.")
             
        self._offsets = self._load_offsets()
        self._file = open(self.jsonl_path, 'rb') # Binary mode for precise seek
        
    def _load_offsets(self) -> List[int]:
        with open(self.offset_path, 'rb') as f:
            data = f.read()
        # Unpack all offsets (unsigned long long, 8 bytes)
        count = len(data) // 8
        return list(struct.unpack(f'<{count}Q', data))
        
    def __len__(self):
        return len(self._offsets)
        
    def __getitem__(self, idx):
        if isinstance(idx, slice):
            # Support slicing (e.g. for batch retrieval)
            # Note: This will be slow if slice is large, but functional
            start, stop, step = idx.indices(len(self))
            results = []
            for i in range(start, stop, step):
                results.append(self[i])
            return results

        if idx < 0:
            idx += len(self)
            
        if idx < 0 or idx >= len(self._offsets):
            raise IndexError("DiskBasedChunks index out of range")
            
        offset = self._offsets[idx]
        self._file.seek(offset)
        line = self._file.readline()
        return json.loads(line.decode('utf-8'))
        
    def __del__(self):
        if hasattr(self, '_file'):
            self._file.close()

# ============================================================================
# RAG ç´¢å¼•åŸºç±»
# ============================================================================



class BaseRAGIndex(ABC):
    """
    RAG ç´¢å¼•å…¬å…±åŸºç±»ï¼Œè´Ÿè´£æ¨¡å‹åŠ è½½ã€æ–‡æœ¬è§£æä¸embeddingç”Ÿæˆã€‚
    å­ç±»éœ€å®ç°å‘é‡å­˜å‚¨ã€ç´¢å¼•æŒä¹…åŒ–åŠæŸ¥è¯¢ç­‰ç‰¹å®šé€»è¾‘ã€‚
    """

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 device: str = "cuda", max_seq_length: int = 512,
                 embedding_devices: Optional[Sequence[str]] = None) -> None:
        if not _SENTENCE_TRANSFORMERS_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£… sentence_transformers: pip install sentence-transformers")

        self.model_name = model_name
        self.requested_device = device
        self.embedding_devices: List[str] = self._normalize_embedding_devices(embedding_devices)
        self._multi_gpu_devices: List[str] = list(self.embedding_devices) if len(self.embedding_devices) > 1 else []
        self._multi_gpu_failed = False

        primary_device = self.embedding_devices[0] if self.embedding_devices else device
        if device.lower().startswith("cpu") and self.embedding_devices:
            print("âš ï¸ æŒ‡å®šäº† embedding_devicesï¼Œä½† device=CPUï¼Œå°†ä¼˜å…ˆä½¿ç”¨ GPU è®¾å¤‡è¿›è¡Œç¼–ç ")

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name, device=primary_device)
        model_max_length = self.model.get_max_seq_length()
        actual_max_length = min(max_seq_length, model_max_length)
        self.model.max_seq_length = actual_max_length
        if actual_max_length < max_seq_length:
            print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹æœ€å¤§æ”¯æŒé•¿åº¦ä¸º {model_max_length}ï¼Œå·²å°† max_seq_length ä» {max_seq_length} è°ƒæ•´ä¸º {actual_max_length}")
        print(f"âœ“ æ¨¡å‹å·²åŠ è½½åˆ° {primary_device}, æœ€å¤§åºåˆ—é•¿åº¦: {actual_max_length}")
        if self._multi_gpu_devices:
            print(f"  - å¤šGPUç¼–ç è®¾å¤‡: {', '.join(self._multi_gpu_devices)}")

        self.chunks: List[Dict[str, Any]] = []
        self.device = primary_device
        self.max_seq_length = actual_max_length

    @staticmethod
    def _normalize_embedding_devices(devices: Optional[Sequence[str]]) -> List[str]:
        normalized: List[str] = []
        if not devices:
            return normalized
        seen: set[str] = set()
        for dev in devices:
            if dev is None:
                continue
            original = str(dev).strip()
            if not original:
                continue
            lowered = original.lower()
            if lowered == "cpu":
                continue
            if lowered == "cuda":
                token = "cuda:0"
            elif lowered.startswith("cuda:"):
                token = f"cuda:{lowered.split(':', 1)[1]}"
            elif lowered.startswith("cuda"):
                suffix = lowered[len("cuda") :].lstrip(":")
                token = f"cuda:{suffix}" if suffix else "cuda:0"
            elif lowered.isdigit():
                token = f"cuda:{lowered}"
            else:
                token = original
            if token not in seen:
                normalized.append(token)
                seen.add(token)
        return normalized

    @staticmethod
    def _normalize_embeddings(vectors: np.ndarray) -> np.ndarray:
        if vectors is None:
            raise ValueError("å‘é‡ä¸ºç©ºï¼Œæ— æ³•å½’ä¸€åŒ–")
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        norms[norms == 0] = 1e-9
        return vectors / norms

    def _resolve_num_workers(self, num_workers: int) -> int:
        if num_workers == 0:
            if self.device == 'cpu':
                resolved = max(1, cpu_count() // 2)
                print(f"ğŸ’¡ è‡ªåŠ¨è®¾ç½®workeræ•°ä¸ºCPUæ ¸å¿ƒæ•°çš„ä¸€åŠ: {resolved}")
                return resolved
            return 1
        return num_workers

    def _combine_sample_text(self, sample: Dict[str, Any]) -> Optional[str]:
        if not isinstance(sample, dict):
            return None
        text = sample.get("text")
        if not isinstance(text, str) or not text:
            return None
        title = sample.get("title", "")
        return f"{title}\n\n{text}" if title else text

    def _prepare_embedding_texts(self) -> List[str]:
        print("æ­£åœ¨åˆå¹¶titleå’Œtext...")
        texts: List[str] = []
        for chunk in tqdm(self.chunks, desc="å¤„ç†æ–‡æœ¬"):
            combined_text = self._combine_sample_text(chunk)
            if combined_text:
                texts.append(combined_text)
        print(f"âœ“ æ–‡æœ¬åˆå¹¶å®Œæˆ")
        print()
        return texts

    def _load_kb_file(self, file_path: str) -> List[Any]:
        kb_content: List[Any] = []
        if file_path.endswith('.jsonl'):
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in tqdm(f, desc="è¯»å–JSONLæ–‡ä»¶"):
                    if line.strip():
                        kb_content.append(json.loads(line))
        elif file_path.endswith('.json'):
            with open(file_path, 'r', encoding='utf-8') as f:
                kb_content = json.load(f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
        return kb_content

    def _extract_valid_chunks(self, kb_content: List[Any],
                              max_chunks: Optional[int]) -> List[Dict[str, Any]]:
        chunks = [
            sample for sample in kb_content
            if isinstance(sample, dict) and "text" in sample and
            isinstance(sample["text"], str) and sample["text"]
        ]
        if max_chunks:
            print(f"âš ï¸  é™åˆ¶æœ€å¤§æ–‡æœ¬å—æ•°: {max_chunks}")
            chunks = chunks[:max_chunks]
        if not chunks:
            raise ValueError("æ— æ³•ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬å—")
        return chunks

    def _format_chunk_for_output(self, chunk: Any) -> str:
        if isinstance(chunk, dict):
            title = chunk.get("title", "")
            text = chunk.get("text", "")
            has_title = isinstance(title, str) and title.strip()
            has_text = isinstance(text, str) and text.strip()
            if has_title and has_text:
                return f"[{title}]\n{text}"
            if has_text:
                return text
            if has_title:
                return title
            return json.dumps(chunk, ensure_ascii=False)
        return str(chunk)

    def _get_embeddings_batch(self, texts: List[str], batch_size: int = 32,
                              show_progress: bool = True, num_workers: int = 1) -> np.ndarray:
        total_texts = len(texts)
        start_time = time.time()

        print(f"ğŸ“Š Embeddingç”Ÿæˆé…ç½®:")
        print(f"  - æ€»æ–‡æœ¬æ•°: {total_texts:,}")
        print(f"  - æ‰¹å¤§å°: {batch_size}")
        print(f"  - è®¾å¤‡: {self.device}")

        if self._multi_gpu_devices:
            print(f"  - å¤šGPUè®¾å¤‡: {', '.join(self._multi_gpu_devices)}")
            if num_workers > 1:
                print("  âš ï¸ å¤šGPUæ¨¡å¼ä¸‹å¿½ç•¥ num_workers é…ç½®")
            embeddings_multi = None
            pool = None
            try:
                if not hasattr(self.model, "start_multi_process_pool") or not hasattr(self.model, "encode_multi_process"):
                    raise AttributeError("å½“å‰ sentence_transformers ç‰ˆæœ¬ä¸æ”¯æŒå¤šè¿›ç¨‹ç¼–ç ")
                pool = self.model.start_multi_process_pool(target_devices=self._multi_gpu_devices)
                encode_kwargs = {
                    "batch_size": batch_size,
                    "normalize_embeddings": False,
                    "convert_to_numpy": True,
                }
                try:
                    embeddings_multi = self.model.encode_multi_process(
                        texts,
                        pool,
                        show_progress_bar=show_progress,
                        **encode_kwargs,
                    )
                except TypeError:
                    embeddings_multi = self.model.encode_multi_process(
                        texts,
                        pool,
                        **encode_kwargs,
                    )
            except Exception as exc:  # pylint: disable=broad-except
                print(f"âš ï¸ å¤šGPUç¼–ç å¤±è´¥({exc})ï¼Œå°†å›é€€åˆ°å•GPUæ¨¡å¼")
                self._multi_gpu_devices = []
                self._multi_gpu_failed = True
                embeddings_multi = None
            finally:
                if pool is not None:
                    try:
                        self.model.stop_multi_process_pool(pool)
                    except Exception:  # pylint: disable=broad-except
                        pass

            if embeddings_multi is not None:
                embeddings_np = np.asarray(embeddings_multi, dtype=np.float32)
                elapsed = time.time() - start_time
                speed = total_texts / elapsed if elapsed > 0 else 0.0
                print(f"\nâœ“ Embeddingç”Ÿæˆå®Œæˆ!")
                print(f"  - è€—æ—¶: {elapsed:.2f}ç§’")
                print(f"  - é€Ÿåº¦: {speed:.1f} æ–‡æœ¬/ç§’")
                print(f"  - å‘é‡å½¢çŠ¶: {embeddings_np.shape}")
                return embeddings_np

        if self.device != 'cpu' and num_workers > 1:
            print("  âš ï¸  GPUæ¨¡å¼ä¸‹ä¸æ”¯æŒå¤šè¿›ç¨‹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å•è¿›ç¨‹")
            num_workers = 1
        print(f"  - Workeræ•°: {num_workers}")

        embeddings: List[np.ndarray] = []

        if num_workers > 1 and self.device == 'cpu':
            print(f"\nğŸš€ ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ (workers={num_workers})...")
            print(f"ğŸ’¡ æç¤º: æ¯ä¸ªworkeréœ€è¦å…ˆåŠ è½½æ¨¡å‹ï¼ˆçº¦10-30ç§’ï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...\n")

            chunk_size_per_worker = (total_texts + num_workers - 1) // num_workers
            text_chunks = [texts[i:i + chunk_size_per_worker] for i in range(0, total_texts, chunk_size_per_worker)]

            args_list = [
                (chunk, self.model_name, self.max_seq_length, batch_size, idx)
                for idx, chunk in enumerate(text_chunks)
            ]

            print(f"æ¯ä¸ªworkerå¤„ç†çº¦ {chunk_size_per_worker:,} æ¡æ–‡æœ¬")
            print(f"å¼€å§‹å¯åŠ¨ {num_workers} ä¸ªworkerè¿›ç¨‹...\n")

            with Pool(processes=num_workers) as pool:
                results = []
                with tqdm(total=num_workers, desc="å¤šè¿›ç¨‹å¤„ç†", unit="worker",
                          bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
                    for result in pool.imap(_encode_chunk_worker, args_list):
                        results.append(result)
                        pbar.update(1)

            embeddings = np.vstack(results).astype(np.float32)
            print(f"\nâœ“ æ‰€æœ‰workerå®Œæˆï¼")

        else:
            iterator = range(0, total_texts, batch_size)
            if show_progress:
                iterator = tqdm(
                    iterator,
                    desc="ç”Ÿæˆembeddings",
                    unit="batch",
                    total=(total_texts + batch_size - 1) // batch_size,
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
                )

            for i in iterator:
                batch = texts[i:i + batch_size]
                batch_embeddings = self.model.encode(
                    batch,
                    convert_to_numpy=True,
                    show_progress_bar=False,
                    normalize_embeddings=False
                )
                embeddings.append(batch_embeddings)

                if (i // batch_size) % 10 == 0:
                    gc.collect()

            embeddings = np.vstack(embeddings).astype(np.float32)

        elapsed = time.time() - start_time
        speed = total_texts / elapsed if elapsed > 0 else 0.0
        print(f"\nâœ“ Embeddingç”Ÿæˆå®Œæˆ!")
        print(f"  - è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"  - é€Ÿåº¦: {speed:.1f} æ–‡æœ¬/ç§’")
        print(f"  - å‘é‡å½¢çŠ¶: {embeddings.shape}")

        return embeddings

    def build_index(self, file_path: str, batch_size: int = 64,
                    max_chunks: Optional[int] = None, num_workers: int = 1) -> None:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        num_workers = self._resolve_num_workers(num_workers)

        print(f"\n{'='*60}")
        print(f"ğŸ“‚ æ­¥éª¤ 1/4: åŠ è½½çŸ¥è¯†åº“")
        print(f"{'='*60}")
        print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
        start_time = time.time()

        kb_content = self._load_kb_file(file_path)

        load_time = time.time() - start_time
        print(f"âœ“ æ–‡ä»¶åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")

        print(f"\n{'='*60}")
        print(f"ğŸ” æ­¥éª¤ 2/4: æå–æ–‡æœ¬å—")
        print(f"{'='*60}")
        self.chunks = self._extract_valid_chunks(kb_content, max_chunks)
        print(f"âœ“ æˆåŠŸæå– {len(self.chunks):,} ä¸ªæ–‡æœ¬å—")

        print(f"\n{'='*60}")
        print(f"ğŸ¤– æ­¥éª¤ 3/4: ç”ŸæˆEmbeddings")
        print(f"{'='*60}")
        texts = self._prepare_embedding_texts()

        vectors = self._get_embeddings_batch(
            texts,
            batch_size=batch_size,
            show_progress=True,
            num_workers=num_workers
        )

        self._store_embeddings(vectors)

        print(f"\n{'='*60}")
        print(f"ğŸ“ æ­¥éª¤ 4/4: å‘é‡åå¤„ç†")
        print(f"{'='*60}")
        self._finalize_embeddings()

        gc.collect()

    @abstractmethod
    def _store_embeddings(self, vectors: np.ndarray) -> None:
        """å­ç±»è´Ÿè´£ç¼“å­˜æˆ–å†™å…¥embeddingå‘é‡ã€‚"""

    @abstractmethod
    def _finalize_embeddings(self) -> None:
        """å­ç±»è´Ÿè´£å‘é‡çš„æœ€ç»ˆå¤„ç†æµç¨‹ï¼Œä¾‹å¦‚å½’ä¸€åŒ–æˆ–æ„å»ºç´¢å¼•ã€‚"""

    def _save_checkpoint(self, checkpoint_file: str, processed_lines: int,
                         processed_chunks: int) -> None:
        checkpoint = {
            "processed_lines": processed_lines,
            "processed_chunks": processed_chunks,
            "timestamp": time.time()
        }
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint, f, indent=2)

    def build_index_streaming(self, *args, **kwargs) -> None:
        raise NotImplementedError("è¯¥ç´¢å¼•ç±»å‹æœªå®ç°æµå¼æ„å»ºï¼Œè¯·åœ¨å­ç±»ä¸­å®ç°ã€‚")

    @abstractmethod
    def save_index(self, index_path: str) -> None:
        """å­ç±»è´Ÿè´£ç´¢å¼•æŒä¹…åŒ–ã€‚"""

    @classmethod
    @abstractmethod
    def load_index(cls, index_path: str, model_name: Optional[str] = None,
                   device: str = "cuda", max_seq_length: int = 512, **kwargs) -> "BaseRAGIndex":
        """å­ç±»è´Ÿè´£ä»ç£ç›˜åŠ è½½ç´¢å¼•ã€‚"""

    @abstractmethod
    def query(self, query: str, top_k: int = 3) -> str:
        """å­ç±»è´Ÿè´£å®ç°æŸ¥è¯¢é€»è¾‘ã€‚"""


# ============================================================================
# æœ¬åœ° RAG ç´¢å¼•å®ç°
# ============================================================================

class RAGIndexLocal(BaseRAGIndex):
    """
    æœ¬åœ°RAGç´¢å¼•å®ç°,ä½¿ç”¨sentence_transformersè¿›è¡Œembedding
    ä¼˜åŒ–å†…å­˜æ¶ˆè€—å’Œæ•ˆç‡,é€‚åˆå¤§è§„æ¨¡çŸ¥è¯†åº“
    """
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2", 
                 device: str = "cuda", max_seq_length: int = 512,
                 embedding_devices: Optional[Sequence[str]] = None):
        super().__init__(
            model_name=model_name,
            device=device,
            max_seq_length=max_seq_length,
            embedding_devices=embedding_devices
        )
        self.vectors = None
        self.normalized_vectors = None

    def _store_embeddings(self, vectors: np.ndarray) -> None:
        self.vectors = vectors.astype(np.float32)

    def _finalize_embeddings(self) -> None:
        self._normalize_vectors()

    def _normalize_vectors(self):
        """å½’ä¸€åŒ–å‘é‡ä»¥åŠ é€Ÿä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—"""
        if self.vectors is not None:
            self.normalized_vectors = self._normalize_embeddings(self.vectors)
            print("âœ“ å‘é‡å·²å½’ä¸€åŒ–")

    def build_index_streaming(self, file_path: str, index_path: str, 
                             batch_size: int = 64, chunk_size: int = 10000,
                             max_chunks: Optional[int] = None, num_workers: int = 1,
                             resume: bool = True):
        """
        æµå¼æ„å»ºç´¢å¼•ï¼Œæ”¯æŒè¶…å¤§è§„æ¨¡çŸ¥è¯†åº“ï¼ˆè¾¹åŠ è½½è¾¹å¤„ç†è¾¹ä¿å­˜ï¼‰
        
        Args:
            file_path: çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„ (ä»…æ”¯æŒjsonlæ ¼å¼)
            index_path: ç´¢å¼•ä¿å­˜ç›®å½•
            batch_size: embeddingç”Ÿæˆçš„æ‰¹é‡å¤§å°
            chunk_size: æ¯æ¬¡å¤„ç†çš„æ–‡æœ¬å—æ•°é‡ï¼ˆæ§åˆ¶å†…å­˜ä½¿ç”¨ï¼‰
            max_chunks: æœ€å¤§å¤„ç†çš„æ–‡æœ¬å—æ•°é‡(ç”¨äºæµ‹è¯•),Noneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
            num_workers: å¹¶è¡Œå¤„ç†çš„workeræ•°é‡
            resume: æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        """
        if not file_path.endswith('.jsonl'):
            raise ValueError("æµå¼æ„å»ºä»…æ”¯æŒJSONLæ ¼å¼æ–‡ä»¶")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è‡ªåŠ¨ç¡®å®šworkeræ•°é‡
        num_workers = self._resolve_num_workers(num_workers)
        
        os.makedirs(index_path, exist_ok=True)
        
        # æ£€æŸ¥checkpoint
        checkpoint_file = os.path.join(index_path, "checkpoint.json")
        start_line = 0
        processed_chunks = 0
        
        if resume and os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
                start_line = checkpoint.get("processed_lines", 0)
                processed_chunks = checkpoint.get("processed_chunks", 0)
            print(f"ğŸ’¾ ä»checkpointæ¢å¤: å·²å¤„ç† {processed_chunks:,} ä¸ªæ–‡æœ¬å—")
        
        print(f"\n{'='*60}")
        print(f"ğŸŒŠ æµå¼ç´¢å¼•æ„å»ºæ¨¡å¼")
        print(f"{'='*60}")
        print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
        print(f"ç´¢å¼•è·¯å¾„: {index_path}")
        print(f"æ¯æ‰¹å¤„ç†: {chunk_size:,} ä¸ªæ–‡æœ¬å—")
        print(f"Batchå¤§å°: {batch_size}")
        print(f"Workeræ•°: {num_workers}")
        print(f"{'='*60}\n")
        
        total_start_time = time.time()
        batch_chunks = []
        batch_texts = []
        current_line = 0
        batch_num = processed_chunks // chunk_size
        
        # æ‰“å¼€æ–‡ä»¶å¹¶é€è¡Œè¯»å–
        with open(file_path, 'r', encoding='utf-8') as f:
            # è·³è¿‡å·²å¤„ç†çš„è¡Œ
            if start_line > 0:
                print(f"â­ï¸  è·³è¿‡å·²å¤„ç†çš„ {start_line:,} è¡Œ...")
                for _ in range(start_line):
                    next(f)
                current_line = start_line
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºæ–‡ä»¶è¯»å–è¿›åº¦
            pbar = tqdm(desc="å¤„ç†æ–‡æœ¬å—", unit="å—", initial=processed_chunks)
            
            for line in f:
                current_line += 1
                
                if not line.strip():
                    continue
                
                try:
                    sample = json.loads(line)
                    
                    # éªŒè¯æ•°æ®æ ¼å¼
                    if not (isinstance(sample, dict) and "text" in sample and 
                           isinstance(sample["text"], str) and sample["text"]):
                        continue
                    
                    # åˆå¹¶titleå’Œtext
                    title = sample.get("title", "")
                    text = sample.get("text", "")
                    combined_text = f"{title}\n\n{text}" if title else text
                    
                    batch_chunks.append(sample)
                    batch_texts.append(combined_text)
                    
                    # è¾¾åˆ°chunk_sizeæˆ–è¾¾åˆ°max_chunksé™åˆ¶æ—¶å¤„ç†è¿™ä¸€æ‰¹
                    if len(batch_chunks) >= chunk_size or \
                       (max_chunks and processed_chunks + len(batch_chunks) >= max_chunks):
                        
                        # å¤„ç†å½“å‰æ‰¹æ¬¡
                        self._process_and_save_batch(
                            batch_chunks, batch_texts, batch_num,
                            index_path, batch_size, num_workers
                        )
                        
                        processed_chunks += len(batch_chunks)
                        pbar.update(len(batch_chunks))
                        batch_num += 1
                        
                        # ä¿å­˜checkpoint
                        self._save_checkpoint(checkpoint_file, current_line, processed_chunks)
                        
                        # æ¸…ç©ºæ‰¹æ¬¡
                        batch_chunks = []
                        batch_texts = []
                        gc.collect()
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°max_chunks
                        if max_chunks and processed_chunks >= max_chunks:
                            print(f"\nâš ï¸  å·²è¾¾åˆ°æœ€å¤§æ–‡æœ¬å—æ•°é™åˆ¶: {max_chunks:,}")
                            break
                
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  è·³è¿‡æ— æ•ˆJSON (è¡Œ{current_line}): {e}")
                    continue
            
            # å¤„ç†å‰©ä½™çš„æ‰¹æ¬¡
            if batch_chunks:
                self._process_and_save_batch(
                    batch_chunks, batch_texts, batch_num,
                    index_path, batch_size, num_workers
                )
                processed_chunks += len(batch_chunks)
                pbar.update(len(batch_chunks))
                self._save_checkpoint(checkpoint_file, current_line, processed_chunks)
            
            pbar.close()
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç´¢å¼•
        print(f"\n{'='*60}")
        print(f"ğŸ”— åˆå¹¶ç´¢å¼•åˆ†ç‰‡")
        print(f"{'='*60}")
        self._merge_index_shards(index_path, batch_num + 1)
        
        # åˆ é™¤checkpointï¼ˆæ„å»ºå®Œæˆï¼‰
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
        
        total_time = time.time() - total_start_time
        print(f"\n{'='*60}")
        print(f"âœ… æµå¼ç´¢å¼•æ„å»ºå®Œæˆ!")
        print(f"{'='*60}")
        print(f"æ€»æ–‡æœ¬å—æ•°: {processed_chunks:,}")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"å¹³å‡é€Ÿåº¦: {processed_chunks/total_time:.1f} å—/ç§’")
        print(f"ç´¢å¼•è·¯å¾„: {index_path}")
        print(f"{'='*60}\n")
    
    def _process_and_save_batch(self, chunks: List[Dict], texts: List[str],
                                batch_num: int, index_path: str,
                                batch_size: int, num_workers: int):
        """å¤„ç†å¹¶ä¿å­˜ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
        print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ #{batch_num} ({len(chunks):,} ä¸ªæ–‡æœ¬å—)...")
        
        start_time = time.time()
        
        # ç”Ÿæˆembeddings
        vectors = self._get_embeddings_batch(
            texts,
            batch_size=batch_size,
            show_progress=False,  # å¤–å±‚å·²æœ‰è¿›åº¦æ¡
            num_workers=num_workers
        )
        
        # ä¿å­˜è¿™ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        batch_dir = os.path.join(index_path, f"batch_{batch_num:04d}")
        os.makedirs(batch_dir, exist_ok=True)
        
        # ä¿å­˜chunks
        chunks_file = os.path.join(batch_dir, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False)
        
        # ä¿å­˜vectors
        vectors_file = os.path.join(batch_dir, "vectors.npy")
        np.save(vectors_file, vectors)
        
        elapsed = time.time() - start_time
        speed = len(chunks) / elapsed
        print(f"  âœ“ æ‰¹æ¬¡ #{batch_num} å®Œæˆ - è€—æ—¶: {elapsed:.2f}ç§’, é€Ÿåº¦: {speed:.1f} å—/ç§’")
    
    def _merge_index_shards(self, index_path: str, num_batches: int):
        """åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç´¢å¼•åˆ†ç‰‡"""
        print(f"æ­£åœ¨åˆå¹¶ {num_batches} ä¸ªæ‰¹æ¬¡...")
        
        all_chunks = []
        all_vectors = []
        
        for batch_num in tqdm(range(num_batches), desc="åŠ è½½æ‰¹æ¬¡"):
            batch_dir = os.path.join(index_path, f"batch_{batch_num:04d}")
            
            if not os.path.exists(batch_dir):
                continue
            
            # åŠ è½½chunks
            chunks_file = os.path.join(batch_dir, "chunks.json")
            if os.path.exists(chunks_file):
                with open(chunks_file, 'r', encoding='utf-8') as f:
                    chunks = json.load(f)
                    all_chunks.extend(chunks)
            
            # åŠ è½½vectors
            vectors_file = os.path.join(batch_dir, "vectors.npy")
            if os.path.exists(vectors_file):
                vectors = np.load(vectors_file)
                all_vectors.append(vectors)
        
        # åˆå¹¶vectors
        if all_vectors:
            self.vectors = np.vstack(all_vectors).astype(np.float32)
            self.chunks = all_chunks
            
            print(f"âœ“ åˆå¹¶å®Œæˆ: {len(self.chunks):,} ä¸ªæ–‡æœ¬å—")
            
            # å½’ä¸€åŒ–å‘é‡
            print("æ­£åœ¨å½’ä¸€åŒ–å‘é‡...")
            self._normalize_vectors()
            
            # ä¿å­˜æœ€ç»ˆç´¢å¼•
            print("æ­£åœ¨ä¿å­˜æœ€ç»ˆç´¢å¼•...")
            self.save_index(index_path)
            
            # æ¸…ç†ä¸´æ—¶æ‰¹æ¬¡æ–‡ä»¶
            print("æ­£åœ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            for batch_num in range(num_batches):
                batch_dir = os.path.join(index_path, f"batch_{batch_num:04d}")
                if os.path.exists(batch_dir):
                    import shutil
                    shutil.rmtree(batch_dir)
            
            print("âœ“ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")

    def save_index(self, index_path: str):
        """
        ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
        
        Args:
            index_path: ä¿å­˜ç´¢å¼•çš„ç›®å½•è·¯å¾„
        """
        if self.vectors is None or not self.chunks:
            raise ValueError("ç´¢å¼•ä¸ºç©º,æ— æ³•ä¿å­˜ã€‚è¯·å…ˆè°ƒç”¨ build_index()")

        os.makedirs(index_path, exist_ok=True)
        
        # ä¿å­˜æ–‡æœ¬å—
        chunks_file = os.path.join(index_path, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å‘é‡
        vectors_file = os.path.join(index_path, "vectors.npy")
        np.save(vectors_file, self.vectors)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "model_name": self.model_name,
            "device": self.device,
            "max_seq_length": self.max_seq_length,
            "num_chunks": len(self.chunks),
            "vector_dim": self.vectors.shape[1],
            "embedding_devices": self.embedding_devices,
        }
        metadata_file = os.path.join(index_path, "metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ ç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
        print(f"  - æ–‡æœ¬å—: {len(self.chunks)}")
        print(f"  - å‘é‡ç»´åº¦: {self.vectors.shape}")

    @classmethod
    def load_index(cls, index_path: str, model_name: Optional[str] = None,
                   device: str = "cuda", max_seq_length: int = 512,
                   embedding_devices: Optional[Sequence[str]] = None, **kwargs):
        """
        ä»ç£ç›˜åŠ è½½ç´¢å¼•
        
        Args:
            index_path: ç´¢å¼•ç›®å½•è·¯å¾„
            model_name: æ¨¡å‹åç§°(å¦‚æœä¸ºNoneåˆ™ä»metadataè¯»å–)
            device: è¿è¡Œè®¾å¤‡
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
        
        Returns:
            åŠ è½½çš„RAGIndexLocalå®ä¾‹
        """
        chunks_file = os.path.join(index_path, "chunks.json")
        vectors_file = os.path.join(index_path, "vectors.npy")
        metadata_file = os.path.join(index_path, "metadata.json")

        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        if not os.path.exists(chunks_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°chunksæ–‡ä»¶: {chunks_file}")
        if not os.path.exists(vectors_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°vectorsæ–‡ä»¶: {vectors_file}")
        
        # è¯»å–å…ƒæ•°æ®
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            if model_name is None:
                model_name = metadata.get("model_name", "sentence-transformers/all-MiniLM-L6-v2")
            if embedding_devices is None:
                embedding_devices = metadata.get("embedding_devices")
            print(f"âœ“ ä»å…ƒæ•°æ®è¯»å–é…ç½®: {metadata}")
        else:
            if model_name is None:
                model_name = "sentence-transformers/all-MiniLM-L6-v2"
            print("âš ï¸ æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶,ä½¿ç”¨é»˜è®¤é…ç½®")

        # åˆ›å»ºå®ä¾‹
        instance = cls(
            model_name=model_name,
            device=device,
            max_seq_length=max_seq_length,
            embedding_devices=embedding_devices,
        )
        
        # åŠ è½½æ•°æ®
        with open(chunks_file, 'r', encoding='utf-8') as f:
            instance.chunks = json.load(f)
        
        instance.vectors = np.load(vectors_file)
        instance._normalize_vectors()
        
        print(f"âœ“ æˆåŠŸåŠ è½½ç´¢å¼•: {len(instance.chunks)} ä¸ªæ–‡æœ¬å—, å‘é‡å½¢çŠ¶: {instance.vectors.shape}")
        
        return instance

    def query(self, query: str, top_k: int = 3) -> str:
        """
        æŸ¥è¯¢ç´¢å¼•
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
        
        Returns:
            æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        if self.vectors is None:
            raise RuntimeError("ç´¢å¼•æœªæ„å»ºæˆ–åŠ è½½,æ— æ³•æŸ¥è¯¢")
        
        if not query:
            raise ValueError("æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.model.encode(
            [query],
            convert_to_numpy=True,
            show_progress_bar=False,
            normalize_embeddings=False
        )[0].astype(np.float32)
        
        # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
        query_norm = query_vector / np.linalg.norm(query_vector)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(self.normalized_vectors, query_norm)
        
        # è·å–top_kç»“æœ
        top_k_indices = np.argsort(similarities)[-top_k:][::-1]

        retrieved_chunks: List[Dict[str, Any]] = []
        for idx in top_k_indices:
            int_idx = int(idx)
            if 0 <= int_idx < len(self.chunks):
                retrieved_chunks.append(self.chunks[int_idx])
            else:
                print(f"âš ï¸ æŸ¥è¯¢ç»“æœç´¢å¼•è¶Šç•Œ: {int_idx} (chunks={len(self.chunks)})ï¼Œå·²å¿½ç•¥")

        if not retrieved_chunks:
            return "[æŸ¥è¯¢é”™è¯¯] æœªæ£€ç´¢åˆ°æœ‰æ•ˆçš„æ–‡æœ¬å—"

        formatted_chunks = [self._format_chunk_for_output(chunk) for chunk in retrieved_chunks]
        context = "\n---\n".join(formatted_chunks)

        return f"### Retrieved Context:\n{context}"



class RAGIndexLocal_faiss(BaseRAGIndex):
    """
    ä½¿ç”¨FaissåŠ é€Ÿçš„æœ¬åœ°RAGç´¢å¼•
    é€‚åˆè¶…å¤§è§„æ¨¡çŸ¥è¯†åº“(ç™¾ä¸‡çº§ä»¥ä¸Š)
    """
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 device: str = "cuda", max_seq_length: int = 512,
                 use_gpu_index: bool = False,
                 gpu_parallel_degree: Optional[int] = None,
                 embedding_devices: Optional[Sequence[str]] = None):
        super().__init__(
            model_name=model_name,
            device=device,
            max_seq_length=max_seq_length,
            embedding_devices=embedding_devices,
        )
        
        if not _FAISS_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£… faiss: pip install faiss-cpu æˆ– faiss-gpu")
        
        self.faiss_index = None
        self.use_gpu_index = use_gpu_index
        self.index_nlist: Optional[int] = None
        self.index_nprobe: Optional[int] = None
        self._faiss_on_gpu = False
        self._gpu_resources: Optional[List["faiss.StandardGpuResources"]] = None
        self._gpu_device_ids: Optional[List[int]] = None
        self._effective_gpu_parallel_degree: int = 0
        self.gpu_parallel_degree = 1
        if gpu_parallel_degree is not None:
            try:
                self.gpu_parallel_degree = max(1, int(gpu_parallel_degree))
            except (TypeError, ValueError):
                print(f"âš ï¸ æ— æ•ˆçš„gpu_parallel_degree({gpu_parallel_degree}), å·²å›é€€ä¸º1")
        if self.use_gpu_index and self.gpu_parallel_degree > 1:
            print(f"âœ“ Faissç´¢å¼•GPUå¹¶è¡Œåº¦: {self.gpu_parallel_degree}")
        self.vectors: Optional[np.ndarray] = None
        print(f"âœ“ Faissç´¢å¼•æ¨¡å¼: {'GPU' if use_gpu_index else 'CPU'}")

    def _store_embeddings(self, vectors: np.ndarray) -> None:
        prepared = np.ascontiguousarray(vectors.astype(np.float32))
        faiss.normalize_L2(prepared)
        self.vectors = prepared

    def _finalize_embeddings(self) -> None:
        print("âœ“ å‘é‡å·²å½’ä¸€åŒ–å¹¶ç¼“å­˜ï¼Œä¸‹ä¸€æ­¥å°†æ„å»ºFaissç´¢å¼•")

    def _create_cpu_ivf_index(self, vector_dim: int, total_vectors: int) -> "faiss.Index":
        total_vectors = max(1, total_vectors)
        nlist, nprobe = _suggest_ivf_params(total_vectors)
        print(f"æ­£åœ¨åˆ›å»ºFaiss IndexIVFFlat (ç»´åº¦={vector_dim}, nlist={nlist}, nprobe={nprobe})")
        quantizer = faiss.IndexFlatIP(vector_dim)
        index = faiss.IndexIVFFlat(quantizer, vector_dim, nlist, faiss.METRIC_INNER_PRODUCT)
        index.nprobe = nprobe
        self.index_nlist = nlist
        self.index_nprobe = nprobe
        return index

    def _prepare_gpu_resources(self) -> tuple[List["faiss.StandardGpuResources"], List[int]]:
        if not hasattr(faiss, "get_num_gpus"):
            raise RuntimeError("å½“å‰Faissåº“æœªç¼–è¯‘GPUæ”¯æŒ")
        available_gpus = faiss.get_num_gpus()
        if available_gpus <= 0:
            raise RuntimeError("æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPUè®¾å¤‡")
        requested = max(1, self.gpu_parallel_degree or 1)
        parallel = min(requested, available_gpus)
        if parallel < requested:
            print(f"âš ï¸ è¯·æ±‚çš„GPUå¹¶è¡Œåº¦ {requested} è¶…å‡ºå¯ç”¨æ•°é‡ {available_gpus}, å®é™…ä½¿ç”¨ {parallel}")
        if self._gpu_resources is None or len(self._gpu_resources) != parallel:
            self._gpu_resources = [faiss.StandardGpuResources() for _ in range(parallel)]
        device_ids = list(range(parallel))
        self._effective_gpu_parallel_degree = parallel
        return self._gpu_resources, device_ids

    def _ensure_faiss_index(self, vector_dim: int, total_vectors: int) -> None:
        if self.faiss_index is None:
            self.faiss_index = self._create_cpu_ivf_index(vector_dim, total_vectors)
            self._faiss_on_gpu = False

    def _train_index_if_needed(self, vectors: np.ndarray) -> None:
        if hasattr(self.faiss_index, "is_trained") and not self.faiss_index.is_trained:
            print("æ­£åœ¨è®­ç»ƒFaissç´¢å¼•...")
            self.faiss_index.train(vectors)

    def _ensure_nprobe(self) -> None:
        if self.faiss_index is None:
            return
        if self.index_nprobe is None:
            source_nlist = None
            if hasattr(self.faiss_index, "nlist"):
                source_nlist = getattr(self.faiss_index, "nlist", None)
            if source_nlist is None and self.index_nlist is not None:
                source_nlist = self.index_nlist
            if source_nlist is not None:
                self.index_nprobe = min(16, max(1, int(source_nlist)))
            else:
                self.index_nprobe = 1
        target_nprobe = int(self.index_nprobe)
        if hasattr(self.faiss_index, "nlist"):
            target_nprobe = min(target_nprobe, getattr(self.faiss_index, "nlist", target_nprobe))
        if hasattr(self.faiss_index, "nprobe"):
            self.faiss_index.nprobe = target_nprobe
        elif hasattr(faiss, "ParameterSpace"):
            try:
                faiss.ParameterSpace().set_index_parameter(self.faiss_index, "nprobe", target_nprobe)
            except Exception as exc:
                print(f"âš ï¸ æ— æ³•åœ¨å½“å‰ç´¢å¼•ä¸Šè®¾ç½®nprobeå‚æ•°({exc})")

    def _finalize_faiss_index(self) -> None:
        if self.faiss_index is None:
            return
        self._ensure_nprobe()
        if self.use_gpu_index and not self._faiss_on_gpu:
            try:
                resources, device_ids = self._prepare_gpu_resources()
                active_device_ids = device_ids
                if len(device_ids) == 1:
                    self.faiss_index = faiss.index_cpu_to_gpu(resources[0], device_ids[0], self.faiss_index)
                else:
                    if hasattr(faiss, "index_cpu_to_gpu_multiple_py"):
                        cloner_opts = faiss.GpuMultipleClonerOptions() if hasattr(faiss, "GpuMultipleClonerOptions") else None
                        self.faiss_index = faiss.index_cpu_to_gpu_multiple_py(resources, self.faiss_index, cloner_opts)
                    else:
                        print("âš ï¸ å½“å‰Faissç‰ˆæœ¬ä¸æ”¯æŒå¤šGPUç´¢å¼•å…‹éš†, å°†é€€å›å•GPUæ¨¡å¼")
                        self.faiss_index = faiss.index_cpu_to_gpu(resources[0], device_ids[0], self.faiss_index)
                        active_device_ids = [device_ids[0]]
                self._faiss_on_gpu = True
                self._gpu_device_ids = active_device_ids
                self._effective_gpu_parallel_degree = len(active_device_ids)
                if hasattr(self.faiss_index, "nlist"):
                    self.index_nlist = self.faiss_index.nlist
                self._ensure_nprobe()
                if len(active_device_ids) > 1:
                    print(f"âœ“ ç´¢å¼•å·²è¿ç§»åˆ°GPU, å¹¶è¡Œåº¦: {len(active_device_ids)}")
                else:
                    print("âœ“ ç´¢å¼•å·²è¿ç§»åˆ°GPU")
            except Exception as e:
                print(f"âš ï¸ ç´¢å¼•è¿ç§»åˆ°GPUå¤±è´¥({e}),ç»§ç»­ä½¿ç”¨CPUç´¢å¼•")
                self._faiss_on_gpu = False
                self._gpu_device_ids = None
                self._effective_gpu_parallel_degree = 0
                if hasattr(self.faiss_index, "nlist"):
                    self.index_nlist = self.faiss_index.nlist
                self._ensure_nprobe()
        elif not self.use_gpu_index:
            self._faiss_on_gpu = False
            self._ensure_nprobe()

    def _cpu_index_for_persistence(self) -> Optional["faiss.Index"]:
        if self.faiss_index is None:
            return None
        if self._faiss_on_gpu:
            return faiss.index_gpu_to_cpu(self.faiss_index)
        return self.faiss_index

    def build_index_streaming(self, file_path: str, index_path: str,
                             batch_size: int = 64, chunk_size: int = 10000,
                             max_chunks: Optional[int] = None, num_workers: int = 1,
                             resume: bool = True):
        """
        æµå¼æ„å»ºFaissç´¢å¼•ï¼Œæ”¯æŒè¶…å¤§è§„æ¨¡çŸ¥è¯†åº“
        
        Args:
            file_path: çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„ (ä»…æ”¯æŒjsonlæ ¼å¼)
            index_path: ç´¢å¼•ä¿å­˜ç›®å½•
            batch_size: embeddingç”Ÿæˆçš„æ‰¹é‡å¤§å°
            chunk_size: æ¯æ¬¡å¤„ç†çš„æ–‡æœ¬å—æ•°é‡ï¼ˆæ§åˆ¶å†…å­˜ä½¿ç”¨ï¼‰
            max_chunks: æœ€å¤§å¤„ç†çš„æ–‡æœ¬å—æ•°é‡
            num_workers: å¹¶è¡Œå¤„ç†çš„workeræ•°é‡
            resume: æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­
        """
        if not file_path.endswith('.jsonl'):
            raise ValueError("æµå¼æ„å»ºä»…æ”¯æŒJSONLæ ¼å¼æ–‡ä»¶")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è‡ªåŠ¨ç¡®å®šworkeræ•°é‡
        num_workers = self._resolve_num_workers(num_workers)
        
        os.makedirs(index_path, exist_ok=True)
        
        # æ£€æŸ¥checkpoint
        checkpoint_file = os.path.join(index_path, "checkpoint.json")
        start_line = 0
        processed_chunks = 0
        
        if resume and os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
                start_line = checkpoint.get("processed_lines", 0)
                processed_chunks = checkpoint.get("processed_chunks", 0)
            print(f"ğŸ’¾ ä»checkpointæ¢å¤: å·²å¤„ç† {processed_chunks:,} ä¸ªæ–‡æœ¬å—")
            
            # åŠ è½½å·²æœ‰çš„chunkså’ŒFaissç´¢å¼•
            self._load_partial_index(index_path)
        
        if self.faiss_index is None:
            print("å°†åœ¨é¦–æ¬¡æ‰¹å¤„ç†æ—¶è‡ªåŠ¨åˆå§‹åŒ–Faiss IndexIVFFlat")
        
        print(f"\n{'='*60}")
        print(f"ğŸŒŠ Faissæµå¼ç´¢å¼•æ„å»ºæ¨¡å¼")
        print(f"{'='*60}")
        print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
        print(f"ç´¢å¼•è·¯å¾„: {index_path}")
        print(f"æ¯æ‰¹å¤„ç†: {chunk_size:,} ä¸ªæ–‡æœ¬å—")
        print(f"Batchå¤§å°: {batch_size}")
        print(f"Workeræ•°: {num_workers}")
        print(f"{'='*60}\n")
        
        total_start_time = time.time()
        batch_chunks = []
        batch_texts = []
        current_line = 0
        
        # æ‰“å¼€æ–‡ä»¶å¹¶é€è¡Œè¯»å–
        with open(file_path, 'r', encoding='utf-8') as f:
            # è·³è¿‡å·²å¤„ç†çš„è¡Œ
            if start_line > 0:
                print(f"â­ï¸  è·³è¿‡å·²å¤„ç†çš„ {start_line:,} è¡Œ...")
                for _ in range(start_line):
                    next(f)
                current_line = start_line
            
            pbar = tqdm(desc="å¤„ç†æ–‡æœ¬å—", unit="å—", initial=processed_chunks)
            
            for line in f:
                current_line += 1
                
                if not line.strip():
                    continue
                
                try:
                    sample = json.loads(line)
                    
                    if not (isinstance(sample, dict) and "text" in sample and 
                           isinstance(sample["text"], str) and sample["text"]):
                        continue
                    
                    title = sample.get("title", "")
                    text = sample.get("text", "")
                    combined_text = f"{title}\n\n{text}" if title else text
                    
                    batch_chunks.append(sample)
                    batch_texts.append(combined_text)
                    
                    if len(batch_chunks) >= chunk_size or \
                       (max_chunks and processed_chunks + len(batch_chunks) >= max_chunks):
                        
                        # å¤„ç†å¹¶æ·»åŠ åˆ°Faissç´¢å¼•
                        self._process_and_add_to_faiss(
                            batch_chunks, batch_texts,
                            index_path, batch_size, num_workers
                        )
                        
                        processed_chunks += len(batch_chunks)
                        pbar.update(len(batch_chunks))
                        
                        # ä¿å­˜checkpoint
                        self._save_checkpoint(checkpoint_file, current_line, processed_chunks)
                        
                        batch_chunks = []
                        batch_texts = []
                        gc.collect()
                        
                        if max_chunks and processed_chunks >= max_chunks:
                            print(f"\nâš ï¸  å·²è¾¾åˆ°æœ€å¤§æ–‡æœ¬å—æ•°é™åˆ¶: {max_chunks:,}")
                            break
                
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  è·³è¿‡æ— æ•ˆJSON (è¡Œ{current_line}): {e}")
                    continue
            
            # å¤„ç†å‰©ä½™æ‰¹æ¬¡
            if batch_chunks:
                self._process_and_add_to_faiss(
                    batch_chunks, batch_texts,
                    index_path, batch_size, num_workers
                )
                processed_chunks += len(batch_chunks)
                pbar.update(len(batch_chunks))
                self._save_checkpoint(checkpoint_file, current_line, processed_chunks)
            
            pbar.close()
        
        # æœ€ç»ˆä¿å­˜
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆç´¢å¼•")
        print(f"{'='*60}")
        self._finalize_faiss_index()
        self.save_index(index_path)
        
        # åˆ é™¤checkpoint
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
        
        total_time = time.time() - total_start_time
        print(f"\n{'='*60}")
        print(f"âœ… Faissæµå¼ç´¢å¼•æ„å»ºå®Œæˆ!")
        print(f"{'='*60}")
        print(f"æ€»æ–‡æœ¬å—æ•°: {processed_chunks:,}")
        print(f"Faissç´¢å¼•åŒ…å«: {self.faiss_index.ntotal:,} ä¸ªå‘é‡")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"å¹³å‡é€Ÿåº¦: {processed_chunks/total_time:.1f} å—/ç§’")
        print(f"ç´¢å¼•è·¯å¾„: {index_path}")
        print(f"{'='*60}\n")
    
    def _process_and_add_to_faiss(self, chunks: List[Dict], texts: List[str],
                                  index_path: str, batch_size: int, num_workers: int):
        """å¤„ç†æ‰¹æ¬¡å¹¶ç›´æ¥æ·»åŠ åˆ°Faissç´¢å¼•"""
        # ç”Ÿæˆembeddings
        vectors = self._get_embeddings_batch(
            texts,
            batch_size=batch_size,
            show_progress=False,
            num_workers=num_workers
        )
        
        # å½’ä¸€åŒ–å‘é‡
        vectors = np.ascontiguousarray(vectors.astype(np.float32))
        faiss.normalize_L2(vectors)
        
        total_vectors = len(self.chunks) + vectors.shape[0]
        self._ensure_faiss_index(vectors.shape[1], total_vectors)
        self._train_index_if_needed(vectors)
        self.faiss_index.add(vectors)
        self._ensure_nprobe()
        
        # ä¿å­˜chunksï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        self.chunks.extend(chunks)
        
        # å°†vectorsæ·»åŠ åˆ°å†…å­˜ä¸­ï¼ˆç”¨äºæœ€åä¿å­˜ï¼‰
        if self.vectors is None:
            self.vectors = vectors
        else:
            self.vectors = np.vstack([self.vectors, vectors])
        
        # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
        if len(self.chunks) % 50000 < len(chunks):  # æ¯5ä¸‡æ¡ä¿å­˜ä¸€æ¬¡
            print(f"  ğŸ’¾ ä¿å­˜ä¸­é—´ç»“æœ ({len(self.chunks):,} ä¸ªæ–‡æœ¬å—)...")
            self._save_partial_index(index_path)
    
    def _save_partial_index(self, index_path: str):
        """ä¿å­˜éƒ¨åˆ†ç´¢å¼•ï¼ˆä¸­é—´checkpointï¼‰"""
        partial_dir = os.path.join(index_path, "partial")
        os.makedirs(partial_dir, exist_ok=True)
        
        # ä¿å­˜chunks
        chunks_file = os.path.join(partial_dir, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False)
        
        # ä¿å­˜Faissç´¢å¼•
        faiss_file = os.path.join(partial_dir, "faiss.index")
        index_to_save = self._cpu_index_for_persistence()
        if index_to_save is not None:
            faiss.write_index(index_to_save, faiss_file)
        
        # ä¿å­˜numpy vectors
        if self.vectors is not None:
            vectors_file = os.path.join(partial_dir, "vectors.npy")
            np.save(vectors_file, self.vectors)
    
    def _load_partial_index(self, index_path: str):
        """åŠ è½½éƒ¨åˆ†ç´¢å¼•ï¼ˆç”¨äºresumeï¼‰"""
        partial_dir = os.path.join(index_path, "partial")
        
        if not os.path.exists(partial_dir):
            return
        
        # åŠ è½½chunks
        chunks_file = os.path.join(partial_dir, "chunks.json")
        if os.path.exists(chunks_file):
            with open(chunks_file, 'r', encoding='utf-8') as f:
                self.chunks = json.load(f)
            print(f"âœ“ åŠ è½½å·²æœ‰chunks: {len(self.chunks):,} ä¸ª")
        
        # åŠ è½½Faissç´¢å¼•
        faiss_file = os.path.join(partial_dir, "faiss.index")
        if os.path.exists(faiss_file):
            self.faiss_index = faiss.read_index(faiss_file)
            if hasattr(self.faiss_index, "nlist"):
                self.index_nlist = self.faiss_index.nlist
                self.index_nprobe = min(16, max(1, self.faiss_index.nlist))
                self._ensure_nprobe()
            self._faiss_on_gpu = False
            print(f"âœ“ åŠ è½½å·²æœ‰Faissç´¢å¼•: {self.faiss_index.ntotal:,} ä¸ªå‘é‡")
        
        # åŠ è½½vectors
        vectors_file = os.path.join(partial_dir, "vectors.npy")
        if os.path.exists(vectors_file):
            self.vectors = np.load(vectors_file)
            print(f"âœ“ åŠ è½½å·²æœ‰vectors: {self.vectors.shape}")

    def build_index(self, file_path: str, batch_size: int = 64,
                   max_chunks: Optional[int] = None, num_workers: int = 1):
        """
        æ„å»ºFaissç´¢å¼•ï¼ˆå¸¸è§„æ¨¡å¼ï¼‰
        
        Args:
            file_path: çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„
            batch_size: embeddingç”Ÿæˆæ‰¹é‡å¤§å°
            max_chunks: æœ€å¤§å¤„ç†çš„æ–‡æœ¬å—æ•°é‡
            num_workers: å¹¶è¡Œå¤„ç†çš„workeræ•°é‡
        
        æç¤º: å¯¹äºè¶…å¤§è§„æ¨¡çŸ¥è¯†åº“ï¼Œå»ºè®®ä½¿ç”¨ build_index_streaming() æ–¹æ³•
        """
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•ç”Ÿæˆembeddings
        super().build_index(file_path, batch_size, max_chunks, num_workers)
        
        # æ„å»ºFaissç´¢å¼•
        print(f"\n{'='*60}")
        print(f"ğŸš€ æ­¥éª¤ 5/5: æ„å»ºFaissç´¢å¼•")
        print(f"{'='*60}")
        
        try:
            start_time = time.time()
            if self.vectors is None:
                raise RuntimeError("å½’ä¸€åŒ–å‘é‡ç¼ºå¤±ï¼Œæ— æ³•æ„å»ºFaissç´¢å¼•")
            vectors = np.ascontiguousarray(self.vectors.astype(np.float32))
            d = vectors.shape[1]
            total_vectors = vectors.shape[0]
            
            print(f"å‘é‡ç»´åº¦: {d}")
            print(f"å‘é‡æ•°é‡: {len(vectors):,}")
            
            # æ„å»ºFaiss IndexIVFFlat
            self.faiss_index = None
            self._faiss_on_gpu = False
            self._ensure_faiss_index(d, total_vectors)
            self._train_index_if_needed(vectors)
            
            print("æ­£åœ¨æ·»åŠ å‘é‡åˆ°Faissç´¢å¼•...")
            self.faiss_index.add(vectors)
            self._ensure_nprobe()
            self._finalize_faiss_index()
            
            elapsed = time.time() - start_time
            print(f"âœ“ Faissç´¢å¼•æ„å»ºå®Œæˆ!")
            print(f"  - åŒ…å«å‘é‡æ•°: {self.faiss_index.ntotal:,}")
            if self.index_nlist is not None:
                print(f"  - nlist: {self.index_nlist}")
            if self.index_nprobe is not None:
                print(f"  - nprobe: {self.index_nprobe}")
            print(f"  - æ„å»ºè€—æ—¶: {elapsed:.2f}ç§’")
            
            # æ¸…ç†å†…å­˜
            del vectors
            gc.collect()
            
        except Exception as e:
            print(f"âœ— æ„å»ºFaissç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
            raise

    def save_index(self, index_path: str):
        """
        ä¿å­˜Faissç´¢å¼•
        
        Args:
            index_path: ä¿å­˜ç´¢å¼•çš„ç›®å½•è·¯å¾„
        """
        if self.faiss_index is None or not self.chunks:
            raise ValueError("ç´¢å¼•ä¸ºç©º,æ— æ³•ä¿å­˜")

        os.makedirs(index_path, exist_ok=True)
        
        # ä¿å­˜æ–‡æœ¬å—
        chunks_file = os.path.join(index_path, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜Faissç´¢å¼•
        index_file = os.path.join(index_path, "faiss.index")
        self._ensure_nprobe()
        index_to_save = self._cpu_index_for_persistence()
        if index_to_save is None:
            raise RuntimeError("Faissç´¢å¼•æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")
        faiss.write_index(index_to_save, index_file)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "model_name": self.model_name,
            "device": self.device,
            "max_seq_length": self.max_seq_length,
            "num_chunks": len(self.chunks),
            "vector_dim": self.faiss_index.d if hasattr(self.faiss_index, 'd') else None,
            "use_gpu_index": self.use_gpu_index,
            "index_type": "faiss.IndexIVFFlat",
            "nlist": self.index_nlist,
            "nprobe": self.index_nprobe,
            "embedding_devices": self.embedding_devices,
        }
        metadata_file = os.path.join(index_path, "metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ Faissç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")

    @classmethod
    def load_index(cls, index_path: str, model_name: Optional[str] = None,
                   device: str = "cuda", max_seq_length: int = 512,
                   use_gpu_index: bool = False, memory_map: bool = True,
                   read_only_index: bool = True,
                   embedding_devices: Optional[Sequence[str]] = None, **kwargs):
        """
        åŠ è½½Faissç´¢å¼•
        
        Args:
            index_path: ç´¢å¼•ç›®å½•è·¯å¾„
            model_name: æ¨¡å‹åç§°
            device: è¿è¡Œè®¾å¤‡
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
            use_gpu_index: æ˜¯å¦ä½¿ç”¨GPUç´¢å¼•
            memory_map: æ˜¯å¦å¼€å¯ç£ç›˜å†…å­˜æ˜ å°„ä»¥é™ä½å¸¸é©»å†…å­˜
            read_only_index: æ˜¯å¦ä»¥åªè¯»æ–¹å¼æ‰“å¼€ç´¢å¼•ï¼ˆé…åˆå†…å­˜æ˜ å°„ï¼‰
        
        Returns:
            åŠ è½½çš„RAGIndexLocal_faisså®ä¾‹
        """
        chunks_file = os.path.join(index_path, "chunks.json")
        index_file = os.path.join(index_path, "faiss.index")
        metadata_file = os.path.join(index_path, "metadata.json")

        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        if not os.path.exists(chunks_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°chunksæ–‡ä»¶: {chunks_file}")
        if not os.path.exists(index_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°Faissç´¢å¼•æ–‡ä»¶: {index_file}")
        
        # è¯»å–å…ƒæ•°æ®
        metadata = None
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            if model_name is None:
                model_name = metadata.get("model_name", "sentence-transformers/all-MiniLM-L6-v2")
            if embedding_devices is None:
                embedding_devices = metadata.get("embedding_devices")
            print(f"âœ“ ä»å…ƒæ•°æ®è¯»å–é…ç½®: {metadata}")
        else:
            if model_name is None:
                model_name = "sentence-transformers/all-MiniLM-L6-v2"
            print("âš ï¸ æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶,ä½¿ç”¨é»˜è®¤é…ç½®")

        kw_embedding = kwargs.pop("embedding_devices", None)
        if embedding_devices is None and kw_embedding is not None:
            embedding_devices = kw_embedding

        # åˆ›å»ºå®ä¾‹
        instance = cls(
            model_name=model_name,
            device=device,
            max_seq_length=max_seq_length,
            use_gpu_index=use_gpu_index,
             embedding_devices=embedding_devices,
            **kwargs
        )
        
        # åŠ è½½æ–‡æœ¬å—
        chunks_jsonl = os.path.join(index_path, "chunks.jsonl")
        chunks_offsets = os.path.join(index_path, "chunks.offsets")

        if os.path.exists(chunks_jsonl) and os.path.exists(chunks_offsets):
             print(f"âœ“ æ£€æµ‹åˆ°ä¼˜åŒ–å­˜å‚¨: ä½¿ç”¨ DiskBasedChunks (æŒ‰éœ€è¯»å– {chunks_jsonl})")
             instance.chunks = DiskBasedChunks(chunks_jsonl, chunks_offsets)
        elif os.path.exists(chunks_file):
             with open(chunks_file, 'r', encoding='utf-8') as f:
                instance.chunks = json.load(f)
        
        # åŠ è½½Faissç´¢å¼•
        io_flags = 0
        mmap_supported = hasattr(faiss, "IO_FLAG_MMAP")
        readonly_supported = hasattr(faiss, "IO_FLAG_READ_ONLY")
        attempted_mmap = False
        if memory_map and mmap_supported:
            io_flags |= faiss.IO_FLAG_MMAP
            attempted_mmap = True
        if read_only_index and readonly_supported:
            io_flags |= faiss.IO_FLAG_READ_ONLY
        load_errors = []
        cpu_index = None
        if io_flags:
            try:
                cpu_index = faiss.read_index(index_file, io_flags)
                if attempted_mmap:
                    print("âœ“ ä»¥å†…å­˜æ˜ å°„æ–¹å¼åŠ è½½Faissç´¢å¼•ï¼Œå‡å°‘å¸¸é©»å†…å­˜å ç”¨")
            except Exception as exc:  # pylint: disable=broad-except
                load_errors.append(exc)
                print(f"âš ï¸ å†…å­˜æ˜ å°„åŠ è½½å¤±è´¥({exc})ï¼Œå°†å›é€€åˆ°å¸¸è§„åŠ è½½")
        if cpu_index is None:
            try:
                cpu_index = faiss.read_index(index_file)
            except Exception as exc:
                load_errors.append(exc)
                raise RuntimeError(
                    f"æ— æ³•åŠ è½½Faissç´¢å¼•: {index_file}ï¼Œé”™è¯¯: {load_errors}"
                ) from exc

        instance.faiss_index = cpu_index
        instance._faiss_on_gpu = False
        if hasattr(cpu_index, "nlist"):
            instance.index_nlist = cpu_index.nlist
        metadata_nprobe = metadata.get("nprobe") if metadata else None
        if metadata_nprobe is not None:
            instance.index_nprobe = int(metadata_nprobe)
        elif instance.index_nlist is not None:
            instance.index_nprobe = min(16, max(1, instance.index_nlist))
        else:
            instance.index_nprobe = 1
        instance._ensure_nprobe()
        
        instance._finalize_faiss_index()

        print(f"âœ“ æˆåŠŸåŠ è½½Faissç´¢å¼•: {len(instance.chunks)} ä¸ªæ–‡æœ¬å—")
        
        return instance

    def query(self, query: str, top_k: int = 3) -> str:
        """
        ä½¿ç”¨FaissæŸ¥è¯¢ç´¢å¼•
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
        
        Returns:
            æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        if self.faiss_index is None:
            raise RuntimeError("Faissç´¢å¼•æœªæ„å»ºæˆ–åŠ è½½,æ— æ³•æŸ¥è¯¢")
        
        if not query:
            raise ValueError("æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º")


        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.model.encode(
                [query],
                convert_to_numpy=True,
                show_progress_bar=False,
                normalize_embeddings=False
            )[0].astype(np.float32)
            
            # å½’ä¸€åŒ–
            query_vector = query_vector.reshape(1, -1)
            faiss.normalize_L2(query_vector)
            
            self._ensure_nprobe()
            
            # æœç´¢
            distances, indices = self.faiss_index.search(query_vector, top_k)
            indices = indices[0]
            
            # è·å–ç»“æœï¼Œç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
            retrieved_chunks: List[Dict[str, Any]] = []
            for raw_idx in indices:
                idx = int(raw_idx)
                if idx < 0:
                    continue
                if idx >= len(self.chunks):
                    print(f"âš ï¸ æŸ¥è¯¢ç»“æœç´¢å¼•è¶Šç•Œ: {idx} (chunks={len(self.chunks)})ï¼Œå·²å¿½ç•¥")
                    continue
                retrieved_chunks.append(self.chunks[idx])

            if not retrieved_chunks:
                return "[æŸ¥è¯¢é”™è¯¯] æœªæ£€ç´¢åˆ°æœ‰æ•ˆçš„æ–‡æœ¬å—"

            formatted_chunks = [self._format_chunk_for_output(chunk) for chunk in retrieved_chunks]
            context = "\n---\n".join(formatted_chunks)
            
            return f"### Retrieved Context:\n{context}"
        except Exception as e:
            return f"[æŸ¥è¯¢é”™è¯¯] {str(e)}"


class RAGIndexLocal_faiss_compact(RAGIndexLocal_faiss):
    """
    åŸºäºIVFPQå‹ç¼©çš„Faissç´¢å¼•å®ç°, ç›®æ ‡æ˜¯åœ¨å¤§è§„æ¨¡çŸ¥è¯†åº“åœºæ™¯ä¸‹æ˜¾è‘—é™ä½ç´¢å¼•ä½“ç§¯ã€‚
    
    ç‰¹æ€§:
        - ä½¿ç”¨ IVF + PQ (Product Quantization) ç»„åˆ, é»˜è®¤çº¦ 24 å­—èŠ‚/å‘é‡
        - è®­ç»ƒé˜¶æ®µè‡ªåŠ¨ç¼“å†²æ•°æ®, ç¡®ä¿PQè®­ç»ƒç¨³å®š, å¹¶æ”¯æŒå°æ ·æœ¬è‡ªåŠ¨å›é€€
        - å¯é€‰ FP16 å†…å­˜ç¼“å­˜å‡å°‘æ„å»ºæœŸå†…å­˜æ¶ˆè€—
        - å…¼å®¹åŸæœ‰çš„ build_index / build_index_streaming æ¥å£å’Œ load/save åè®®
    """

    def __init__(self,
                 model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 device: str = "cuda",
                 max_seq_length: int = 512,
                 use_gpu_index: bool = False,
                 gpu_parallel_degree: Optional[int] = None,
                 embedding_devices: Optional[Sequence[str]] = None,
                 pq_m: Optional[int] = None,
                 pq_bits: int = 8,
                 target_bytes_per_vector: Optional[int] = 24,
                 training_samples: int = 262_144,
                 store_embeddings_fp16: bool = True) -> None:
        """
        Args:
            pq_m: Product Quantization åˆ†å—æ•°, None æ—¶æ ¹æ® target_bytes_per_vector è‡ªåŠ¨æ¨å¯¼
            pq_bits: æ¯ä¸ªå­å‘é‡çš„ç å­—ä½æ•° (é»˜è®¤8, æ”¯æŒ4/5/6/7/8)
            target_bytes_per_vector: å¸Œæœ›çš„å‹ç¼©åå­—èŠ‚æ•°, å°†è¿‘ä¼¼æ˜ å°„ä¸º pq_m
            training_samples: ç”¨äºIVF+PQè®­ç»ƒçš„æœ€å¤§æ ·æœ¬é‡
            store_embeddings_fp16: æ„å»ºæµç¨‹ä¸­æ˜¯å¦ä»¥FP16ç¼“å­˜å‘é‡ä»¥é™ä½å†…å­˜
        """
        super().__init__(
            model_name=model_name,
            device=device,
            max_seq_length=max_seq_length,
            use_gpu_index=use_gpu_index,
            gpu_parallel_degree=gpu_parallel_degree,
            embedding_devices=embedding_devices,
        )
        self.pq_m = int(pq_m) if pq_m is not None else None
        self.pq_bits = max(4, min(8, int(pq_bits)))
        self.target_bytes_per_vector = target_bytes_per_vector
        self.training_samples = max(2048, int(training_samples))
        self.store_embeddings_fp16 = store_embeddings_fp16

        self._effective_pq_m: Optional[int] = self.pq_m
        self._min_training_vectors: Optional[int] = None
        self._pending_training_vectors: List[np.ndarray] = []
        self._pending_training_chunks: List[Dict[str, Any]] = []
        self._pending_training_count: int = 0
        self._fallback_to_flat: bool = False

    @staticmethod
    def _divisors(value: int) -> List[int]:
        divs: set[int] = set()
        upper = int(math.sqrt(value)) + 1
        for factor in range(1, upper):
            if value % factor == 0:
                divs.add(factor)
                divs.add(value // factor)
        return sorted(divs)

    def _resolve_pq_m(self, vector_dim: int) -> int:
        if self.pq_m is not None:
            if vector_dim % self.pq_m != 0:
                raise ValueError(
                    f"pq_m={self.pq_m} ä¸å‘é‡ç»´åº¦ {vector_dim} ä¸æ•´é™¤, è¯·è°ƒæ•´ pq_m æˆ–ä½¿ç”¨ target_bytes_per_vector è‡ªåŠ¨æ¨å¯¼"
                )
            return self.pq_m

        divisors = [d for d in self._divisors(vector_dim) if d > 1]
        if not divisors:
            return 1
        target = max(8, int(self.target_bytes_per_vector)) if self.target_bytes_per_vector else 8
        candidate = min(divisors, key=lambda d: (abs(d - target), d))
        candidate = max(8, candidate)

        # ç¡®ä¿èƒ½è¢«ç»´åº¦æ•´é™¤
        while vector_dim % candidate != 0 and candidate < vector_dim:
            candidate += 1
        if vector_dim % candidate != 0:
            # å…œåº•é€‰æ‹©æœ€å¤§çš„å¯è¡Œå› å­
            candidate = max(d for d in divisors if vector_dim % d == 0)
        return max(4, min(candidate, vector_dim))

    def _compute_min_training(self, vector_dim: int) -> int:
        if self._effective_pq_m is None:
            self._effective_pq_m = self._resolve_pq_m(vector_dim)
        sub_vector_train = self._effective_pq_m * (1 << self.pq_bits)
        coarse_train = max(1024, (self.index_nlist or 1) * 16)
        return max(2048, sub_vector_train, coarse_train)

    def _create_cpu_ivf_index(self, vector_dim: int, total_vectors: int) -> "faiss.Index":
        pq_m = self._resolve_pq_m(vector_dim)
        nlist, nprobe = _suggest_ivf_params(max(total_vectors, pq_m * 64))
        quantizer = faiss.IndexFlatIP(vector_dim)
        try:
            index = faiss.IndexIVFPQ(quantizer, vector_dim, nlist, pq_m, self.pq_bits)
            print(
                f"âœ“ ä½¿ç”¨IVFPQç´¢å¼• (ç»´åº¦={vector_dim}, nlist={nlist}, m={pq_m}, bits={self.pq_bits}) "
                f"â‰ˆ {pq_m} å­—èŠ‚/å‘é‡"
            )
        except Exception as exc:
            raise RuntimeError(f"åˆå§‹åŒ–IVFPQç´¢å¼•å¤±è´¥: {exc}") from exc

        index.nprobe = min(nprobe, nlist)
        self.index_nlist = nlist
        self.index_nprobe = index.nprobe
        self._effective_pq_m = pq_m
        self._min_training_vectors = self._compute_min_training(vector_dim)
        return index

    def _store_embeddings(self, vectors: np.ndarray) -> None:
        super()._store_embeddings(vectors)
        if self.store_embeddings_fp16 and self.vectors is not None:
            self.vectors = np.ascontiguousarray(self.vectors.astype(np.float16))
            print("âœ“ å‘é‡ç¼“å­˜å·²è½¬æ¢ä¸ºFP16ä»¥é™ä½å†…å­˜å ç”¨")

    def _append_training_buffer(self, vectors: np.ndarray, chunks: List[Dict[str, Any]]) -> None:
        self._pending_training_vectors.append(vectors)
        self._pending_training_chunks.extend(chunks)
        self._pending_training_count += vectors.shape[0]

    def _clear_training_buffer(self) -> None:
        self._pending_training_vectors = []
        self._pending_training_chunks = []
        self._pending_training_count = 0

    def _attempt_training(self, force: bool = False) -> None:
        if self.faiss_index is None:
            return
        if not hasattr(self.faiss_index, "is_trained"):
            return

        if self.faiss_index.is_trained:
            if self._pending_training_vectors:
                pending = np.vstack(self._pending_training_vectors)
                self.faiss_index.add(pending)
                self._ensure_nprobe()
                self.chunks.extend(self._pending_training_chunks)
                self._clear_training_buffer()
            return

        if not self._pending_training_vectors:
            return

        sample = np.vstack(self._pending_training_vectors)
        min_needed = self._min_training_vectors or self._compute_min_training(sample.shape[1])
        if not force and sample.shape[0] < min_needed:
            return

        train_limit = min(sample.shape[0], self.training_samples)
        train_data = sample[:train_limit]
        try:
            self.faiss_index.train(train_data)
            print(f"âœ“ IVFPQç´¢å¼•è®­ç»ƒå®Œæˆ, ä½¿ç”¨ {train_limit:,} æ¡å‘é‡ä½œä¸ºè®­ç»ƒæ ·æœ¬")
        except Exception as exc:
            print(f"âš ï¸ IVFPQè®­ç»ƒå¤±è´¥({exc}), å›é€€åˆ° IndexIVFFlat")
            self._fallback_to_ivfflat(sample)
            return

        self.faiss_index.add(sample)
        self._ensure_nprobe()
        self.chunks.extend(self._pending_training_chunks)
        self._clear_training_buffer()

    def _fallback_to_ivfflat(self, sample: np.ndarray) -> None:
        backup_index = super()._create_cpu_ivf_index(sample.shape[1], sample.shape[0])
        if hasattr(backup_index, "is_trained") and not backup_index.is_trained:
            backup_index.train(sample)
        backup_index.add(sample)
        self.faiss_index = backup_index
        self._fallback_to_flat = True
        self._ensure_nprobe()
        self._clear_training_buffer()
        print("âœ“ å·²å›é€€è‡³ IndexIVFFlat, ä»å¯ç»§ç»­æ„å»ºä½†ç´¢å¼•ä½“ç§¯ä¼šå¢å¤§")

    def _process_and_add_to_faiss(self, chunks: List[Dict], texts: List[str],
                                  index_path: str, batch_size: int, num_workers: int):
        vectors = self._get_embeddings_batch(
            texts,
            batch_size=batch_size,
            show_progress=False,
            num_workers=num_workers
        )
        vectors = np.ascontiguousarray(vectors.astype(np.float32))
        faiss.normalize_L2(vectors)

        total_vectors = len(self.chunks) + self._pending_training_count + vectors.shape[0]
        self._ensure_faiss_index(vectors.shape[1], total_vectors)

        if hasattr(self.faiss_index, "is_trained") and not self.faiss_index.is_trained and not self._fallback_to_flat:
            self._append_training_buffer(vectors, chunks)
            self._attempt_training(force=False)
            return

        # ç´¢å¼•å·²è®­ç»ƒæˆ–å·²å›é€€ä¸ºIVFFlat
        self.faiss_index.add(vectors)
        self._ensure_nprobe()
        self.chunks.extend(chunks)

        # streaming ä¸‹ä»ç„¶æ”¯æŒcheckpoint
        if len(self.chunks) % 50000 < len(chunks):
            print(f"  ğŸ’¾ ä¿å­˜ä¸­é—´ç»“æœ ({len(self.chunks):,} ä¸ªæ–‡æœ¬å—)...")
            self._save_partial_index(index_path)

    def _save_partial_index(self, index_path: str):
        partial_dir = os.path.join(index_path, "partial")
        os.makedirs(partial_dir, exist_ok=True)

        chunks_file = os.path.join(partial_dir, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False)

        faiss_file = os.path.join(partial_dir, "faiss.index")
        index_to_save = self._cpu_index_for_persistence()
        if index_to_save is not None:
            faiss.write_index(index_to_save, faiss_file)

    def _load_partial_index(self, index_path: str):
        partial_dir = os.path.join(index_path, "partial")
        if not os.path.exists(partial_dir):
            return

        chunks_file = os.path.join(partial_dir, "chunks.json")
        if os.path.exists(chunks_file):
            with open(chunks_file, 'r', encoding='utf-8') as f:
                self.chunks = json.load(f)

        faiss_file = os.path.join(partial_dir, "faiss.index")
        if os.path.exists(faiss_file):
            self.faiss_index = faiss.read_index(faiss_file)
            self._faiss_on_gpu = False
            if hasattr(self.faiss_index, "nlist"):
                self.index_nlist = self.faiss_index.nlist
            if hasattr(self.faiss_index, "nprobe"):
                self.index_nprobe = self.faiss_index.nprobe
            if hasattr(self.faiss_index, "pq"):
                self._effective_pq_m = getattr(self.faiss_index.pq, "M", None)
            self._ensure_nprobe()

    def _finalize_faiss_index(self) -> None:
        self._attempt_training(force=True)
        if hasattr(self.faiss_index, "is_trained") and not self.faiss_index.is_trained:
            raise RuntimeError("Faissç´¢å¼•å°šæœªå®Œæˆè®­ç»ƒï¼Œæ— æ³•æœ€ç»ˆåŒ–")
        super()._finalize_faiss_index()
        # æ„å»ºå®Œæ¯•å¯é‡Šæ”¾ç¼“å­˜å‘é‡
        self.vectors = None

    def build_index(self, file_path: str, batch_size: int = 64,
                    max_chunks: Optional[int] = None, num_workers: int = 1):
        super().build_index(file_path, batch_size, max_chunks, num_workers)
        self.vectors = None
        gc.collect()

    def save_index(self, index_path: str):
        if self.faiss_index is None or not self.chunks:
            raise ValueError("ç´¢å¼•ä¸ºç©º,æ— æ³•ä¿å­˜")

        os.makedirs(index_path, exist_ok=True)

        chunks_file = os.path.join(index_path, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)

        self._ensure_nprobe()
        index_to_save = self._cpu_index_for_persistence()
        if index_to_save is None:
            raise RuntimeError("Faissç´¢å¼•æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")

        index_file = os.path.join(index_path, "faiss.index")
        faiss.write_index(index_to_save, index_file)

        metadata = {
            "model_name": self.model_name,
            "device": self.device,
            "max_seq_length": self.max_seq_length,
            "num_chunks": len(self.chunks),
            "vector_dim": self.faiss_index.d if hasattr(self.faiss_index, 'd') else None,
            "use_gpu_index": self.use_gpu_index,
            "index_type": "faiss.IndexIVFPQ" if not self._fallback_to_flat else "faiss.IndexIVFFlat",
            "nlist": self.index_nlist,
            "nprobe": self.index_nprobe,
            "pq_m": self._effective_pq_m,
            "pq_bits": self.pq_bits,
            "target_bytes_per_vector": self.target_bytes_per_vector,
            "store_embeddings_fp16": self.store_embeddings_fp16,
            "training_samples": self.training_samples,
            "fallback_to_flat": self._fallback_to_flat,
            "embedding_devices": self.embedding_devices,
        }
        metadata_file = os.path.join(index_path, "metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"âœ“ å‹ç¼©Faissç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")

    @classmethod
    def load_index(cls, index_path: str, model_name: Optional[str] = None,
                   device: str = "cuda", max_seq_length: int = 512,
                   use_gpu_index: bool = False, memory_map: bool = True,
                   read_only_index: bool = True,
                   embedding_devices: Optional[Sequence[str]] = None, **kwargs):
        chunks_file = os.path.join(index_path, "chunks.json")
        index_file = os.path.join(index_path, "faiss.index")
        metadata_file = os.path.join(index_path, "metadata.json")

        if not os.path.exists(chunks_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°chunksæ–‡ä»¶: {chunks_file}")
        if not os.path.exists(index_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°Faissç´¢å¼•æ–‡ä»¶: {index_file}")

        metadata: Optional[Dict[str, Any]] = None
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            if model_name is None:
                model_name = metadata.get("model_name", "sentence-transformers/all-MiniLM-L6-v2")
            if embedding_devices is None:
                embedding_devices = metadata.get("embedding_devices")
            print(f"âœ“ ä»å…ƒæ•°æ®è¯»å–é…ç½®: {metadata}")
        else:
            if model_name is None:
                model_name = "sentence-transformers/all-MiniLM-L6-v2"
            print("âš ï¸ æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶,ä½¿ç”¨é»˜è®¤é…ç½®")

        kw_embedding = kwargs.pop("embedding_devices", None)
        if embedding_devices is None and kw_embedding is not None:
            embedding_devices = kw_embedding

        pq_m = metadata.get("pq_m") if metadata else None
        pq_bits = metadata.get("pq_bits", 8) if metadata else 8
        target_bytes = metadata.get("target_bytes_per_vector") if metadata else kwargs.get("target_bytes_per_vector")
        store_fp16 = metadata.get("store_embeddings_fp16", True) if metadata else kwargs.get("store_embeddings_fp16", True)
        training_samples = metadata.get("training_samples", kwargs.get("training_samples", 262_144)) if metadata else kwargs.get("training_samples", 262_144)

        instance = cls(
            model_name=model_name,
            device=device,
            max_seq_length=max_seq_length,
            use_gpu_index=use_gpu_index,
            embedding_devices=embedding_devices,
            pq_m=pq_m,
            pq_bits=pq_bits,
            target_bytes_per_vector=target_bytes,
            training_samples=training_samples,
            store_embeddings_fp16=store_fp16,
            **{k: v for k, v in kwargs.items() if k not in {"target_bytes_per_vector", "store_embeddings_fp16", "training_samples"}}
        )

        # åŠ è½½æ–‡æœ¬å—
        chunks_jsonl = os.path.join(index_path, "chunks.jsonl")
        chunks_offsets = os.path.join(index_path, "chunks.offsets")
        
        if os.path.exists(chunks_jsonl) and os.path.exists(chunks_offsets):
             print(f"âœ“ æ£€æµ‹åˆ°ä¼˜åŒ–å­˜å‚¨: ä½¿ç”¨ DiskBasedChunks (æŒ‰éœ€è¯»å– {chunks_jsonl})")
             instance.chunks = DiskBasedChunks(chunks_jsonl, chunks_offsets)
        elif os.path.exists(chunks_file):
             with open(chunks_file, 'r', encoding='utf-8') as f:
                instance.chunks = json.load(f)

        io_flags = 0
        mmap_supported = hasattr(faiss, "IO_FLAG_MMAP")
        readonly_supported = hasattr(faiss, "IO_FLAG_READ_ONLY")
        attempted_mmap = False
        if memory_map and mmap_supported:
            io_flags |= faiss.IO_FLAG_MMAP
            attempted_mmap = True
        if read_only_index and readonly_supported:
            io_flags |= faiss.IO_FLAG_READ_ONLY

        load_errors = []
        cpu_index = None
        if io_flags:
            try:
                cpu_index = faiss.read_index(index_file, io_flags)
                if attempted_mmap:
                    print("âœ“ ä»¥å†…å­˜æ˜ å°„æ–¹å¼åŠ è½½å‹ç¼©Faissç´¢å¼•")
            except Exception as exc:  # pylint: disable=broad-except
                load_errors.append(exc)
                print(f"âš ï¸ å†…å­˜æ˜ å°„åŠ è½½å¤±è´¥({exc})ï¼Œå°†å›é€€åˆ°å¸¸è§„åŠ è½½")
        if cpu_index is None:
            try:
                cpu_index = faiss.read_index(index_file)
            except Exception as exc:
                load_errors.append(exc)
                raise RuntimeError(
                    f"æ— æ³•åŠ è½½Faissç´¢å¼•: {index_file}ï¼Œé”™è¯¯: {load_errors}"
                ) from exc

        instance.faiss_index = cpu_index
        instance._faiss_on_gpu = False
        if hasattr(cpu_index, "nlist"):
            instance.index_nlist = cpu_index.nlist
        if hasattr(cpu_index, "nprobe"):
            instance.index_nprobe = cpu_index.nprobe
        elif metadata and metadata.get("nprobe") is not None:
            instance.index_nprobe = int(metadata["nprobe"])
        elif instance.index_nlist is not None:
            instance.index_nprobe = min(16, max(1, instance.index_nlist))
        else:
            instance.index_nprobe = 1

        if hasattr(cpu_index, "pq"):
            instance._effective_pq_m = getattr(cpu_index.pq, "M", pq_m)
            instance.pq_bits = getattr(cpu_index.pq, "nbits", instance.pq_bits)

        instance._fallback_to_flat = bool(metadata.get("fallback_to_flat")) if metadata else False
        instance._min_training_vectors = instance._compute_min_training(cpu_index.d) if hasattr(cpu_index, "d") else None
        instance._ensure_nprobe()
        instance._finalize_faiss_index()

        print(f"âœ“ æˆåŠŸåŠ è½½å‹ç¼©Faissç´¢å¼•: {len(instance.chunks)} ä¸ªæ–‡æœ¬å—")
        return instance


def get_rag_index_class(use_faiss: bool = False, use_compact: bool = False, use_gainrag: bool = False):
    """æ ¹æ®é…ç½®è·å–æœ¬åœ°RAGç´¢å¼•ç±»"""
    if use_gainrag:
        if not _GAINRAG_TRANSFORMERS_AVAILABLE or GainRAGContriever is None:
            raise ImportError("æœªæ‰¾åˆ° transformers ä¾èµ–, æ— æ³•ä½¿ç”¨ GainRAGIndex")
        if not _FAISS_AVAILABLE:
            raise ImportError("faiss æœªå®‰è£…, æ— æ³•ä½¿ç”¨ GainRAGIndex")
        print("âœ… ä½¿ç”¨ GainRAGIndex (Contriever + GainRAG ç´¢å¼•)")
        return GainRAGIndex

    if not _SENTENCE_TRANSFORMERS_AVAILABLE:
        raise ImportError("sentence_transformers æœªå®‰è£…, æ— æ³•ä½¿ç”¨æœ¬åœ°RAGç´¢å¼•")

    if use_compact:
        if not _FAISS_AVAILABLE:
            print("âš ï¸ Faiss ä¸å¯ç”¨,æ— æ³•ä½¿ç”¨ Compact ç´¢å¼•, å›é€€åˆ° RAGIndexLocal (Numpy å®ç°)")
            return RAGIndexLocal
        print("âœ… ä½¿ç”¨ RAGIndexLocal_faiss_compact (æœ¬åœ°embedding + Faisså‹ç¼©ç´¢å¼•)")
        return RAGIndexLocal_faiss_compact

    if use_faiss:
        if not _FAISS_AVAILABLE:
            print("âš ï¸ Faiss ä¸å¯ç”¨,å›é€€åˆ° RAGIndexLocal (Numpy å®ç°)")
            return RAGIndexLocal
        print("âœ… ä½¿ç”¨ RAGIndexLocal_faiss (æœ¬åœ°embedding + FaissåŠ é€Ÿ)")
        return RAGIndexLocal_faiss

    print("âœ… ä½¿ç”¨ RAGIndexLocal (æœ¬åœ°embedding)")
    return RAGIndexLocal


class GainRAGIndex(BaseRAGIndex):
    """GainRAG é£æ ¼çš„ç´¢å¼•, å¤ç° GainRAG æ£€ç´¢é€»è¾‘ã€‚"""

    DEFAULT_MODEL_NAME = "facebook/contriever"

    def __init__(
        self,
        model_name: str = DEFAULT_MODEL_NAME,
        device: str = "cuda",
        index_path: Optional[str] = None,
        passages_path: Optional[str] = None,
        use_gpu_index: bool = False,
        gpu_id: int = 0,
        query_batch_size: int = 64,
        max_query_length: int = 512,
    ) -> None:
        if not _GAINRAG_TRANSFORMERS_AVAILABLE or GainRAGContriever is None:
            raise ImportError("GainRAGIndex éœ€è¦å®‰è£… transformers ä¾èµ– (AutoTokenizer/AutoConfig/BertModel)")
        if not _FAISS_AVAILABLE:
            raise ImportError("GainRAGIndex éœ€è¦ faiss æ”¯æŒ")

        self.model_name = model_name or self.DEFAULT_MODEL_NAME
        self.raw_device = device or "cuda"
        self.device = self._resolve_device(self.raw_device)
        self.torch_device = torch.device(self.device)
        self.use_gpu_index = bool(use_gpu_index)
        self.gpu_id = int(gpu_id)
        self.query_batch_size = max(1, int(query_batch_size))
        self.max_query_length = max(8, int(max_query_length))
        self.index_path = index_path
        self.passages_path = passages_path

        self.faiss_index: Optional["faiss.Index"] = None
        self.index_id_to_db_id: List[str] = []
        self.passages_map: Dict[str, Dict[str, Any]] = {}
        self.chunks: List[Dict[str, Any]] = []
        self.vector_dim: Optional[int] = None

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = GainRAGContriever.from_pretrained(self.model_name)
        self.model.eval()
        self.model.to(self.torch_device)

        if index_path:
            self._load_index_files(index_path)
        if passages_path:
            self._load_passages(passages_path)

    @staticmethod
    def _resolve_device(device: str) -> str:
        if device and device.lower().startswith("cuda") and torch.cuda.is_available():
            return device
        if device and device.lower().startswith("cpu"):
            return "cpu"
        if torch.cuda.is_available():
            return "cuda"
        return "cpu"

    def _load_index_files(self, index_dir: str) -> None:
        index_file = os.path.join(index_dir, "index.faiss")
        meta_file = os.path.join(index_dir, "index_meta.faiss")

        if not os.path.exists(index_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ° GainRAG ç´¢å¼•æ–‡ä»¶: {index_file}")

        cpu_index = faiss.read_index(index_file)

        self.faiss_index = cpu_index
        if self.use_gpu_index and hasattr(faiss, "StandardGpuResources"):
            try:
                num_gpus = faiss.get_num_gpus() if hasattr(faiss, "get_num_gpus") else 0
                if num_gpus <= 0:
                    print("âš ï¸ GainRAGIndex: æœªæ£€æµ‹åˆ°å¯ç”¨GPU, å›é€€åˆ°CPUç´¢å¼•")
                    self.use_gpu_index = False
                else:
                    target_gpu = min(max(0, self.gpu_id), num_gpus - 1)
                    resources = faiss.StandardGpuResources()
                    self.faiss_index = faiss.index_cpu_to_gpu(resources, target_gpu, cpu_index)
            except Exception as exc:  # pylint: disable=broad-except
                print(f"âš ï¸ GainRAGIndex: ç´¢å¼•è¿ç§»åˆ°GPUå¤±è´¥({exc}), å›é€€åˆ°CPUç´¢å¼•")
                self.faiss_index = cpu_index

        if not os.path.exists(meta_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ° GainRAG ç´¢å¼•æ˜ å°„æ–‡ä»¶: {meta_file}")
        with open(meta_file, "rb") as reader:
            mapping = pickle.load(reader)
        self.index_id_to_db_id = [str(doc_id) for doc_id in mapping]
        self.vector_dim = self.faiss_index.d if hasattr(self.faiss_index, "d") else None
        self.index_path = index_dir

    def _load_passages(self, path: str) -> None:
        if not os.path.exists(path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ° GainRAG passages æ–‡ä»¶: {path}")

        passages: List[Dict[str, Any]] = []
        if path.endswith(".jsonl"):
            with open(path, "r", encoding="utf-8") as fin:
                for line in fin:
                    line = line.strip()
                    if not line:
                        continue
                    passages.append(json.loads(line))
        elif path.endswith(".json"):
            with open(path, "r", encoding="utf-8") as fin:
                data = json.load(fin)
            if isinstance(data, dict):
                candidates = data.get("passages") or data.get("items") or data.get("data")
                if isinstance(candidates, list):
                    passages = candidates
                else:
                    passages = [data]
            elif isinstance(data, list):
                passages = data
        else:
            with open(path, "r", encoding="utf-8") as fin:
                reader = csv.reader(fin, delimiter="\t")
                for row in reader:
                    if not row:
                        continue
                    if row[0] == "id":
                        continue
                    text = row[1] if len(row) > 1 else ""
                    title = row[2] if len(row) > 2 else ""
                    passages.append({"id": row[0], "title": title, "text": text})

        normalized: List[Dict[str, Any]] = []
        self.passages_map = {}
        for passage in passages:
            if not isinstance(passage, dict):
                continue
            doc_id = str(passage.get("id") or passage.get("docid") or passage.get("doc_id") or len(normalized))
            normalized_passage = dict(passage)
            normalized_passage["id"] = doc_id
            normalized_passage.setdefault("title", passage.get("title", ""))
            normalized_passage.setdefault("text", passage.get("text", ""))
            self.passages_map[doc_id] = normalized_passage
            normalized.append(normalized_passage)

        if not normalized:
            raise ValueError(f"æœªèƒ½ä» {path} è§£æå‡ºæœ‰æ•ˆçš„ passages æ•°æ®")

        self.chunks = normalized
        self.passages_path = path

    def _ensure_ready(self) -> None:
        if self.faiss_index is None:
            raise RuntimeError("GainRAGIndex: ç´¢å¼•å°šæœªåŠ è½½")
        if not self.passages_map:
            raise RuntimeError("GainRAGIndex: passages å°šæœªåŠ è½½")

    def _encode_queries(self, queries: List[str]) -> np.ndarray:
        if not queries:
            return np.zeros((0, self.vector_dim or 0), dtype=np.float32)

        self.model.eval()
        outputs: List[torch.Tensor] = []
        with torch.no_grad():
            for start in range(0, len(queries), self.query_batch_size):
                batch = queries[start:start + self.query_batch_size]
                encoded = self.tokenizer(
                    batch,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=self.max_query_length,
                )
                encoded = {key: value.to(self.torch_device) for key, value in encoded.items()}
                embeddings = self.model(**encoded)
                outputs.append(embeddings.detach().cpu())

        if not outputs:
            return np.zeros((0, self.vector_dim or 0), dtype=np.float32)

        matrix = torch.cat(outputs, dim=0).numpy().astype(np.float32)
        if self.vector_dim is None and matrix.size > 0:
            self.vector_dim = matrix.shape[1]
        return matrix

    def _store_embeddings(self, vectors: np.ndarray) -> None:  # type: ignore[override]
        self._pending_embeddings = np.asarray(vectors, dtype=np.float32)

    def _finalize_embeddings(self) -> None:  # type: ignore[override]
        # GainRAGIndex ä¸åœ¨æœ¬ç±»ä¸­æ„å»ºç´¢å¼•, å› æ­¤æ— éœ€é¢å¤–å¤„ç†
        return

    def build_index(self, *args, **kwargs) -> None:  # type: ignore[override]
        raise NotImplementedError("GainRAGIndex æš‚ä¸æ”¯æŒåœ¨è¯¥ä»“åº“ä¸­ç›´æ¥æ„å»ºç´¢å¼•, è¯·å…ˆç¦»çº¿ç”ŸæˆGainRAGç´¢å¼•")

    def save_index(self, index_path: str) -> None:  # type: ignore[override]
        raise NotImplementedError("GainRAGIndex æš‚ä¸æ”¯æŒä¿å­˜ç´¢å¼•, è¯·ä½¿ç”¨ GainRAG å®˜æ–¹è„šæœ¬")

    @classmethod
    def load_index(
        cls,
        index_path: str,
        model_name: Optional[str] = None,
        device: str = "cuda",
        passages_path: Optional[str] = None,
        use_gpu_index: bool = False,
        gpu_id: int = 0,
        query_batch_size: int = 64,
        max_query_length: int = 512,
        **kwargs: Any,
    ) -> "GainRAGIndex":
        if not index_path:
            raise ValueError("GainRAGIndex.load_index éœ€è¦æä¾› index_path")

        resolved_model = model_name or cls.DEFAULT_MODEL_NAME

        resolved_passages = passages_path
        if resolved_passages is None:
            for candidate in ("passages.jsonl", "passages.json", "passages.csv"):
                candidate_path = os.path.join(index_path, candidate)
                if os.path.exists(candidate_path):
                    resolved_passages = candidate_path
                    break
        if resolved_passages is None:
            raise FileNotFoundError("GainRAGIndex éœ€è¦æä¾› passages æ–‡ä»¶è·¯å¾„")

        instance = cls(
            model_name=resolved_model,
            device=device,
            index_path=index_path,
            passages_path=resolved_passages,
            use_gpu_index=use_gpu_index,
            gpu_id=gpu_id,
            query_batch_size=query_batch_size,
            max_query_length=max_query_length,
        )
        return instance

    def query(self, query: str, top_k: int = 3) -> str:  # type: ignore[override]
        if not isinstance(query, str) or not query.strip():
            raise ValueError("æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        self._ensure_ready()

        sanitized_query = query.strip()
        vectors = self._encode_queries([sanitized_query])
        if vectors.size == 0:
            return "[æŸ¥è¯¢é”™è¯¯] æŸ¥è¯¢ç¼–ç å¤±è´¥"

        search_vectors = np.ascontiguousarray(vectors.astype(np.float32))
        if search_vectors.ndim == 1:
            search_vectors = search_vectors.reshape(1, -1)

        k = max(1, int(top_k))
        distances, indices = self.faiss_index.search(search_vectors, k)

        retrieved_chunks: List[Dict[str, Any]] = []
        seen_ids: set[str] = set()
        for raw_idx in indices[0]:
            idx = int(raw_idx)
            if idx < 0 or idx >= len(self.index_id_to_db_id):
                continue
            doc_id = self.index_id_to_db_id[idx]
            if doc_id in seen_ids:
                continue
            passage = self.passages_map.get(doc_id)
            if passage is None:
                continue
            retrieved_chunks.append(passage)
            seen_ids.add(doc_id)
            if len(retrieved_chunks) >= k:
                break

        if not retrieved_chunks:
            return "[æŸ¥è¯¢é”™è¯¯] æœªæ£€ç´¢åˆ°æœ‰æ•ˆçš„æ–‡æœ¬å—"

        formatted = [self._format_chunk_for_output(chunk) for chunk in retrieved_chunks]
        context = "\n---\n".join(formatted)
        return f"### Retrieved Context:\n{context}"


# ============================================================================
# GainRAG ç›¸å…³æ¨¡å‹
# ============================================================================

if BertModel is not None:

    class GainRAGContriever(BertModel):
        """Minimal Contriever implementation compatible with GainRAG indices."""

        def __init__(self, config, pooling: str = "average", **kwargs):
            super().__init__(config, add_pooling_layer=False)
            if not hasattr(config, "pooling"):
                self.config.pooling = pooling

        def forward(
            self,
            input_ids: Optional[torch.LongTensor] = None,
            attention_mask: Optional[torch.Tensor] = None,
            token_type_ids: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.Tensor] = None,
            head_mask: Optional[torch.Tensor] = None,
            inputs_embeds: Optional[torch.Tensor] = None,
            encoder_hidden_states: Optional[torch.Tensor] = None,
            encoder_attention_mask: Optional[torch.Tensor] = None,
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            normalize: bool = False,
        ) -> torch.Tensor:
            model_output = super().forward(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                position_ids=position_ids,
                head_mask=head_mask,
                inputs_embeds=inputs_embeds,
                encoder_hidden_states=encoder_hidden_states,
                encoder_attention_mask=encoder_attention_mask,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
            )

            last_hidden = (
                model_output.last_hidden_state
                if hasattr(model_output, "last_hidden_state")
                else model_output[0]
            )

            if attention_mask is None:
                if input_ids is None:
                    raise ValueError("GainRAGContriever éœ€è¦ attention_mask æˆ– input_ids")
                attention_mask = (input_ids != 0).long()

            mask = attention_mask[..., None].bool()
            last_hidden = last_hidden.masked_fill(~mask, 0.0)

            if getattr(self.config, "pooling", "average") == "average":
                denom = attention_mask.sum(dim=1, keepdim=True).clamp(min=1)
                emb = last_hidden.sum(dim=1) / denom
            elif self.config.pooling == "sqrt":
                denom = attention_mask.sum(dim=1, keepdim=True).float().clamp(min=1)
                emb = last_hidden.sum(dim=1) / torch.sqrt(denom)
            elif self.config.pooling == "cls":
                emb = last_hidden[:, 0]
            else:
                emb = last_hidden.sum(dim=1)

            if normalize:
                emb = torch.nn.functional.normalize(emb, dim=-1)

            return emb

else:  # pragma: no cover - transformers æœªå®‰è£…æ—¶çš„å…œåº•

    GainRAGContriever = None  # type: ignore


# ============================================================================
# GainRAG ç´¢å¼•
# ============================================================================




