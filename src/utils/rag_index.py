import os
import json
import numpy as np
from typing import Dict, List, Any, Optional
from tqdm import trange, tqdm
import sys
import gc
import pickle
from multiprocessing import Pool, cpu_count
import time
from abc import ABC, abstractmethod
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pdb

# Check if Faiss is available
_FAISS_AVAILABLE = True
try:
    import faiss
except ImportError:
    _FAISS_AVAILABLE = False

# Check if sentence_transformers is available
_SENTENCE_TRANSFORMERS_AVAILABLE = True
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    _SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ sentence_transformers æœªå®‰è£…,æœ¬åœ°embeddingåŠŸèƒ½ä¸å¯ç”¨")


# ============================================================================
# Faiss è¾…åŠ©å‡½æ•°
# ============================================================================

def _suggest_ivf_params(num_vectors: int,
                        max_nlist: int = 4096,
                        max_nprobe: int = 16) -> tuple[int, int]:
    """
    æ ¹æ®å‘é‡æ•°é‡ç»™å‡ºåˆé€‚çš„IVFå‚æ•°

    Returns:
        (nlist, nprobe)
    """
    if num_vectors <= 0:
        return 1, 1
    nlist = max(1, min(max_nlist, int(np.sqrt(num_vectors))))
    nprobe = max(1, min(max_nprobe, nlist))
    return nlist, nprobe


# ============================================================================
# å¤šè¿›ç¨‹è¾…åŠ©å‡½æ•°ï¼ˆå¿…é¡»åœ¨æ¨¡å—çº§åˆ«å®šä¹‰æ‰èƒ½è¢«pickleï¼‰
# ============================================================================

def _encode_chunk_worker(args):
    """
    å¤šè¿›ç¨‹workerå‡½æ•°ï¼šç¼–ç ä¸€æ‰¹æ–‡æœ¬
    å¿…é¡»åœ¨æ¨¡å—çº§åˆ«å®šä¹‰ä»¥æ”¯æŒpickleåºåˆ—åŒ–
    
    Args:
        args: (chunk_texts, model_name, max_seq_length, batch_size_inner, worker_id)
    
    Returns:
        ç¼–ç åçš„å‘é‡æ•°ç»„
    """
    chunk_texts, model_name, max_seq_length, batch_size_inner, worker_id = args
    
    import os
    pid = os.getpid()
    
    # åœ¨workerè¿›ç¨‹ä¸­åŠ è½½æ¨¡å‹
    from sentence_transformers import SentenceTransformer
    import numpy as np
    import time
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # åŠ è½½æ¨¡å‹ï¼ˆè¿™æ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†ï¼‰
    print(f"  Worker {worker_id} (PID {pid}): å¼€å§‹åŠ è½½æ¨¡å‹...")
    model = SentenceTransformer(model_name, device='cpu')
    model.max_seq_length = max_seq_length
    load_time = time.time() - start_time
    print(f"  Worker {worker_id} (PID {pid}): æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {load_time:.1f}ç§’")
    
    # å¤„ç†æ–‡æœ¬
    print(f"  Worker {worker_id} (PID {pid}): å¼€å§‹å¤„ç† {len(chunk_texts):,} æ¡æ–‡æœ¬...")
    chunk_embeddings = []
    process_start = time.time()
    
    for i in range(0, len(chunk_texts), batch_size_inner):
        batch = chunk_texts[i:i+batch_size_inner]
        batch_emb = model.encode(
            batch,
            convert_to_numpy=True,
            show_progress_bar=False,
            normalize_embeddings=False
        )
        chunk_embeddings.append(batch_emb)
    
    process_time = time.time() - process_start
    total_time = time.time() - start_time
    print(f"  Worker {worker_id} (PID {pid}): å¤„ç†å®Œæˆï¼Œå¤„ç†è€—æ—¶ {process_time:.1f}ç§’ï¼Œæ€»è€—æ—¶ {total_time:.1f}ç§’")
    
    return np.vstack(chunk_embeddings).astype(np.float32)


# ============================================================================
# RAG ç´¢å¼•åŸºç±»
# ============================================================================



class BaseRAGIndex(ABC):
    """
    RAG ç´¢å¼•å…¬å…±åŸºç±»ï¼Œè´Ÿè´£æ¨¡å‹åŠ è½½ã€æ–‡æœ¬è§£æä¸embeddingç”Ÿæˆã€‚
    å­ç±»éœ€å®ç°å‘é‡å­˜å‚¨ã€ç´¢å¼•æŒä¹…åŒ–åŠæŸ¥è¯¢ç­‰ç‰¹å®šé€»è¾‘ã€‚
    """

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 device: str = "cuda", max_seq_length: int = 512) -> None:
        if not _SENTENCE_TRANSFORMERS_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£… sentence_transformers: pip install sentence-transformers")

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name, device=device)
        model_max_length = self.model.get_max_seq_length()
        actual_max_length = min(max_seq_length, model_max_length)
        self.model.max_seq_length = actual_max_length
        if actual_max_length < max_seq_length:
            print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹æœ€å¤§æ”¯æŒé•¿åº¦ä¸º {model_max_length}ï¼Œå·²å°† max_seq_length ä» {max_seq_length} è°ƒæ•´ä¸º {actual_max_length}")
        print(f"âœ“ æ¨¡å‹å·²åŠ è½½åˆ° {device}, æœ€å¤§åºåˆ—é•¿åº¦: {actual_max_length}")

        self.chunks: List[Dict[str, Any]] = []
        self.model_name = model_name
        self.device = device
        self.max_seq_length = actual_max_length

    @staticmethod
    def _normalize_embeddings(vectors: np.ndarray) -> np.ndarray:
        if vectors is None:
            raise ValueError("å‘é‡ä¸ºç©ºï¼Œæ— æ³•å½’ä¸€åŒ–")
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        norms[norms == 0] = 1e-9
        return vectors / norms

    def _resolve_num_workers(self, num_workers: int) -> int:
        if num_workers == 0:
            if self.device == 'cpu':
                resolved = max(1, cpu_count() // 2)
                print(f"ğŸ’¡ è‡ªåŠ¨è®¾ç½®workeræ•°ä¸ºCPUæ ¸å¿ƒæ•°çš„ä¸€åŠ: {resolved}")
                return resolved
            return 1
        return num_workers

    def _combine_sample_text(self, sample: Dict[str, Any]) -> Optional[str]:
        if not isinstance(sample, dict):
            return None
        text = sample.get("text")
        if not isinstance(text, str) or not text:
            return None
        title = sample.get("title", "")
        return f"{title}\n\n{text}" if title else text

    def _prepare_embedding_texts(self) -> List[str]:
        print("æ­£åœ¨åˆå¹¶titleå’Œtext...")
        texts: List[str] = []
        for chunk in tqdm(self.chunks, desc="å¤„ç†æ–‡æœ¬"):
            combined_text = self._combine_sample_text(chunk)
            if combined_text:
                texts.append(combined_text)
        print(f"âœ“ æ–‡æœ¬åˆå¹¶å®Œæˆ")
        print()
        return texts

    def _load_kb_file(self, file_path: str) -> List[Any]:
        kb_content: List[Any] = []
        if file_path.endswith('.jsonl'):
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in tqdm(f, desc="è¯»å–JSONLæ–‡ä»¶"):
                    if line.strip():
                        kb_content.append(json.loads(line))
        elif file_path.endswith('.json'):
            with open(file_path, 'r', encoding='utf-8') as f:
                kb_content = json.load(f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
        return kb_content

    def _extract_valid_chunks(self, kb_content: List[Any],
                              max_chunks: Optional[int]) -> List[Dict[str, Any]]:
        chunks = [
            sample for sample in kb_content
            if isinstance(sample, dict) and "text" in sample and
            isinstance(sample["text"], str) and sample["text"]
        ]
        if max_chunks:
            print(f"âš ï¸  é™åˆ¶æœ€å¤§æ–‡æœ¬å—æ•°: {max_chunks}")
            chunks = chunks[:max_chunks]
        if not chunks:
            raise ValueError("æ— æ³•ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬å—")
        return chunks

    def _format_chunk_for_output(self, chunk: Any) -> str:
        if isinstance(chunk, dict):
            title = chunk.get("title", "")
            text = chunk.get("text", "")
            has_title = isinstance(title, str) and title.strip()
            has_text = isinstance(text, str) and text.strip()
            if has_title and has_text:
                return f"[{title}]\n{text}"
            if has_text:
                return text
            if has_title:
                return title
            return json.dumps(chunk, ensure_ascii=False)
        return str(chunk)

    def _get_embeddings_batch(self, texts: List[str], batch_size: int = 32,
                              show_progress: bool = True, num_workers: int = 1) -> np.ndarray:
        total_texts = len(texts)
        start_time = time.time()

        print(f"ğŸ“Š Embeddingç”Ÿæˆé…ç½®:")
        print(f"  - æ€»æ–‡æœ¬æ•°: {total_texts:,}")
        print(f"  - æ‰¹å¤§å°: {batch_size}")
        print(f"  - è®¾å¤‡: {self.device}")

        if self.device != 'cpu' and num_workers > 1:
            print(f"  âš ï¸  GPUæ¨¡å¼ä¸‹ä¸æ”¯æŒå¤šè¿›ç¨‹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å•è¿›ç¨‹")
            num_workers = 1
        else:
            print(f"  - Workeræ•°: {num_workers}")

        embeddings: List[np.ndarray] = []

        if num_workers > 1 and self.device == 'cpu':
            print(f"\nğŸš€ ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ (workers={num_workers})...")
            print(f"ğŸ’¡ æç¤º: æ¯ä¸ªworkeréœ€è¦å…ˆåŠ è½½æ¨¡å‹ï¼ˆçº¦10-30ç§’ï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...\n")

            chunk_size_per_worker = (total_texts + num_workers - 1) // num_workers
            text_chunks = [texts[i:i + chunk_size_per_worker] for i in range(0, total_texts, chunk_size_per_worker)]

            args_list = [
                (chunk, self.model_name, self.max_seq_length, batch_size, idx)
                for idx, chunk in enumerate(text_chunks)
            ]

            print(f"æ¯ä¸ªworkerå¤„ç†çº¦ {chunk_size_per_worker:,} æ¡æ–‡æœ¬")
            print(f"å¼€å§‹å¯åŠ¨ {num_workers} ä¸ªworkerè¿›ç¨‹...\n")

            with Pool(processes=num_workers) as pool:
                results = []
                with tqdm(total=num_workers, desc="å¤šè¿›ç¨‹å¤„ç†", unit="worker",
                          bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
                    for result in pool.imap(_encode_chunk_worker, args_list):
                        results.append(result)
                        pbar.update(1)

            embeddings = np.vstack(results).astype(np.float32)
            print(f"\nâœ“ æ‰€æœ‰workerå®Œæˆï¼")

        else:
            iterator = range(0, total_texts, batch_size)
            if show_progress:
                iterator = tqdm(
                    iterator,
                    desc="ç”Ÿæˆembeddings",
                    unit="batch",
                    total=(total_texts + batch_size - 1) // batch_size,
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
                )

            for i in iterator:
                batch = texts[i:i + batch_size]
                batch_embeddings = self.model.encode(
                    batch,
                    convert_to_numpy=True,
                    show_progress_bar=False,
                    normalize_embeddings=False
                )
                embeddings.append(batch_embeddings)

                if (i // batch_size) % 10 == 0:
                    gc.collect()

            embeddings = np.vstack(embeddings).astype(np.float32)

        elapsed = time.time() - start_time
        speed = total_texts / elapsed if elapsed > 0 else 0.0
        print(f"\nâœ“ Embeddingç”Ÿæˆå®Œæˆ!")
        print(f"  - è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"  - é€Ÿåº¦: {speed:.1f} æ–‡æœ¬/ç§’")
        print(f"  - å‘é‡å½¢çŠ¶: {embeddings.shape}")

        return embeddings

    def build_index(self, file_path: str, batch_size: int = 64,
                    max_chunks: Optional[int] = None, num_workers: int = 1) -> None:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        num_workers = self._resolve_num_workers(num_workers)

        print(f"\n{'='*60}")
        print(f"ğŸ“‚ æ­¥éª¤ 1/4: åŠ è½½çŸ¥è¯†åº“")
        print(f"{'='*60}")
        print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
        start_time = time.time()

        kb_content = self._load_kb_file(file_path)

        load_time = time.time() - start_time
        print(f"âœ“ æ–‡ä»¶åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")

        print(f"\n{'='*60}")
        print(f"ğŸ” æ­¥éª¤ 2/4: æå–æ–‡æœ¬å—")
        print(f"{'='*60}")
        self.chunks = self._extract_valid_chunks(kb_content, max_chunks)
        print(f"âœ“ æˆåŠŸæå– {len(self.chunks):,} ä¸ªæ–‡æœ¬å—")

        print(f"\n{'='*60}")
        print(f"ğŸ¤– æ­¥éª¤ 3/4: ç”ŸæˆEmbeddings")
        print(f"{'='*60}")
        texts = self._prepare_embedding_texts()

        vectors = self._get_embeddings_batch(
            texts,
            batch_size=batch_size,
            show_progress=True,
            num_workers=num_workers
        )

        self._store_embeddings(vectors)

        print(f"\n{'='*60}")
        print(f"ğŸ“ æ­¥éª¤ 4/4: å‘é‡åå¤„ç†")
        print(f"{'='*60}")
        self._finalize_embeddings()

        gc.collect()

    @abstractmethod
    def _store_embeddings(self, vectors: np.ndarray) -> None:
        """å­ç±»è´Ÿè´£ç¼“å­˜æˆ–å†™å…¥embeddingå‘é‡ã€‚"""

    @abstractmethod
    def _finalize_embeddings(self) -> None:
        """å­ç±»è´Ÿè´£å‘é‡çš„æœ€ç»ˆå¤„ç†æµç¨‹ï¼Œä¾‹å¦‚å½’ä¸€åŒ–æˆ–æ„å»ºç´¢å¼•ã€‚"""

    def _save_checkpoint(self, checkpoint_file: str, processed_lines: int,
                         processed_chunks: int) -> None:
        checkpoint = {
            "processed_lines": processed_lines,
            "processed_chunks": processed_chunks,
            "timestamp": time.time()
        }
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint, f, indent=2)

    def build_index_streaming(self, *args, **kwargs) -> None:
        raise NotImplementedError("è¯¥ç´¢å¼•ç±»å‹æœªå®ç°æµå¼æ„å»ºï¼Œè¯·åœ¨å­ç±»ä¸­å®ç°ã€‚")

    @abstractmethod
    def save_index(self, index_path: str) -> None:
        """å­ç±»è´Ÿè´£ç´¢å¼•æŒä¹…åŒ–ã€‚"""

    @classmethod
    @abstractmethod
    def load_index(cls, index_path: str, model_name: Optional[str] = None,
                   device: str = "cuda", max_seq_length: int = 512, **kwargs) -> "BaseRAGIndex":
        """å­ç±»è´Ÿè´£ä»ç£ç›˜åŠ è½½ç´¢å¼•ã€‚"""

    @abstractmethod
    def query(self, query: str, top_k: int = 3) -> str:
        """å­ç±»è´Ÿè´£å®ç°æŸ¥è¯¢é€»è¾‘ã€‚"""


# ============================================================================
# æœ¬åœ° RAG ç´¢å¼•å®ç°
# ============================================================================

class RAGIndexLocal(BaseRAGIndex):
    """
    æœ¬åœ°RAGç´¢å¼•å®ç°,ä½¿ç”¨sentence_transformersè¿›è¡Œembedding
    ä¼˜åŒ–å†…å­˜æ¶ˆè€—å’Œæ•ˆç‡,é€‚åˆå¤§è§„æ¨¡çŸ¥è¯†åº“
    """
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2", 
                 device: str = "cuda", max_seq_length: int = 512):
        super().__init__(model_name=model_name, device=device, max_seq_length=max_seq_length)
        self.vectors = None
        self.normalized_vectors = None

    def _store_embeddings(self, vectors: np.ndarray) -> None:
        self.vectors = vectors.astype(np.float32)

    def _finalize_embeddings(self) -> None:
        self._normalize_vectors()

    def _normalize_vectors(self):
        """å½’ä¸€åŒ–å‘é‡ä»¥åŠ é€Ÿä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—"""
        if self.vectors is not None:
            self.normalized_vectors = self._normalize_embeddings(self.vectors)
            print("âœ“ å‘é‡å·²å½’ä¸€åŒ–")

    def build_index_streaming(self, file_path: str, index_path: str, 
                             batch_size: int = 64, chunk_size: int = 10000,
                             max_chunks: Optional[int] = None, num_workers: int = 1,
                             resume: bool = True):
        """
        æµå¼æ„å»ºç´¢å¼•ï¼Œæ”¯æŒè¶…å¤§è§„æ¨¡çŸ¥è¯†åº“ï¼ˆè¾¹åŠ è½½è¾¹å¤„ç†è¾¹ä¿å­˜ï¼‰
        
        Args:
            file_path: çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„ (ä»…æ”¯æŒjsonlæ ¼å¼)
            index_path: ç´¢å¼•ä¿å­˜ç›®å½•
            batch_size: embeddingç”Ÿæˆçš„æ‰¹é‡å¤§å°
            chunk_size: æ¯æ¬¡å¤„ç†çš„æ–‡æœ¬å—æ•°é‡ï¼ˆæ§åˆ¶å†…å­˜ä½¿ç”¨ï¼‰
            max_chunks: æœ€å¤§å¤„ç†çš„æ–‡æœ¬å—æ•°é‡(ç”¨äºæµ‹è¯•),Noneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
            num_workers: å¹¶è¡Œå¤„ç†çš„workeræ•°é‡
            resume: æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        """
        if not file_path.endswith('.jsonl'):
            raise ValueError("æµå¼æ„å»ºä»…æ”¯æŒJSONLæ ¼å¼æ–‡ä»¶")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è‡ªåŠ¨ç¡®å®šworkeræ•°é‡
        num_workers = self._resolve_num_workers(num_workers)
        
        os.makedirs(index_path, exist_ok=True)
        
        # æ£€æŸ¥checkpoint
        checkpoint_file = os.path.join(index_path, "checkpoint.json")
        start_line = 0
        processed_chunks = 0
        
        if resume and os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
                start_line = checkpoint.get("processed_lines", 0)
                processed_chunks = checkpoint.get("processed_chunks", 0)
            print(f"ğŸ’¾ ä»checkpointæ¢å¤: å·²å¤„ç† {processed_chunks:,} ä¸ªæ–‡æœ¬å—")
        
        print(f"\n{'='*60}")
        print(f"ğŸŒŠ æµå¼ç´¢å¼•æ„å»ºæ¨¡å¼")
        print(f"{'='*60}")
        print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
        print(f"ç´¢å¼•è·¯å¾„: {index_path}")
        print(f"æ¯æ‰¹å¤„ç†: {chunk_size:,} ä¸ªæ–‡æœ¬å—")
        print(f"Batchå¤§å°: {batch_size}")
        print(f"Workeræ•°: {num_workers}")
        print(f"{'='*60}\n")
        
        total_start_time = time.time()
        batch_chunks = []
        batch_texts = []
        current_line = 0
        batch_num = processed_chunks // chunk_size
        
        # æ‰“å¼€æ–‡ä»¶å¹¶é€è¡Œè¯»å–
        with open(file_path, 'r', encoding='utf-8') as f:
            # è·³è¿‡å·²å¤„ç†çš„è¡Œ
            if start_line > 0:
                print(f"â­ï¸  è·³è¿‡å·²å¤„ç†çš„ {start_line:,} è¡Œ...")
                for _ in range(start_line):
                    next(f)
                current_line = start_line
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºæ–‡ä»¶è¯»å–è¿›åº¦
            pbar = tqdm(desc="å¤„ç†æ–‡æœ¬å—", unit="å—", initial=processed_chunks)
            
            for line in f:
                current_line += 1
                
                if not line.strip():
                    continue
                
                try:
                    sample = json.loads(line)
                    
                    # éªŒè¯æ•°æ®æ ¼å¼
                    if not (isinstance(sample, dict) and "text" in sample and 
                           isinstance(sample["text"], str) and sample["text"]):
                        continue
                    
                    # åˆå¹¶titleå’Œtext
                    title = sample.get("title", "")
                    text = sample.get("text", "")
                    combined_text = f"{title}\n\n{text}" if title else text
                    
                    batch_chunks.append(sample)
                    batch_texts.append(combined_text)
                    
                    # è¾¾åˆ°chunk_sizeæˆ–è¾¾åˆ°max_chunksé™åˆ¶æ—¶å¤„ç†è¿™ä¸€æ‰¹
                    if len(batch_chunks) >= chunk_size or \
                       (max_chunks and processed_chunks + len(batch_chunks) >= max_chunks):
                        
                        # å¤„ç†å½“å‰æ‰¹æ¬¡
                        self._process_and_save_batch(
                            batch_chunks, batch_texts, batch_num,
                            index_path, batch_size, num_workers
                        )
                        
                        processed_chunks += len(batch_chunks)
                        pbar.update(len(batch_chunks))
                        batch_num += 1
                        
                        # ä¿å­˜checkpoint
                        self._save_checkpoint(checkpoint_file, current_line, processed_chunks)
                        
                        # æ¸…ç©ºæ‰¹æ¬¡
                        batch_chunks = []
                        batch_texts = []
                        gc.collect()
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°max_chunks
                        if max_chunks and processed_chunks >= max_chunks:
                            print(f"\nâš ï¸  å·²è¾¾åˆ°æœ€å¤§æ–‡æœ¬å—æ•°é™åˆ¶: {max_chunks:,}")
                            break
                
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  è·³è¿‡æ— æ•ˆJSON (è¡Œ{current_line}): {e}")
                    continue
            
            # å¤„ç†å‰©ä½™çš„æ‰¹æ¬¡
            if batch_chunks:
                self._process_and_save_batch(
                    batch_chunks, batch_texts, batch_num,
                    index_path, batch_size, num_workers
                )
                processed_chunks += len(batch_chunks)
                pbar.update(len(batch_chunks))
                self._save_checkpoint(checkpoint_file, current_line, processed_chunks)
            
            pbar.close()
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç´¢å¼•
        print(f"\n{'='*60}")
        print(f"ğŸ”— åˆå¹¶ç´¢å¼•åˆ†ç‰‡")
        print(f"{'='*60}")
        self._merge_index_shards(index_path, batch_num + 1)
        
        # åˆ é™¤checkpointï¼ˆæ„å»ºå®Œæˆï¼‰
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
        
        total_time = time.time() - total_start_time
        print(f"\n{'='*60}")
        print(f"âœ… æµå¼ç´¢å¼•æ„å»ºå®Œæˆ!")
        print(f"{'='*60}")
        print(f"æ€»æ–‡æœ¬å—æ•°: {processed_chunks:,}")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"å¹³å‡é€Ÿåº¦: {processed_chunks/total_time:.1f} å—/ç§’")
        print(f"ç´¢å¼•è·¯å¾„: {index_path}")
        print(f"{'='*60}\n")
    
    def _process_and_save_batch(self, chunks: List[Dict], texts: List[str],
                                batch_num: int, index_path: str,
                                batch_size: int, num_workers: int):
        """å¤„ç†å¹¶ä¿å­˜ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
        print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ #{batch_num} ({len(chunks):,} ä¸ªæ–‡æœ¬å—)...")
        
        start_time = time.time()
        
        # ç”Ÿæˆembeddings
        vectors = self._get_embeddings_batch(
            texts,
            batch_size=batch_size,
            show_progress=False,  # å¤–å±‚å·²æœ‰è¿›åº¦æ¡
            num_workers=num_workers
        )
        
        # ä¿å­˜è¿™ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        batch_dir = os.path.join(index_path, f"batch_{batch_num:04d}")
        os.makedirs(batch_dir, exist_ok=True)
        
        # ä¿å­˜chunks
        chunks_file = os.path.join(batch_dir, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False)
        
        # ä¿å­˜vectors
        vectors_file = os.path.join(batch_dir, "vectors.npy")
        np.save(vectors_file, vectors)
        
        elapsed = time.time() - start_time
        speed = len(chunks) / elapsed
        print(f"  âœ“ æ‰¹æ¬¡ #{batch_num} å®Œæˆ - è€—æ—¶: {elapsed:.2f}ç§’, é€Ÿåº¦: {speed:.1f} å—/ç§’")
    
    def _merge_index_shards(self, index_path: str, num_batches: int):
        """åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç´¢å¼•åˆ†ç‰‡"""
        print(f"æ­£åœ¨åˆå¹¶ {num_batches} ä¸ªæ‰¹æ¬¡...")
        
        all_chunks = []
        all_vectors = []
        
        for batch_num in tqdm(range(num_batches), desc="åŠ è½½æ‰¹æ¬¡"):
            batch_dir = os.path.join(index_path, f"batch_{batch_num:04d}")
            
            if not os.path.exists(batch_dir):
                continue
            
            # åŠ è½½chunks
            chunks_file = os.path.join(batch_dir, "chunks.json")
            if os.path.exists(chunks_file):
                with open(chunks_file, 'r', encoding='utf-8') as f:
                    chunks = json.load(f)
                    all_chunks.extend(chunks)
            
            # åŠ è½½vectors
            vectors_file = os.path.join(batch_dir, "vectors.npy")
            if os.path.exists(vectors_file):
                vectors = np.load(vectors_file)
                all_vectors.append(vectors)
        
        # åˆå¹¶vectors
        if all_vectors:
            self.vectors = np.vstack(all_vectors).astype(np.float32)
            self.chunks = all_chunks
            
            print(f"âœ“ åˆå¹¶å®Œæˆ: {len(self.chunks):,} ä¸ªæ–‡æœ¬å—")
            
            # å½’ä¸€åŒ–å‘é‡
            print("æ­£åœ¨å½’ä¸€åŒ–å‘é‡...")
            self._normalize_vectors()
            
            # ä¿å­˜æœ€ç»ˆç´¢å¼•
            print("æ­£åœ¨ä¿å­˜æœ€ç»ˆç´¢å¼•...")
            self.save_index(index_path)
            
            # æ¸…ç†ä¸´æ—¶æ‰¹æ¬¡æ–‡ä»¶
            print("æ­£åœ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            for batch_num in range(num_batches):
                batch_dir = os.path.join(index_path, f"batch_{batch_num:04d}")
                if os.path.exists(batch_dir):
                    import shutil
                    shutil.rmtree(batch_dir)
            
            print("âœ“ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")

    def save_index(self, index_path: str):
        """
        ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
        
        Args:
            index_path: ä¿å­˜ç´¢å¼•çš„ç›®å½•è·¯å¾„
        """
        if self.vectors is None or not self.chunks:
            raise ValueError("ç´¢å¼•ä¸ºç©º,æ— æ³•ä¿å­˜ã€‚è¯·å…ˆè°ƒç”¨ build_index()")

        os.makedirs(index_path, exist_ok=True)
        
        # ä¿å­˜æ–‡æœ¬å—
        chunks_file = os.path.join(index_path, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å‘é‡
        vectors_file = os.path.join(index_path, "vectors.npy")
        np.save(vectors_file, self.vectors)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "model_name": self.model_name,
            "device": self.device,
            "max_seq_length": self.max_seq_length,
            "num_chunks": len(self.chunks),
            "vector_dim": self.vectors.shape[1]
        }
        metadata_file = os.path.join(index_path, "metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ ç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
        print(f"  - æ–‡æœ¬å—: {len(self.chunks)}")
        print(f"  - å‘é‡ç»´åº¦: {self.vectors.shape}")

    @classmethod
    def load_index(cls, index_path: str, model_name: Optional[str] = None,
                   device: str = "cuda", max_seq_length: int = 512, **kwargs):
        """
        ä»ç£ç›˜åŠ è½½ç´¢å¼•
        
        Args:
            index_path: ç´¢å¼•ç›®å½•è·¯å¾„
            model_name: æ¨¡å‹åç§°(å¦‚æœä¸ºNoneåˆ™ä»metadataè¯»å–)
            device: è¿è¡Œè®¾å¤‡
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
        
        Returns:
            åŠ è½½çš„RAGIndexLocalå®ä¾‹
        """
        chunks_file = os.path.join(index_path, "chunks.json")
        vectors_file = os.path.join(index_path, "vectors.npy")
        metadata_file = os.path.join(index_path, "metadata.json")

        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        if not os.path.exists(chunks_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°chunksæ–‡ä»¶: {chunks_file}")
        if not os.path.exists(vectors_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°vectorsæ–‡ä»¶: {vectors_file}")
        
        # è¯»å–å…ƒæ•°æ®
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            if model_name is None:
                model_name = metadata.get("model_name", "sentence-transformers/all-MiniLM-L6-v2")
            print(f"âœ“ ä»å…ƒæ•°æ®è¯»å–é…ç½®: {metadata}")
        else:
            if model_name is None:
                model_name = "sentence-transformers/all-MiniLM-L6-v2"
            print("âš ï¸ æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶,ä½¿ç”¨é»˜è®¤é…ç½®")

        # åˆ›å»ºå®ä¾‹
        instance = cls(model_name=model_name, device=device, max_seq_length=max_seq_length)
        
        # åŠ è½½æ•°æ®
        with open(chunks_file, 'r', encoding='utf-8') as f:
            instance.chunks = json.load(f)
        
        instance.vectors = np.load(vectors_file)
        instance._normalize_vectors()
        
        print(f"âœ“ æˆåŠŸåŠ è½½ç´¢å¼•: {len(instance.chunks)} ä¸ªæ–‡æœ¬å—, å‘é‡å½¢çŠ¶: {instance.vectors.shape}")
        
        return instance

    def query(self, query: str, top_k: int = 3) -> str:
        """
        æŸ¥è¯¢ç´¢å¼•
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
        
        Returns:
            æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        if self.vectors is None:
            raise RuntimeError("ç´¢å¼•æœªæ„å»ºæˆ–åŠ è½½,æ— æ³•æŸ¥è¯¢")
        
        if not query:
            raise ValueError("æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.model.encode(
            [query],
            convert_to_numpy=True,
            show_progress_bar=False,
            normalize_embeddings=False
        )[0].astype(np.float32)
        
        # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
        query_norm = query_vector / np.linalg.norm(query_vector)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(self.normalized_vectors, query_norm)
        
        # è·å–top_kç»“æœ
        top_k_indices = np.argsort(similarities)[-top_k:][::-1]
        
        retrieved_chunks = [self.chunks[i] for i in top_k_indices]
        formatted_chunks = [self._format_chunk_for_output(chunk) for chunk in retrieved_chunks]
        context = "\n---\n".join(formatted_chunks)
        
        return f"### Retrieved Context:\n{context}"



class RAGIndexLocal_faiss(BaseRAGIndex):
    """
    ä½¿ç”¨FaissåŠ é€Ÿçš„æœ¬åœ°RAGç´¢å¼•
    é€‚åˆè¶…å¤§è§„æ¨¡çŸ¥è¯†åº“(ç™¾ä¸‡çº§ä»¥ä¸Š)
    """
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 device: str = "cuda", max_seq_length: int = 512,
                 use_gpu_index: bool = False):
        super().__init__(model_name=model_name, device=device, max_seq_length=max_seq_length)
        
        if not _FAISS_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£… faiss: pip install faiss-cpu æˆ– faiss-gpu")
        
        self.faiss_index = None
        self.use_gpu_index = use_gpu_index
        self.index_nlist: Optional[int] = None
        self.index_nprobe: Optional[int] = None
        self._faiss_on_gpu = False
        self._gpu_resources = None
        self.vectors: Optional[np.ndarray] = None
        print(f"âœ“ Faissç´¢å¼•æ¨¡å¼: {'GPU' if use_gpu_index else 'CPU'}")

    def _store_embeddings(self, vectors: np.ndarray) -> None:
        prepared = np.ascontiguousarray(vectors.astype(np.float32))
        faiss.normalize_L2(prepared)
        self.vectors = prepared

    def _finalize_embeddings(self) -> None:
        print("âœ“ å‘é‡å·²å½’ä¸€åŒ–å¹¶ç¼“å­˜ï¼Œä¸‹ä¸€æ­¥å°†æ„å»ºFaissç´¢å¼•")

    def _create_cpu_ivf_index(self, vector_dim: int, total_vectors: int) -> "faiss.Index":
        total_vectors = max(1, total_vectors)
        nlist, nprobe = _suggest_ivf_params(total_vectors)
        print(f"æ­£åœ¨åˆ›å»ºFaiss IndexIVFFlat (ç»´åº¦={vector_dim}, nlist={nlist}, nprobe={nprobe})")
        quantizer = faiss.IndexFlatIP(vector_dim)
        index = faiss.IndexIVFFlat(quantizer, vector_dim, nlist, faiss.METRIC_INNER_PRODUCT)
        index.nprobe = nprobe
        self.index_nlist = nlist
        self.index_nprobe = nprobe
        return index

    def _ensure_faiss_index(self, vector_dim: int, total_vectors: int) -> None:
        if self.faiss_index is None:
            self.faiss_index = self._create_cpu_ivf_index(vector_dim, total_vectors)
            self._faiss_on_gpu = False

    def _train_index_if_needed(self, vectors: np.ndarray) -> None:
        if hasattr(self.faiss_index, "is_trained") and not self.faiss_index.is_trained:
            print("æ­£åœ¨è®­ç»ƒFaissç´¢å¼•...")
            self.faiss_index.train(vectors)

    def _ensure_nprobe(self) -> None:
        if self.faiss_index is None or not hasattr(self.faiss_index, "nprobe"):
            return
        if self.index_nprobe is None:
            if hasattr(self.faiss_index, "nlist"):
                self.index_nprobe = min(16, max(1, self.faiss_index.nlist))
            else:
                self.index_nprobe = 1
        self.faiss_index.nprobe = min(self.index_nprobe, getattr(self.faiss_index, "nlist", self.index_nprobe))

    def _finalize_faiss_index(self) -> None:
        if self.faiss_index is None:
            return
        self._ensure_nprobe()
        if self.use_gpu_index and not self._faiss_on_gpu:
            try:
                if self._gpu_resources is None:
                    self._gpu_resources = faiss.StandardGpuResources()
                self.faiss_index = faiss.index_cpu_to_gpu(self._gpu_resources, 0, self.faiss_index)
                self._faiss_on_gpu = True
                if hasattr(self.faiss_index, "nlist"):
                    self.index_nlist = self.faiss_index.nlist
                self._ensure_nprobe()
                print("âœ“ ç´¢å¼•å·²è¿ç§»åˆ°GPU")
            except Exception as e:
                print(f"âš ï¸ ç´¢å¼•è¿ç§»åˆ°GPUå¤±è´¥({e}),ç»§ç»­ä½¿ç”¨CPUç´¢å¼•")
                self._faiss_on_gpu = False
        elif not self.use_gpu_index:
            self._faiss_on_gpu = False
            self._ensure_nprobe()

    def _cpu_index_for_persistence(self) -> Optional["faiss.Index"]:
        if self.faiss_index is None:
            return None
        if self._faiss_on_gpu:
            return faiss.index_gpu_to_cpu(self.faiss_index)
        return self.faiss_index

    def build_index_streaming(self, file_path: str, index_path: str,
                             batch_size: int = 64, chunk_size: int = 10000,
                             max_chunks: Optional[int] = None, num_workers: int = 1,
                             resume: bool = True):
        """
        æµå¼æ„å»ºFaissç´¢å¼•ï¼Œæ”¯æŒè¶…å¤§è§„æ¨¡çŸ¥è¯†åº“
        
        Args:
            file_path: çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„ (ä»…æ”¯æŒjsonlæ ¼å¼)
            index_path: ç´¢å¼•ä¿å­˜ç›®å½•
            batch_size: embeddingç”Ÿæˆçš„æ‰¹é‡å¤§å°
            chunk_size: æ¯æ¬¡å¤„ç†çš„æ–‡æœ¬å—æ•°é‡ï¼ˆæ§åˆ¶å†…å­˜ä½¿ç”¨ï¼‰
            max_chunks: æœ€å¤§å¤„ç†çš„æ–‡æœ¬å—æ•°é‡
            num_workers: å¹¶è¡Œå¤„ç†çš„workeræ•°é‡
            resume: æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­
        """
        if not file_path.endswith('.jsonl'):
            raise ValueError("æµå¼æ„å»ºä»…æ”¯æŒJSONLæ ¼å¼æ–‡ä»¶")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è‡ªåŠ¨ç¡®å®šworkeræ•°é‡
        num_workers = self._resolve_num_workers(num_workers)
        
        os.makedirs(index_path, exist_ok=True)
        
        # æ£€æŸ¥checkpoint
        checkpoint_file = os.path.join(index_path, "checkpoint.json")
        start_line = 0
        processed_chunks = 0
        
        if resume and os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
                start_line = checkpoint.get("processed_lines", 0)
                processed_chunks = checkpoint.get("processed_chunks", 0)
            print(f"ğŸ’¾ ä»checkpointæ¢å¤: å·²å¤„ç† {processed_chunks:,} ä¸ªæ–‡æœ¬å—")
            
            # åŠ è½½å·²æœ‰çš„chunkså’ŒFaissç´¢å¼•
            self._load_partial_index(index_path)
        
        if self.faiss_index is None:
            print("å°†åœ¨é¦–æ¬¡æ‰¹å¤„ç†æ—¶è‡ªåŠ¨åˆå§‹åŒ–Faiss IndexIVFFlat")
        
        print(f"\n{'='*60}")
        print(f"ğŸŒŠ Faissæµå¼ç´¢å¼•æ„å»ºæ¨¡å¼")
        print(f"{'='*60}")
        print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
        print(f"ç´¢å¼•è·¯å¾„: {index_path}")
        print(f"æ¯æ‰¹å¤„ç†: {chunk_size:,} ä¸ªæ–‡æœ¬å—")
        print(f"Batchå¤§å°: {batch_size}")
        print(f"Workeræ•°: {num_workers}")
        print(f"{'='*60}\n")
        
        total_start_time = time.time()
        batch_chunks = []
        batch_texts = []
        current_line = 0
        
        # æ‰“å¼€æ–‡ä»¶å¹¶é€è¡Œè¯»å–
        with open(file_path, 'r', encoding='utf-8') as f:
            # è·³è¿‡å·²å¤„ç†çš„è¡Œ
            if start_line > 0:
                print(f"â­ï¸  è·³è¿‡å·²å¤„ç†çš„ {start_line:,} è¡Œ...")
                for _ in range(start_line):
                    next(f)
                current_line = start_line
            
            pbar = tqdm(desc="å¤„ç†æ–‡æœ¬å—", unit="å—", initial=processed_chunks)
            
            for line in f:
                current_line += 1
                
                if not line.strip():
                    continue
                
                try:
                    sample = json.loads(line)
                    
                    if not (isinstance(sample, dict) and "text" in sample and 
                           isinstance(sample["text"], str) and sample["text"]):
                        continue
                    
                    title = sample.get("title", "")
                    text = sample.get("text", "")
                    combined_text = f"{title}\n\n{text}" if title else text
                    
                    batch_chunks.append(sample)
                    batch_texts.append(combined_text)
                    
                    if len(batch_chunks) >= chunk_size or \
                       (max_chunks and processed_chunks + len(batch_chunks) >= max_chunks):
                        
                        # å¤„ç†å¹¶æ·»åŠ åˆ°Faissç´¢å¼•
                        self._process_and_add_to_faiss(
                            batch_chunks, batch_texts,
                            index_path, batch_size, num_workers
                        )
                        
                        processed_chunks += len(batch_chunks)
                        pbar.update(len(batch_chunks))
                        
                        # ä¿å­˜checkpoint
                        self._save_checkpoint(checkpoint_file, current_line, processed_chunks)
                        
                        batch_chunks = []
                        batch_texts = []
                        gc.collect()
                        
                        if max_chunks and processed_chunks >= max_chunks:
                            print(f"\nâš ï¸  å·²è¾¾åˆ°æœ€å¤§æ–‡æœ¬å—æ•°é™åˆ¶: {max_chunks:,}")
                            break
                
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  è·³è¿‡æ— æ•ˆJSON (è¡Œ{current_line}): {e}")
                    continue
            
            # å¤„ç†å‰©ä½™æ‰¹æ¬¡
            if batch_chunks:
                self._process_and_add_to_faiss(
                    batch_chunks, batch_texts,
                    index_path, batch_size, num_workers
                )
                processed_chunks += len(batch_chunks)
                pbar.update(len(batch_chunks))
                self._save_checkpoint(checkpoint_file, current_line, processed_chunks)
            
            pbar.close()
        
        # æœ€ç»ˆä¿å­˜
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆç´¢å¼•")
        print(f"{'='*60}")
        self._finalize_faiss_index()
        self.save_index(index_path)
        
        # åˆ é™¤checkpoint
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
        
        total_time = time.time() - total_start_time
        print(f"\n{'='*60}")
        print(f"âœ… Faissæµå¼ç´¢å¼•æ„å»ºå®Œæˆ!")
        print(f"{'='*60}")
        print(f"æ€»æ–‡æœ¬å—æ•°: {processed_chunks:,}")
        print(f"Faissç´¢å¼•åŒ…å«: {self.faiss_index.ntotal:,} ä¸ªå‘é‡")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"å¹³å‡é€Ÿåº¦: {processed_chunks/total_time:.1f} å—/ç§’")
        print(f"ç´¢å¼•è·¯å¾„: {index_path}")
        print(f"{'='*60}\n")
    
    def _process_and_add_to_faiss(self, chunks: List[Dict], texts: List[str],
                                  index_path: str, batch_size: int, num_workers: int):
        """å¤„ç†æ‰¹æ¬¡å¹¶ç›´æ¥æ·»åŠ åˆ°Faissç´¢å¼•"""
        # ç”Ÿæˆembeddings
        vectors = self._get_embeddings_batch(
            texts,
            batch_size=batch_size,
            show_progress=False,
            num_workers=num_workers
        )
        
        # å½’ä¸€åŒ–å‘é‡
        vectors = np.ascontiguousarray(vectors.astype(np.float32))
        faiss.normalize_L2(vectors)
        
        total_vectors = len(self.chunks) + vectors.shape[0]
        self._ensure_faiss_index(vectors.shape[1], total_vectors)
        self._train_index_if_needed(vectors)
        self.faiss_index.add(vectors)
        self._ensure_nprobe()
        
        # ä¿å­˜chunksï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        self.chunks.extend(chunks)
        
        # å°†vectorsæ·»åŠ åˆ°å†…å­˜ä¸­ï¼ˆç”¨äºæœ€åä¿å­˜ï¼‰
        if self.vectors is None:
            self.vectors = vectors
        else:
            self.vectors = np.vstack([self.vectors, vectors])
        
        # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
        if len(self.chunks) % 50000 < len(chunks):  # æ¯5ä¸‡æ¡ä¿å­˜ä¸€æ¬¡
            print(f"  ğŸ’¾ ä¿å­˜ä¸­é—´ç»“æœ ({len(self.chunks):,} ä¸ªæ–‡æœ¬å—)...")
            self._save_partial_index(index_path)
    
    def _save_partial_index(self, index_path: str):
        """ä¿å­˜éƒ¨åˆ†ç´¢å¼•ï¼ˆä¸­é—´checkpointï¼‰"""
        partial_dir = os.path.join(index_path, "partial")
        os.makedirs(partial_dir, exist_ok=True)
        
        # ä¿å­˜chunks
        chunks_file = os.path.join(partial_dir, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False)
        
        # ä¿å­˜Faissç´¢å¼•
        faiss_file = os.path.join(partial_dir, "faiss.index")
        index_to_save = self._cpu_index_for_persistence()
        if index_to_save is not None:
            faiss.write_index(index_to_save, faiss_file)
        
        # ä¿å­˜numpy vectors
        if self.vectors is not None:
            vectors_file = os.path.join(partial_dir, "vectors.npy")
            np.save(vectors_file, self.vectors)
    
    def _load_partial_index(self, index_path: str):
        """åŠ è½½éƒ¨åˆ†ç´¢å¼•ï¼ˆç”¨äºresumeï¼‰"""
        partial_dir = os.path.join(index_path, "partial")
        
        if not os.path.exists(partial_dir):
            return
        
        # åŠ è½½chunks
        chunks_file = os.path.join(partial_dir, "chunks.json")
        if os.path.exists(chunks_file):
            with open(chunks_file, 'r', encoding='utf-8') as f:
                self.chunks = json.load(f)
            print(f"âœ“ åŠ è½½å·²æœ‰chunks: {len(self.chunks):,} ä¸ª")
        
        # åŠ è½½Faissç´¢å¼•
        faiss_file = os.path.join(partial_dir, "faiss.index")
        if os.path.exists(faiss_file):
            self.faiss_index = faiss.read_index(faiss_file)
            if hasattr(self.faiss_index, "nlist"):
                self.index_nlist = self.faiss_index.nlist
                self.index_nprobe = min(16, max(1, self.faiss_index.nlist))
                self._ensure_nprobe()
            self._faiss_on_gpu = False
            print(f"âœ“ åŠ è½½å·²æœ‰Faissç´¢å¼•: {self.faiss_index.ntotal:,} ä¸ªå‘é‡")
        
        # åŠ è½½vectors
        vectors_file = os.path.join(partial_dir, "vectors.npy")
        if os.path.exists(vectors_file):
            self.vectors = np.load(vectors_file)
            print(f"âœ“ åŠ è½½å·²æœ‰vectors: {self.vectors.shape}")

    def build_index(self, file_path: str, batch_size: int = 64,
                   max_chunks: Optional[int] = None, num_workers: int = 1):
        """
        æ„å»ºFaissç´¢å¼•ï¼ˆå¸¸è§„æ¨¡å¼ï¼‰
        
        Args:
            file_path: çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„
            batch_size: embeddingç”Ÿæˆæ‰¹é‡å¤§å°
            max_chunks: æœ€å¤§å¤„ç†çš„æ–‡æœ¬å—æ•°é‡
            num_workers: å¹¶è¡Œå¤„ç†çš„workeræ•°é‡
        
        æç¤º: å¯¹äºè¶…å¤§è§„æ¨¡çŸ¥è¯†åº“ï¼Œå»ºè®®ä½¿ç”¨ build_index_streaming() æ–¹æ³•
        """
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•ç”Ÿæˆembeddings
        super().build_index(file_path, batch_size, max_chunks, num_workers)
        
        # æ„å»ºFaissç´¢å¼•
        print(f"\n{'='*60}")
        print(f"ğŸš€ æ­¥éª¤ 5/5: æ„å»ºFaissç´¢å¼•")
        print(f"{'='*60}")
        
        try:
            start_time = time.time()
            if self.vectors is None:
                raise RuntimeError("å½’ä¸€åŒ–å‘é‡ç¼ºå¤±ï¼Œæ— æ³•æ„å»ºFaissç´¢å¼•")
            vectors = np.ascontiguousarray(self.vectors.astype(np.float32))
            d = vectors.shape[1]
            total_vectors = vectors.shape[0]
            
            print(f"å‘é‡ç»´åº¦: {d}")
            print(f"å‘é‡æ•°é‡: {len(vectors):,}")
            
            # æ„å»ºFaiss IndexIVFFlat
            self.faiss_index = None
            self._faiss_on_gpu = False
            self._ensure_faiss_index(d, total_vectors)
            self._train_index_if_needed(vectors)
            
            print("æ­£åœ¨æ·»åŠ å‘é‡åˆ°Faissç´¢å¼•...")
            self.faiss_index.add(vectors)
            self._ensure_nprobe()
            self._finalize_faiss_index()
            
            elapsed = time.time() - start_time
            print(f"âœ“ Faissç´¢å¼•æ„å»ºå®Œæˆ!")
            print(f"  - åŒ…å«å‘é‡æ•°: {self.faiss_index.ntotal:,}")
            if self.index_nlist is not None:
                print(f"  - nlist: {self.index_nlist}")
            if self.index_nprobe is not None:
                print(f"  - nprobe: {self.index_nprobe}")
            print(f"  - æ„å»ºè€—æ—¶: {elapsed:.2f}ç§’")
            
            # æ¸…ç†å†…å­˜
            del vectors
            gc.collect()
            
        except Exception as e:
            print(f"âœ— æ„å»ºFaissç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
            raise

    def save_index(self, index_path: str):
        """
        ä¿å­˜Faissç´¢å¼•
        
        Args:
            index_path: ä¿å­˜ç´¢å¼•çš„ç›®å½•è·¯å¾„
        """
        if self.faiss_index is None or not self.chunks:
            raise ValueError("ç´¢å¼•ä¸ºç©º,æ— æ³•ä¿å­˜")

        os.makedirs(index_path, exist_ok=True)
        
        # ä¿å­˜æ–‡æœ¬å—
        chunks_file = os.path.join(index_path, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜Faissç´¢å¼•
        index_file = os.path.join(index_path, "faiss.index")
        self._ensure_nprobe()
        index_to_save = self._cpu_index_for_persistence()
        if index_to_save is None:
            raise RuntimeError("Faissç´¢å¼•æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")
        faiss.write_index(index_to_save, index_file)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "model_name": self.model_name,
            "device": self.device,
            "max_seq_length": self.max_seq_length,
            "num_chunks": len(self.chunks),
            "vector_dim": self.faiss_index.d if hasattr(self.faiss_index, 'd') else None,
            "use_gpu_index": self.use_gpu_index,
            "index_type": "faiss.IndexIVFFlat",
            "nlist": self.index_nlist,
            "nprobe": self.index_nprobe
        }
        metadata_file = os.path.join(index_path, "metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ Faissç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")

    @classmethod
    def load_index(cls, index_path: str, model_name: Optional[str] = None,
                   device: str = "cuda", max_seq_length: int = 512,
                   use_gpu_index: bool = False, **kwargs):
        """
        åŠ è½½Faissç´¢å¼•
        
        Args:
            index_path: ç´¢å¼•ç›®å½•è·¯å¾„
            model_name: æ¨¡å‹åç§°
            device: è¿è¡Œè®¾å¤‡
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
            use_gpu_index: æ˜¯å¦ä½¿ç”¨GPUç´¢å¼•
        
        Returns:
            åŠ è½½çš„RAGIndexLocal_faisså®ä¾‹
        """
        chunks_file = os.path.join(index_path, "chunks.json")
        index_file = os.path.join(index_path, "faiss.index")
        metadata_file = os.path.join(index_path, "metadata.json")

        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        if not os.path.exists(chunks_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°chunksæ–‡ä»¶: {chunks_file}")
        if not os.path.exists(index_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°Faissç´¢å¼•æ–‡ä»¶: {index_file}")
        
        # è¯»å–å…ƒæ•°æ®
        metadata = None
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            if model_name is None:
                model_name = metadata.get("model_name", "sentence-transformers/all-MiniLM-L6-v2")
            print(f"âœ“ ä»å…ƒæ•°æ®è¯»å–é…ç½®: {metadata}")
        else:
            if model_name is None:
                model_name = "sentence-transformers/all-MiniLM-L6-v2"
            print("âš ï¸ æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶,ä½¿ç”¨é»˜è®¤é…ç½®")

        # åˆ›å»ºå®ä¾‹
        instance = cls(
            model_name=model_name,
            device=device,
            max_seq_length=max_seq_length,
            use_gpu_index=use_gpu_index
        )
        
        # åŠ è½½æ–‡æœ¬å—
        with open(chunks_file, 'r', encoding='utf-8') as f:
            instance.chunks = json.load(f)
        
        # åŠ è½½Faissç´¢å¼•
        cpu_index = faiss.read_index(index_file)
        instance.faiss_index = cpu_index
        instance._faiss_on_gpu = False
        if hasattr(cpu_index, "nlist"):
            instance.index_nlist = cpu_index.nlist
        metadata_nprobe = metadata.get("nprobe") if metadata else None
        if metadata_nprobe is not None:
            instance.index_nprobe = int(metadata_nprobe)
        elif instance.index_nlist is not None:
            instance.index_nprobe = min(16, max(1, instance.index_nlist))
        else:
            instance.index_nprobe = 1
        instance._ensure_nprobe()
        
        instance._finalize_faiss_index()

        print(f"âœ“ æˆåŠŸåŠ è½½Faissç´¢å¼•: {len(instance.chunks)} ä¸ªæ–‡æœ¬å—")
        
        return instance

    def query(self, query: str, top_k: int = 3) -> str:
        """
        ä½¿ç”¨FaissæŸ¥è¯¢ç´¢å¼•
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
        
        Returns:
            æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        if self.faiss_index is None:
            raise RuntimeError("Faissç´¢å¼•æœªæ„å»ºæˆ–åŠ è½½,æ— æ³•æŸ¥è¯¢")
        
        if not query:
            raise ValueError("æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º")


        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.model.encode(
                [query],
                convert_to_numpy=True,
                show_progress_bar=False,
                normalize_embeddings=False
            )[0].astype(np.float32)
            
            # å½’ä¸€åŒ–
            query_vector = query_vector.reshape(1, -1)
            faiss.normalize_L2(query_vector)
            
            self._ensure_nprobe()
            
            # æœç´¢
            distances, indices = self.faiss_index.search(query_vector, top_k)
            indices = indices[0]
            
            # è·å–ç»“æœ
            retrieved_chunks = [self.chunks[i] for i in indices if i != -1 and i < len(self.chunks)]
            formatted_chunks = [self._format_chunk_for_output(chunk) for chunk in retrieved_chunks]
            context = "\n---\n".join(formatted_chunks)
            
            return f"### Retrieved Context:\n{context}"
        except Exception as e:
            return f"[æŸ¥è¯¢é”™è¯¯] {str(e)}"


def get_rag_index_class(use_faiss: bool = False):
    """æ ¹æ®é…ç½®è·å–æœ¬åœ°RAGç´¢å¼•ç±»"""
    if not _SENTENCE_TRANSFORMERS_AVAILABLE:
        raise ImportError("sentence_transformers æœªå®‰è£…, æ— æ³•ä½¿ç”¨æœ¬åœ°RAGç´¢å¼•")

    if use_faiss:
        if not _FAISS_AVAILABLE:
            print("âš ï¸ Faiss ä¸å¯ç”¨,å›é€€åˆ° RAGIndexLocal (Numpy å®ç°)")
            return RAGIndexLocal
        print("âœ… ä½¿ç”¨ RAGIndexLocal_faiss (æœ¬åœ°embedding + FaissåŠ é€Ÿ)")
        return RAGIndexLocal_faiss

    print("âœ… ä½¿ç”¨ RAGIndexLocal (æœ¬åœ°embedding)")
    return RAGIndexLocal
